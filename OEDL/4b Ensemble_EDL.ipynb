{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score,auc,roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer,StandardScaler\n",
    "\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,LSTM,GRU,BatchNormalization\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NIHSS</th>\n",
       "      <th>original_shape_Elongation</th>\n",
       "      <th>original_firstorder_Skewness</th>\n",
       "      <th>original_glcm_SumSquares</th>\n",
       "      <th>original_gldm_SmallDependenceLowGrayLevelEmphasis</th>\n",
       "      <th>wavelet-LLH_glrlm_GrayLevelVariance</th>\n",
       "      <th>wavelet-LLH_glrlm_ShortRunEmphasis</th>\n",
       "      <th>wavelet-LLH_gldm_GrayLevelVariance</th>\n",
       "      <th>wavelet-LLH_glszm_GrayLevelNonUniformity</th>\n",
       "      <th>wavelet-LHL_firstorder_90Percentile</th>\n",
       "      <th>...</th>\n",
       "      <th>Drinkalcohol _1.0</th>\n",
       "      <th>Drinkalcohol _2.0</th>\n",
       "      <th>Drinkalcohol _nan</th>\n",
       "      <th>Drinkalcohol _-1</th>\n",
       "      <th>OCSP_4.0</th>\n",
       "      <th>OCSP_3.0</th>\n",
       "      <th>OCSP_2.0</th>\n",
       "      <th>OCSP_1.0</th>\n",
       "      <th>OCSP_nan</th>\n",
       "      <th>OCSP_-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.565605</td>\n",
       "      <td>0.470144</td>\n",
       "      <td>0.373676</td>\n",
       "      <td>0.029411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.491783</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451330</td>\n",
       "      <td>0.392714</td>\n",
       "      <td>0.068449</td>\n",
       "      <td>0.106469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.497301</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.612372</td>\n",
       "      <td>0.851287</td>\n",
       "      <td>0.069507</td>\n",
       "      <td>0.273280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.178226</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.489943</td>\n",
       "      <td>0.405634</td>\n",
       "      <td>0.313635</td>\n",
       "      <td>0.066088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.372396</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.653883</td>\n",
       "      <td>0.291065</td>\n",
       "      <td>0.236634</td>\n",
       "      <td>0.056482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.326073</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>2</td>\n",
       "      <td>0.126109</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.036365</td>\n",
       "      <td>0.934492</td>\n",
       "      <td>0.123367</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.929336</td>\n",
       "      <td>0.104301</td>\n",
       "      <td>0.069522</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>2</td>\n",
       "      <td>0.458162</td>\n",
       "      <td>0.712629</td>\n",
       "      <td>0.361885</td>\n",
       "      <td>0.134446</td>\n",
       "      <td>0.498550</td>\n",
       "      <td>0.636870</td>\n",
       "      <td>0.061614</td>\n",
       "      <td>0.162788</td>\n",
       "      <td>0.234031</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>2</td>\n",
       "      <td>0.580937</td>\n",
       "      <td>0.412865</td>\n",
       "      <td>0.122301</td>\n",
       "      <td>0.643417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273111</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.488470</td>\n",
       "      <td>0.266404</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2</td>\n",
       "      <td>0.311074</td>\n",
       "      <td>0.637602</td>\n",
       "      <td>0.023883</td>\n",
       "      <td>0.302411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560780</td>\n",
       "      <td>0.223492</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.070366</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2</td>\n",
       "      <td>0.667845</td>\n",
       "      <td>0.560664</td>\n",
       "      <td>0.077014</td>\n",
       "      <td>0.299535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411454</td>\n",
       "      <td>0.109783</td>\n",
       "      <td>0.201142</td>\n",
       "      <td>0.131911</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>626 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NIHSS  original_shape_Elongation  original_firstorder_Skewness  \\\n",
       "0        0                   0.565605                      0.470144   \n",
       "1        0                   0.451330                      0.392714   \n",
       "2        0                   0.612372                      0.851287   \n",
       "3        0                   0.489943                      0.405634   \n",
       "4        0                   0.653883                      0.291065   \n",
       "..     ...                        ...                           ...   \n",
       "621      2                   0.126109                      0.447496   \n",
       "622      2                   0.458162                      0.712629   \n",
       "623      2                   0.580937                      0.412865   \n",
       "624      2                   0.311074                      0.637602   \n",
       "625      2                   0.667845                      0.560664   \n",
       "\n",
       "     original_glcm_SumSquares  \\\n",
       "0                    0.373676   \n",
       "1                    0.068449   \n",
       "2                    0.069507   \n",
       "3                    0.313635   \n",
       "4                    0.236634   \n",
       "..                        ...   \n",
       "621                  0.036365   \n",
       "622                  0.361885   \n",
       "623                  0.122301   \n",
       "624                  0.023883   \n",
       "625                  0.077014   \n",
       "\n",
       "     original_gldm_SmallDependenceLowGrayLevelEmphasis  \\\n",
       "0                                             0.029411   \n",
       "1                                             0.106469   \n",
       "2                                             0.273280   \n",
       "3                                             0.066088   \n",
       "4                                             0.056482   \n",
       "..                                                 ...   \n",
       "621                                           0.934492   \n",
       "622                                           0.134446   \n",
       "623                                           0.643417   \n",
       "624                                           0.302411   \n",
       "625                                           0.299535   \n",
       "\n",
       "     wavelet-LLH_glrlm_GrayLevelVariance  wavelet-LLH_glrlm_ShortRunEmphasis  \\\n",
       "0                               0.000000                            0.431860   \n",
       "1                               0.000000                            0.495983   \n",
       "2                               0.000000                            0.863248   \n",
       "3                               0.000000                            0.457018   \n",
       "4                               0.000000                            0.203632   \n",
       "..                                   ...                                 ...   \n",
       "621                             0.123367                            0.062207   \n",
       "622                             0.498550                            0.636870   \n",
       "623                             0.000000                            0.273111   \n",
       "624                             0.000000                            0.560780   \n",
       "625                             0.000000                            0.411454   \n",
       "\n",
       "     wavelet-LLH_gldm_GrayLevelVariance  \\\n",
       "0                              0.000000   \n",
       "1                              0.000000   \n",
       "2                              0.000000   \n",
       "3                              0.000000   \n",
       "4                              0.000000   \n",
       "..                                  ...   \n",
       "621                            0.929336   \n",
       "622                            0.061614   \n",
       "623                            0.000198   \n",
       "624                            0.223492   \n",
       "625                            0.109783   \n",
       "\n",
       "     wavelet-LLH_glszm_GrayLevelNonUniformity  \\\n",
       "0                                    0.160000   \n",
       "1                                    0.160000   \n",
       "2                                    0.160000   \n",
       "3                                    0.160000   \n",
       "4                                    0.160000   \n",
       "..                                        ...   \n",
       "621                                  0.104301   \n",
       "622                                  0.162788   \n",
       "623                                  0.488470   \n",
       "624                                  0.596927   \n",
       "625                                  0.201142   \n",
       "\n",
       "     wavelet-LHL_firstorder_90Percentile  ...  Drinkalcohol _1.0  \\\n",
       "0                               0.491783  ...                  1   \n",
       "1                               0.497301  ...                  1   \n",
       "2                               0.178226  ...                  1   \n",
       "3                               0.372396  ...                  1   \n",
       "4                               0.326073  ...                  1   \n",
       "..                                   ...  ...                ...   \n",
       "621                             0.069522  ...                  1   \n",
       "622                             0.234031  ...                  1   \n",
       "623                             0.266404  ...                  1   \n",
       "624                             0.070366  ...                  0   \n",
       "625                             0.131911  ...                  1   \n",
       "\n",
       "     Drinkalcohol _2.0  Drinkalcohol _nan  Drinkalcohol _-1  OCSP_4.0  \\\n",
       "0                    0                  0                 0         0   \n",
       "1                    0                  0                 0         1   \n",
       "2                    0                  0                 0         1   \n",
       "3                    0                  0                 0         1   \n",
       "4                    0                  0                 0         0   \n",
       "..                 ...                ...               ...       ...   \n",
       "621                  0                  0                 0         0   \n",
       "622                  0                  0                 0         0   \n",
       "623                  0                  0                 0         0   \n",
       "624                  0                  0                 0         0   \n",
       "625                  0                  0                 0         0   \n",
       "\n",
       "     OCSP_3.0  OCSP_2.0  OCSP_1.0  OCSP_nan  OCSP_-1  \n",
       "0           1         0         0         0        0  \n",
       "1           0         0         0         0        0  \n",
       "2           0         0         0         0        0  \n",
       "3           0         0         0         0        0  \n",
       "4           0         1         0         0        0  \n",
       "..        ...       ...       ...       ...      ...  \n",
       "621         0         1         0         0        0  \n",
       "622         1         0         0         0        0  \n",
       "623         0         1         0         0        0  \n",
       "624         0         1         0         0        0  \n",
       "625         0         1         0         0        0  \n",
       "\n",
       "[626 rows x 91 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/combine/2.storke_combine_code_multi_balance.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(df.pop('NIHSS'))\n",
    "X = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 90)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = [0,1,2]\n",
    "Class_dict = dict(zip(Class, range(len(Class))))\n",
    "Class_dict\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(list(Class_dict.values()))\n",
    "y_train_labels = lb.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lstm = x_train.reshape(x_train.shape[0],1,x_train.shape[1])\n",
    "x_test_lstm = x_test.reshape(x_test.shape[0],1,x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(X):\n",
    "    RNN_test_label = []\n",
    "    Class = [0,1,2]\n",
    "    Class_dict = dict(zip(Class, range(len(Class))))\n",
    "    Class_dict\n",
    "    for i in range(0,X.shape[0]):\n",
    "        RNN_test_label.append(Class_dict[np.argmax(X[i])])\n",
    "    RNN_test_label = np.array(RNN_test_label,dtype = 'int64')\n",
    "    return RNN_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                910       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,053\n",
      "Trainable params: 1,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "def buildDNN(layer1,layer2,n_class):\n",
    "    init = K.initializers.glorot_uniform(seed=1)\n",
    "    simple_adam = tf.keras.optimizers.Adam()\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Dense(units=layer1, input_dim=90, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=layer2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=n_class, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "dnn = buildDNN(layer1=10,layer2=10,n_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBN\n",
    "def buildDBN(layer1,layer2,epoca,K=500):\n",
    "    \n",
    "    # cantidad de neuronas ocultas \n",
    "    hidden_layers = []\n",
    "    hidden_layers.append( int(layer1))\n",
    "    hidden_layers.append( int(layer2))\n",
    "\n",
    "    DBN_classifier = SupervisedDBNClassification(hidden_layers_structure = hidden_layers,\n",
    "                                                    learning_rate_rbm=0.05,\n",
    "                                                    learning_rate=0.1,\n",
    "                                                    n_epochs_rbm=epoca,\n",
    "                                                    n_iter_backprop=K,\n",
    "                                                    batch_size=32,\n",
    "                                                    activation_function='relu',\n",
    "                                                    dropout_p=0.2)\n",
    "    return DBN_classifier\n",
    "\n",
    "dbn = buildDBN(layer1=10,layer2=10,epoca=20,K=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 90)]           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10)                4040      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,183\n",
      "Trainable params: 4,183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM_RNN\n",
    "def buildLSTM(timeStep,inputColNum,outStep,learnRate=1e-4):\n",
    "    inputLayer = Input(shape=(timeStep,inputColNum))\n",
    "    middle = LSTM(activation='tanh')(inputLayer)\n",
    "    middle = Dense(activation='tanh')(middle)\n",
    "    outputLayer = Dense(outStep)(middle)\n",
    "    \n",
    "    #建模\n",
    "    model = Model(inputs=inputLayer,outputs=outputLayer)\n",
    "    optimizer = Adam(learning_rate=learnRate)\n",
    "    model.compile(optimizer=optimizer,loss='mse') \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#LSTM\n",
    "lstm = buildLSTM(timeStep=1,inputColNum=90,outStep=3,learnRate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_FOLDS = 4  \n",
    "kf = StratifiedKFold(n_splits=4, random_state=2020, shuffle=True)  \n",
    "_N_CLASS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 0s 205us/sample - loss: 1.1026 - acc: 0.4512\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 1.0658 - acc: 0.4604\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 1.0356 - acc: 0.4604\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 1.0002 - acc: 0.4665\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.9678 - acc: 0.4695\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.9343 - acc: 0.4817\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.8963 - acc: 0.5640\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.8495 - acc: 0.6585\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.7965 - acc: 0.6982\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.7319 - acc: 0.7348\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.6726 - acc: 0.7500\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.6254 - acc: 0.7774\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.5849 - acc: 0.7835\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.5376 - acc: 0.7866\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.5029 - acc: 0.8140\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.4688 - acc: 0.8201\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.4368 - acc: 0.8354\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.4056 - acc: 0.8445\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.3742 - acc: 0.8537\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.3519 - acc: 0.8659\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.3244 - acc: 0.8841\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.3028 - acc: 0.8933\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.2859 - acc: 0.8902\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.2722 - acc: 0.9116\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 0.2555 - acc: 0.8994\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.2432 - acc: 0.9268\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.2327 - acc: 0.9238\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 0.2237 - acc: 0.9329\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.2113 - acc: 0.9329\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.2030 - acc: 0.9360\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1965 - acc: 0.9329\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1885 - acc: 0.9451\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1817 - acc: 0.9543\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1717 - acc: 0.9482\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1677 - acc: 0.9512\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1639 - acc: 0.9543\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.1552 - acc: 0.9573\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1502 - acc: 0.9512\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1485 - acc: 0.9665\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1403 - acc: 0.9543\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1342 - acc: 0.9573\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1290 - acc: 0.9665\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1286 - acc: 0.9573\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1234 - acc: 0.9695\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 24us/sample - loss: 0.1186 - acc: 0.9634\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.1154 - acc: 0.9604\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1114 - acc: 0.9634\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1111 - acc: 0.9665\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.1043 - acc: 0.9665\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.1026 - acc: 0.9726\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.1010 - acc: 0.9817\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0973 - acc: 0.9726\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0936 - acc: 0.9787\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0917 - acc: 0.9787\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0882 - acc: 0.9756\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0868 - acc: 0.9787\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0856 - acc: 0.9756\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0817 - acc: 0.9848\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0817 - acc: 0.9787\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0789 - acc: 0.9817\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0777 - acc: 0.9817\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0775 - acc: 0.9878\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0715 - acc: 0.9878\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0716 - acc: 0.9878\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0714 - acc: 0.9848\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0720 - acc: 0.9817\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0658 - acc: 0.9878\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0636 - acc: 0.9939\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0616 - acc: 0.9878\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0626 - acc: 0.9848\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0653 - acc: 0.9848\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 28us/sample - loss: 0.0612 - acc: 0.9878\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0569 - acc: 0.9939\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0593 - acc: 0.9939\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0541 - acc: 0.9909\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0508 - acc: 0.9939\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0508 - acc: 0.9909\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0493 - acc: 0.9939\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0484 - acc: 0.9939\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0452 - acc: 0.9970\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0445 - acc: 0.9970\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0445 - acc: 0.9970\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0427 - acc: 0.9939\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0416 - acc: 0.9970\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0412 - acc: 0.9970\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0401 - acc: 0.9970\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0397 - acc: 0.9970\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0374 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0370 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0344 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0337 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0343 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0340 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0325 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0310 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0292 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0282 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0278 - acc: 1.0000\n",
      "Train on 328 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0826 - acc: 0.9817\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0714 - acc: 0.9848\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0671 - acc: 0.9878\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0602 - acc: 0.9817\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0566 - acc: 0.9939\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0562 - acc: 0.9878\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0510 - acc: 0.9939\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0481 - acc: 0.9939\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0482 - acc: 0.9939\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0445 - acc: 0.9939\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0424 - acc: 0.9939\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0415 - acc: 0.9939\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0399 - acc: 0.9939\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 0.0391 - acc: 0.9939\n",
      "Epoch 15/100\n",
      " 20/328 [>.............................] - ETA: 0s - loss: 0.0350 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0376 - acc: 0.9939\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0362 - acc: 0.9939\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0354 - acc: 0.9939\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0350 - acc: 0.9939\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0370 - acc: 0.9939\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0326 - acc: 0.9939\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0320 - acc: 0.9939\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 4us/sample - loss: 0.0305 - acc: 0.9939\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0304 - acc: 0.9939\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0306 - acc: 0.9939\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0292 - acc: 0.9939\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0276 - acc: 0.9939\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0278 - acc: 0.9939\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0259 - acc: 0.9939\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0264 - acc: 0.9939\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0248 - acc: 0.9939\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0245 - acc: 0.9939\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0241 - acc: 0.9939\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0227 - acc: 0.9970\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0223 - acc: 0.9939\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0242 - acc: 0.9939\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0203 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0209 - acc: 0.9970\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0202 - acc: 0.9970\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0197 - acc: 0.9970\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0156 - acc: 0.9970\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0145 - acc: 0.9970\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0125 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 48us/sample - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 0s/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 61us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 27us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Train on 329 samples\n",
      "Epoch 1/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0280 - acc: 0.9909\n",
      "Epoch 2/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0170 - acc: 0.9970\n",
      "Epoch 3/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "329/329 [==============================] - 0s 45us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "329/329 [==============================] - 0s 33us/sample - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "329/329 [==============================] - 0s 33us/sample - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "329/329 [==============================] - 0s 42us/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "329/329 [==============================] - 0s 33us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "329/329 [==============================] - 0s 13us/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "329/329 [==============================] - 0s 51us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "329/329 [==============================] - 0s 33us/sample - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "329/329 [==============================] - 0s 25us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "329/329 [==============================] - 0s 36us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "329/329 [==============================] - 0s 33us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "329/329 [==============================] - 0s 39us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "329/329 [==============================] - 0s 25us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Train on 329 samples\n",
      "Epoch 1/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0266 - acc: 0.9939\n",
      "Epoch 2/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 9.9873e-04 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 9.8237e-04 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 9.5778e-04 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 9.8057e-04 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 9.3911e-04 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 9.2861e-04 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 9.3057e-04 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.8802e-04 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.8503e-04 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 8.9421e-04 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 8.9738e-04 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 8.5429e-04 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.3872e-04 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.2703e-04 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.2167e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.0983e-04 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.1134e-04 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.1254e-04 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 8.0734e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 7.6424e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "329/329 [==============================] - 0s 28us/sample - loss: 7.5634e-04 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 7.5384e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 7.3848e-04 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 7.2570e-04 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 7.8883e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 7.3006e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 7.1077e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 7.1497e-04 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 7.0624e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.7023e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.6139e-04 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.5406e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.5377e-04 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.4770e-04 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.2752e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.2441e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.1907e-04 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.2895e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.9787e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 6.0508e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 5.9240e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 6.2861e-04 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.7812e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.7563e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 5.6869e-04 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 5.5613e-04 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 5.5145e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "329/329 [==============================] - 0s 30us/sample - loss: 5.5078e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.4243e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 5.5092e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.3069e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.1746e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 5.1684e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.1122e-04 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "329/329 [==============================] - 0s 27us/sample - loss: 5.0046e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "329/329 [==============================] - 0s 24us/sample - loss: 4.9176e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "oof_train1 = np.zeros((x_train.shape[0], _N_CLASS)) \n",
    "oof_test1 = np.empty((x_test.shape[0], _N_CLASS))  \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train,y_train)):\n",
    "    kf_X_train = x_train[train_index] \n",
    "    kf_y_train = y_train[train_index]  \n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = x_train[test_index] \n",
    "\n",
    "    dnn.fit(kf_X_train, kf_y_train,batch_size=20, epochs=100, shuffle=True, verbose=1)  \n",
    "    \n",
    "    oof_train1[test_index] = dnn.predict(kf_X_test) \n",
    "    oof_test1 += dnn.predict(x_test) \n",
    "\n",
    "oof_test1 /= _N_FOLDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, X_train, y_train, X_test):\n",
    "    oof_train = np.zeros((X_train.shape[0], _N_CLASS))  \n",
    "    oof_test = np.empty((X_test.shape[0], _N_CLASS))  \n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train,y_train)):\n",
    "        kf_X_train = X_train[train_index]  \n",
    "        kf_y_train = y_train[train_index]   \n",
    "        kf_X_test = X_train[test_index]  \n",
    "\n",
    "        clf.fit(kf_X_train, kf_y_train) \n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test) \n",
    "        oof_test += clf.predict_proba(X_test) \n",
    "\n",
    "    oof_test /= _N_FOLDS  \n",
    "    return oof_train, oof_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 8.410513\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 8.756864\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 8.675249\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 8.710267\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 8.769672\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 8.706336\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 8.629455\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 8.083120\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 7.527162\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 7.453904\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 6.952784\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 7.004488\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 6.925089\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 6.765139\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 6.790122\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 6.460555\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 6.722475\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 6.533151\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 6.627659\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 6.625484\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 4.560243\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 5.633710\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 5.556095\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5.216882\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 4.711284\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 4.826027\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 4.574371\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3.968900\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4.037614\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 4.394606\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 4.493594\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 4.927036\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 4.569903\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 5.022556\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 5.215961\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 5.059712\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 5.316275\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 5.426783\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 5.821867\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 6.256321\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.969537\n",
      ">> Epoch 1 finished \tANN training loss 0.943050\n",
      ">> Epoch 2 finished \tANN training loss 0.858141\n",
      ">> Epoch 3 finished \tANN training loss 0.867319\n",
      ">> Epoch 4 finished \tANN training loss 0.777932\n",
      ">> Epoch 5 finished \tANN training loss 0.751352\n",
      ">> Epoch 6 finished \tANN training loss 0.760269\n",
      ">> Epoch 7 finished \tANN training loss 0.681601\n",
      ">> Epoch 8 finished \tANN training loss 0.647373\n",
      ">> Epoch 9 finished \tANN training loss 0.599634\n",
      ">> Epoch 10 finished \tANN training loss 0.584177\n",
      ">> Epoch 11 finished \tANN training loss 0.600534\n",
      ">> Epoch 12 finished \tANN training loss 0.583387\n",
      ">> Epoch 13 finished \tANN training loss 0.562627\n",
      ">> Epoch 14 finished \tANN training loss 0.520917\n",
      ">> Epoch 15 finished \tANN training loss 0.506815\n",
      ">> Epoch 16 finished \tANN training loss 0.478401\n",
      ">> Epoch 17 finished \tANN training loss 0.468684\n",
      ">> Epoch 18 finished \tANN training loss 0.474964\n",
      ">> Epoch 19 finished \tANN training loss 0.468438\n",
      ">> Epoch 20 finished \tANN training loss 0.440713\n",
      ">> Epoch 21 finished \tANN training loss 0.446713\n",
      ">> Epoch 22 finished \tANN training loss 0.428634\n",
      ">> Epoch 23 finished \tANN training loss 0.458760\n",
      ">> Epoch 24 finished \tANN training loss 0.416376\n",
      ">> Epoch 25 finished \tANN training loss 0.424589\n",
      ">> Epoch 26 finished \tANN training loss 0.414989\n",
      ">> Epoch 27 finished \tANN training loss 0.435133\n",
      ">> Epoch 28 finished \tANN training loss 0.409375\n",
      ">> Epoch 29 finished \tANN training loss 0.402373\n",
      ">> Epoch 30 finished \tANN training loss 0.410281\n",
      ">> Epoch 31 finished \tANN training loss 0.396355\n",
      ">> Epoch 32 finished \tANN training loss 0.386316\n",
      ">> Epoch 33 finished \tANN training loss 0.405374\n",
      ">> Epoch 34 finished \tANN training loss 0.395592\n",
      ">> Epoch 35 finished \tANN training loss 0.378709\n",
      ">> Epoch 36 finished \tANN training loss 0.368349\n",
      ">> Epoch 37 finished \tANN training loss 0.417686\n",
      ">> Epoch 38 finished \tANN training loss 0.361424\n",
      ">> Epoch 39 finished \tANN training loss 0.359692\n",
      ">> Epoch 40 finished \tANN training loss 0.445370\n",
      ">> Epoch 41 finished \tANN training loss 0.328443\n",
      ">> Epoch 42 finished \tANN training loss 0.313780\n",
      ">> Epoch 43 finished \tANN training loss 0.332707\n",
      ">> Epoch 44 finished \tANN training loss 0.313595\n",
      ">> Epoch 45 finished \tANN training loss 0.334868\n",
      ">> Epoch 46 finished \tANN training loss 0.655158\n",
      ">> Epoch 47 finished \tANN training loss 0.346370\n",
      ">> Epoch 48 finished \tANN training loss 0.325433\n",
      ">> Epoch 49 finished \tANN training loss 0.313042\n",
      ">> Epoch 50 finished \tANN training loss 0.311825\n",
      ">> Epoch 51 finished \tANN training loss 0.295239\n",
      ">> Epoch 52 finished \tANN training loss 0.306820\n",
      ">> Epoch 53 finished \tANN training loss 0.289640\n",
      ">> Epoch 54 finished \tANN training loss 0.366412\n",
      ">> Epoch 55 finished \tANN training loss 0.282234\n",
      ">> Epoch 56 finished \tANN training loss 0.274616\n",
      ">> Epoch 57 finished \tANN training loss 0.291861\n",
      ">> Epoch 58 finished \tANN training loss 0.276042\n",
      ">> Epoch 59 finished \tANN training loss 0.294371\n",
      ">> Epoch 60 finished \tANN training loss 0.366199\n",
      ">> Epoch 61 finished \tANN training loss 0.261533\n",
      ">> Epoch 62 finished \tANN training loss 0.267707\n",
      ">> Epoch 63 finished \tANN training loss 0.352685\n",
      ">> Epoch 64 finished \tANN training loss 0.298526\n",
      ">> Epoch 65 finished \tANN training loss 0.332116\n",
      ">> Epoch 66 finished \tANN training loss 0.262178\n",
      ">> Epoch 67 finished \tANN training loss 0.285974\n",
      ">> Epoch 68 finished \tANN training loss 0.253483\n",
      ">> Epoch 69 finished \tANN training loss 0.261606\n",
      ">> Epoch 70 finished \tANN training loss 0.300045\n",
      ">> Epoch 71 finished \tANN training loss 0.237515\n",
      ">> Epoch 72 finished \tANN training loss 0.310399\n",
      ">> Epoch 73 finished \tANN training loss 0.231185\n",
      ">> Epoch 74 finished \tANN training loss 0.210859\n",
      ">> Epoch 75 finished \tANN training loss 0.224961\n",
      ">> Epoch 76 finished \tANN training loss 0.250554\n",
      ">> Epoch 77 finished \tANN training loss 0.238691\n",
      ">> Epoch 78 finished \tANN training loss 0.273041\n",
      ">> Epoch 79 finished \tANN training loss 0.408386\n",
      ">> Epoch 80 finished \tANN training loss 0.237901\n",
      ">> Epoch 81 finished \tANN training loss 0.438934\n",
      ">> Epoch 82 finished \tANN training loss 0.231518\n",
      ">> Epoch 83 finished \tANN training loss 0.204689\n",
      ">> Epoch 84 finished \tANN training loss 0.209803\n",
      ">> Epoch 85 finished \tANN training loss 0.232852\n",
      ">> Epoch 86 finished \tANN training loss 0.265096\n",
      ">> Epoch 87 finished \tANN training loss 0.275056\n",
      ">> Epoch 88 finished \tANN training loss 0.221391\n",
      ">> Epoch 89 finished \tANN training loss 0.220684\n",
      ">> Epoch 90 finished \tANN training loss 0.218716\n",
      ">> Epoch 91 finished \tANN training loss 0.193947\n",
      ">> Epoch 92 finished \tANN training loss 0.188165\n",
      ">> Epoch 93 finished \tANN training loss 0.179289\n",
      ">> Epoch 94 finished \tANN training loss 0.232886\n",
      ">> Epoch 95 finished \tANN training loss 0.232510\n",
      ">> Epoch 96 finished \tANN training loss 0.181220\n",
      ">> Epoch 97 finished \tANN training loss 0.361031\n",
      ">> Epoch 98 finished \tANN training loss 0.251741\n",
      ">> Epoch 99 finished \tANN training loss 0.197044\n",
      ">> Epoch 100 finished \tANN training loss 0.187294\n",
      ">> Epoch 101 finished \tANN training loss 0.213690\n",
      ">> Epoch 102 finished \tANN training loss 0.261534\n",
      ">> Epoch 103 finished \tANN training loss 0.217093\n",
      ">> Epoch 104 finished \tANN training loss 0.163104\n",
      ">> Epoch 105 finished \tANN training loss 0.173422\n",
      ">> Epoch 106 finished \tANN training loss 0.197903\n",
      ">> Epoch 107 finished \tANN training loss 0.191209\n",
      ">> Epoch 108 finished \tANN training loss 0.187279\n",
      ">> Epoch 109 finished \tANN training loss 0.200663\n",
      ">> Epoch 110 finished \tANN training loss 0.192081\n",
      ">> Epoch 111 finished \tANN training loss 0.153607\n",
      ">> Epoch 112 finished \tANN training loss 0.178370\n",
      ">> Epoch 113 finished \tANN training loss 0.170474\n",
      ">> Epoch 114 finished \tANN training loss 0.162043\n",
      ">> Epoch 115 finished \tANN training loss 0.150971\n",
      ">> Epoch 116 finished \tANN training loss 0.141103\n",
      ">> Epoch 117 finished \tANN training loss 0.143384\n",
      ">> Epoch 118 finished \tANN training loss 0.165361\n",
      ">> Epoch 119 finished \tANN training loss 0.153113\n",
      ">> Epoch 120 finished \tANN training loss 0.177159\n",
      ">> Epoch 121 finished \tANN training loss 0.186195\n",
      ">> Epoch 122 finished \tANN training loss 0.169450\n",
      ">> Epoch 123 finished \tANN training loss 0.146558\n",
      ">> Epoch 124 finished \tANN training loss 0.153904\n",
      ">> Epoch 125 finished \tANN training loss 0.149967\n",
      ">> Epoch 126 finished \tANN training loss 0.204608\n",
      ">> Epoch 127 finished \tANN training loss 0.153360\n",
      ">> Epoch 128 finished \tANN training loss 0.175797\n",
      ">> Epoch 129 finished \tANN training loss 0.147784\n",
      ">> Epoch 130 finished \tANN training loss 0.154740\n",
      ">> Epoch 131 finished \tANN training loss 0.153382\n",
      ">> Epoch 132 finished \tANN training loss 0.372127\n",
      ">> Epoch 133 finished \tANN training loss 0.145808\n",
      ">> Epoch 134 finished \tANN training loss 0.143520\n",
      ">> Epoch 135 finished \tANN training loss 0.182574\n",
      ">> Epoch 136 finished \tANN training loss 0.121140\n",
      ">> Epoch 137 finished \tANN training loss 0.148859\n",
      ">> Epoch 138 finished \tANN training loss 0.142072\n",
      ">> Epoch 139 finished \tANN training loss 0.143538\n",
      ">> Epoch 140 finished \tANN training loss 0.143760\n",
      ">> Epoch 141 finished \tANN training loss 0.437773\n",
      ">> Epoch 142 finished \tANN training loss 0.154060\n",
      ">> Epoch 143 finished \tANN training loss 0.132950\n",
      ">> Epoch 144 finished \tANN training loss 0.138232\n",
      ">> Epoch 145 finished \tANN training loss 0.200436\n",
      ">> Epoch 146 finished \tANN training loss 0.119028\n",
      ">> Epoch 147 finished \tANN training loss 0.130173\n",
      ">> Epoch 148 finished \tANN training loss 0.124273\n",
      ">> Epoch 149 finished \tANN training loss 0.147419\n",
      ">> Epoch 150 finished \tANN training loss 0.140467\n",
      ">> Epoch 151 finished \tANN training loss 0.206491\n",
      ">> Epoch 152 finished \tANN training loss 0.144361\n",
      ">> Epoch 153 finished \tANN training loss 0.136081\n",
      ">> Epoch 154 finished \tANN training loss 0.154874\n",
      ">> Epoch 155 finished \tANN training loss 0.143942\n",
      ">> Epoch 156 finished \tANN training loss 0.216243\n",
      ">> Epoch 157 finished \tANN training loss 0.159960\n",
      ">> Epoch 158 finished \tANN training loss 0.124911\n",
      ">> Epoch 159 finished \tANN training loss 0.124684\n",
      ">> Epoch 160 finished \tANN training loss 0.143154\n",
      ">> Epoch 161 finished \tANN training loss 0.168388\n",
      ">> Epoch 162 finished \tANN training loss 0.208599\n",
      ">> Epoch 163 finished \tANN training loss 0.144767\n",
      ">> Epoch 164 finished \tANN training loss 0.203555\n",
      ">> Epoch 165 finished \tANN training loss 0.137815\n",
      ">> Epoch 166 finished \tANN training loss 0.120726\n",
      ">> Epoch 167 finished \tANN training loss 0.120947\n",
      ">> Epoch 168 finished \tANN training loss 0.199938\n",
      ">> Epoch 169 finished \tANN training loss 0.147515\n",
      ">> Epoch 170 finished \tANN training loss 0.139985\n",
      ">> Epoch 171 finished \tANN training loss 0.146859\n",
      ">> Epoch 172 finished \tANN training loss 0.126841\n",
      ">> Epoch 173 finished \tANN training loss 0.133932\n",
      ">> Epoch 174 finished \tANN training loss 0.136620\n",
      ">> Epoch 175 finished \tANN training loss 0.134559\n",
      ">> Epoch 176 finished \tANN training loss 0.127368\n",
      ">> Epoch 177 finished \tANN training loss 0.114681\n",
      ">> Epoch 178 finished \tANN training loss 0.098610\n",
      ">> Epoch 179 finished \tANN training loss 0.299439\n",
      ">> Epoch 180 finished \tANN training loss 0.121688\n",
      ">> Epoch 181 finished \tANN training loss 0.113866\n",
      ">> Epoch 182 finished \tANN training loss 0.118069\n",
      ">> Epoch 183 finished \tANN training loss 0.140668\n",
      ">> Epoch 184 finished \tANN training loss 0.135850\n",
      ">> Epoch 185 finished \tANN training loss 0.117325\n",
      ">> Epoch 186 finished \tANN training loss 0.109393\n",
      ">> Epoch 187 finished \tANN training loss 0.099369\n",
      ">> Epoch 188 finished \tANN training loss 0.128777\n",
      ">> Epoch 189 finished \tANN training loss 0.123260\n",
      ">> Epoch 190 finished \tANN training loss 0.119364\n",
      ">> Epoch 191 finished \tANN training loss 0.102226\n",
      ">> Epoch 192 finished \tANN training loss 0.107368\n",
      ">> Epoch 193 finished \tANN training loss 0.103064\n",
      ">> Epoch 194 finished \tANN training loss 0.106902\n",
      ">> Epoch 195 finished \tANN training loss 0.107864\n",
      ">> Epoch 196 finished \tANN training loss 0.103424\n",
      ">> Epoch 197 finished \tANN training loss 0.102138\n",
      ">> Epoch 198 finished \tANN training loss 0.114000\n",
      ">> Epoch 199 finished \tANN training loss 0.128735\n",
      ">> Epoch 200 finished \tANN training loss 0.095002\n",
      ">> Epoch 201 finished \tANN training loss 0.109035\n",
      ">> Epoch 202 finished \tANN training loss 0.098985\n",
      ">> Epoch 203 finished \tANN training loss 0.095572\n",
      ">> Epoch 204 finished \tANN training loss 0.101460\n",
      ">> Epoch 205 finished \tANN training loss 0.102215\n",
      ">> Epoch 206 finished \tANN training loss 0.144805\n",
      ">> Epoch 207 finished \tANN training loss 0.113929\n",
      ">> Epoch 208 finished \tANN training loss 0.093919\n",
      ">> Epoch 209 finished \tANN training loss 0.099228\n",
      ">> Epoch 210 finished \tANN training loss 0.112210\n",
      ">> Epoch 211 finished \tANN training loss 0.097370\n",
      ">> Epoch 212 finished \tANN training loss 0.172212\n",
      ">> Epoch 213 finished \tANN training loss 0.112924\n",
      ">> Epoch 214 finished \tANN training loss 0.106117\n",
      ">> Epoch 215 finished \tANN training loss 0.112903\n",
      ">> Epoch 216 finished \tANN training loss 0.111572\n",
      ">> Epoch 217 finished \tANN training loss 0.112802\n",
      ">> Epoch 218 finished \tANN training loss 0.108456\n",
      ">> Epoch 219 finished \tANN training loss 0.090401\n",
      ">> Epoch 220 finished \tANN training loss 0.343317\n",
      ">> Epoch 221 finished \tANN training loss 0.119505\n",
      ">> Epoch 222 finished \tANN training loss 0.113335\n",
      ">> Epoch 223 finished \tANN training loss 0.162744\n",
      ">> Epoch 224 finished \tANN training loss 0.127544\n",
      ">> Epoch 225 finished \tANN training loss 0.156842\n",
      ">> Epoch 226 finished \tANN training loss 0.221283\n",
      ">> Epoch 227 finished \tANN training loss 0.138485\n",
      ">> Epoch 228 finished \tANN training loss 0.108842\n",
      ">> Epoch 229 finished \tANN training loss 0.103420\n",
      ">> Epoch 230 finished \tANN training loss 0.215126\n",
      ">> Epoch 231 finished \tANN training loss 0.111590\n",
      ">> Epoch 232 finished \tANN training loss 0.121878\n",
      ">> Epoch 233 finished \tANN training loss 0.097904\n",
      ">> Epoch 234 finished \tANN training loss 0.097520\n",
      ">> Epoch 235 finished \tANN training loss 0.097725\n",
      ">> Epoch 236 finished \tANN training loss 0.102942\n",
      ">> Epoch 237 finished \tANN training loss 0.096361\n",
      ">> Epoch 238 finished \tANN training loss 0.101305\n",
      ">> Epoch 239 finished \tANN training loss 0.110373\n",
      ">> Epoch 240 finished \tANN training loss 0.107872\n",
      ">> Epoch 241 finished \tANN training loss 0.096251\n",
      ">> Epoch 242 finished \tANN training loss 0.101758\n",
      ">> Epoch 243 finished \tANN training loss 0.091968\n",
      ">> Epoch 244 finished \tANN training loss 0.087057\n",
      ">> Epoch 245 finished \tANN training loss 0.120512\n",
      ">> Epoch 246 finished \tANN training loss 0.111132\n",
      ">> Epoch 247 finished \tANN training loss 0.088154\n",
      ">> Epoch 248 finished \tANN training loss 0.079161\n",
      ">> Epoch 249 finished \tANN training loss 0.083774\n",
      ">> Epoch 250 finished \tANN training loss 0.072065\n",
      ">> Epoch 251 finished \tANN training loss 0.156293\n",
      ">> Epoch 252 finished \tANN training loss 0.105217\n",
      ">> Epoch 253 finished \tANN training loss 0.099087\n",
      ">> Epoch 254 finished \tANN training loss 0.134117\n",
      ">> Epoch 255 finished \tANN training loss 0.105986\n",
      ">> Epoch 256 finished \tANN training loss 0.093005\n",
      ">> Epoch 257 finished \tANN training loss 0.112723\n",
      ">> Epoch 258 finished \tANN training loss 0.103197\n",
      ">> Epoch 259 finished \tANN training loss 0.095779\n",
      ">> Epoch 260 finished \tANN training loss 0.112612\n",
      ">> Epoch 261 finished \tANN training loss 0.099427\n",
      ">> Epoch 262 finished \tANN training loss 0.098443\n",
      ">> Epoch 263 finished \tANN training loss 0.078578\n",
      ">> Epoch 264 finished \tANN training loss 0.084474\n",
      ">> Epoch 265 finished \tANN training loss 0.110982\n",
      ">> Epoch 266 finished \tANN training loss 0.087924\n",
      ">> Epoch 267 finished \tANN training loss 0.087672\n",
      ">> Epoch 268 finished \tANN training loss 0.087232\n",
      ">> Epoch 269 finished \tANN training loss 0.081494\n",
      ">> Epoch 270 finished \tANN training loss 0.116623\n",
      ">> Epoch 271 finished \tANN training loss 0.092846\n",
      ">> Epoch 272 finished \tANN training loss 0.153062\n",
      ">> Epoch 273 finished \tANN training loss 0.092835\n",
      ">> Epoch 274 finished \tANN training loss 0.093187\n",
      ">> Epoch 275 finished \tANN training loss 0.076682\n",
      ">> Epoch 276 finished \tANN training loss 0.085855\n",
      ">> Epoch 277 finished \tANN training loss 0.077056\n",
      ">> Epoch 278 finished \tANN training loss 0.070031\n",
      ">> Epoch 279 finished \tANN training loss 0.075099\n",
      ">> Epoch 280 finished \tANN training loss 0.104261\n",
      ">> Epoch 281 finished \tANN training loss 0.075825\n",
      ">> Epoch 282 finished \tANN training loss 0.154116\n",
      ">> Epoch 283 finished \tANN training loss 0.102267\n",
      ">> Epoch 284 finished \tANN training loss 0.095775\n",
      ">> Epoch 285 finished \tANN training loss 0.080977\n",
      ">> Epoch 286 finished \tANN training loss 0.079840\n",
      ">> Epoch 287 finished \tANN training loss 0.095443\n",
      ">> Epoch 288 finished \tANN training loss 0.120890\n",
      ">> Epoch 289 finished \tANN training loss 0.077713\n",
      ">> Epoch 290 finished \tANN training loss 0.087923\n",
      ">> Epoch 291 finished \tANN training loss 0.082823\n",
      ">> Epoch 292 finished \tANN training loss 0.075829\n",
      ">> Epoch 293 finished \tANN training loss 0.103261\n",
      ">> Epoch 294 finished \tANN training loss 0.076833\n",
      ">> Epoch 295 finished \tANN training loss 0.076794\n",
      ">> Epoch 296 finished \tANN training loss 0.077418\n",
      ">> Epoch 297 finished \tANN training loss 0.081359\n",
      ">> Epoch 298 finished \tANN training loss 0.075692\n",
      ">> Epoch 299 finished \tANN training loss 0.069308\n",
      ">> Epoch 300 finished \tANN training loss 0.091259\n",
      ">> Epoch 301 finished \tANN training loss 0.130533\n",
      ">> Epoch 302 finished \tANN training loss 0.084744\n",
      ">> Epoch 303 finished \tANN training loss 0.097089\n",
      ">> Epoch 304 finished \tANN training loss 0.091874\n",
      ">> Epoch 305 finished \tANN training loss 0.091803\n",
      ">> Epoch 306 finished \tANN training loss 0.073161\n",
      ">> Epoch 307 finished \tANN training loss 0.078246\n",
      ">> Epoch 308 finished \tANN training loss 0.079518\n",
      ">> Epoch 309 finished \tANN training loss 0.105738\n",
      ">> Epoch 310 finished \tANN training loss 0.076340\n",
      ">> Epoch 311 finished \tANN training loss 0.096813\n",
      ">> Epoch 312 finished \tANN training loss 0.105121\n",
      ">> Epoch 313 finished \tANN training loss 0.073243\n",
      ">> Epoch 314 finished \tANN training loss 0.115110\n",
      ">> Epoch 315 finished \tANN training loss 0.098925\n",
      ">> Epoch 316 finished \tANN training loss 0.093848\n",
      ">> Epoch 317 finished \tANN training loss 0.085953\n",
      ">> Epoch 318 finished \tANN training loss 0.077585\n",
      ">> Epoch 319 finished \tANN training loss 0.085589\n",
      ">> Epoch 320 finished \tANN training loss 0.079311\n",
      ">> Epoch 321 finished \tANN training loss 0.060896\n",
      ">> Epoch 322 finished \tANN training loss 0.082420\n",
      ">> Epoch 323 finished \tANN training loss 0.074076\n",
      ">> Epoch 324 finished \tANN training loss 0.089902\n",
      ">> Epoch 325 finished \tANN training loss 0.095242\n",
      ">> Epoch 326 finished \tANN training loss 0.088430\n",
      ">> Epoch 327 finished \tANN training loss 0.094450\n",
      ">> Epoch 328 finished \tANN training loss 0.100435\n",
      ">> Epoch 329 finished \tANN training loss 0.102789\n",
      ">> Epoch 330 finished \tANN training loss 0.090024\n",
      ">> Epoch 331 finished \tANN training loss 0.094336\n",
      ">> Epoch 332 finished \tANN training loss 0.089078\n",
      ">> Epoch 333 finished \tANN training loss 0.083425\n",
      ">> Epoch 334 finished \tANN training loss 0.171912\n",
      ">> Epoch 335 finished \tANN training loss 0.077007\n",
      ">> Epoch 336 finished \tANN training loss 0.084444\n",
      ">> Epoch 337 finished \tANN training loss 0.086096\n",
      ">> Epoch 338 finished \tANN training loss 0.105069\n",
      ">> Epoch 339 finished \tANN training loss 0.084276\n",
      ">> Epoch 340 finished \tANN training loss 0.087027\n",
      ">> Epoch 341 finished \tANN training loss 0.074622\n",
      ">> Epoch 342 finished \tANN training loss 0.102916\n",
      ">> Epoch 343 finished \tANN training loss 0.088675\n",
      ">> Epoch 344 finished \tANN training loss 0.096909\n",
      ">> Epoch 345 finished \tANN training loss 0.087472\n",
      ">> Epoch 346 finished \tANN training loss 0.085900\n",
      ">> Epoch 347 finished \tANN training loss 0.126399\n",
      ">> Epoch 348 finished \tANN training loss 0.101890\n",
      ">> Epoch 349 finished \tANN training loss 0.090379\n",
      ">> Epoch 350 finished \tANN training loss 0.108544\n",
      ">> Epoch 351 finished \tANN training loss 0.081760\n",
      ">> Epoch 352 finished \tANN training loss 0.085268\n",
      ">> Epoch 353 finished \tANN training loss 0.080590\n",
      ">> Epoch 354 finished \tANN training loss 0.069920\n",
      ">> Epoch 355 finished \tANN training loss 0.073341\n",
      ">> Epoch 356 finished \tANN training loss 0.105934\n",
      ">> Epoch 357 finished \tANN training loss 0.078783\n",
      ">> Epoch 358 finished \tANN training loss 0.068608\n",
      ">> Epoch 359 finished \tANN training loss 0.082126\n",
      ">> Epoch 360 finished \tANN training loss 0.086264\n",
      ">> Epoch 361 finished \tANN training loss 0.112681\n",
      ">> Epoch 362 finished \tANN training loss 0.071355\n",
      ">> Epoch 363 finished \tANN training loss 0.069182\n",
      ">> Epoch 364 finished \tANN training loss 0.082289\n",
      ">> Epoch 365 finished \tANN training loss 0.144586\n",
      ">> Epoch 366 finished \tANN training loss 0.072104\n",
      ">> Epoch 367 finished \tANN training loss 0.075463\n",
      ">> Epoch 368 finished \tANN training loss 0.080326\n",
      ">> Epoch 369 finished \tANN training loss 0.076172\n",
      ">> Epoch 370 finished \tANN training loss 0.082429\n",
      ">> Epoch 371 finished \tANN training loss 0.090488\n",
      ">> Epoch 372 finished \tANN training loss 0.096430\n",
      ">> Epoch 373 finished \tANN training loss 0.093086\n",
      ">> Epoch 374 finished \tANN training loss 0.090473\n",
      ">> Epoch 375 finished \tANN training loss 0.079010\n",
      ">> Epoch 376 finished \tANN training loss 0.089430\n",
      ">> Epoch 377 finished \tANN training loss 0.078068\n",
      ">> Epoch 378 finished \tANN training loss 0.065976\n",
      ">> Epoch 379 finished \tANN training loss 0.086356\n",
      ">> Epoch 380 finished \tANN training loss 0.074377\n",
      ">> Epoch 381 finished \tANN training loss 0.117862\n",
      ">> Epoch 382 finished \tANN training loss 0.080633\n",
      ">> Epoch 383 finished \tANN training loss 0.097652\n",
      ">> Epoch 384 finished \tANN training loss 0.079316\n",
      ">> Epoch 385 finished \tANN training loss 0.076706\n",
      ">> Epoch 386 finished \tANN training loss 0.081244\n",
      ">> Epoch 387 finished \tANN training loss 0.067841\n",
      ">> Epoch 388 finished \tANN training loss 0.064481\n",
      ">> Epoch 389 finished \tANN training loss 0.065366\n",
      ">> Epoch 390 finished \tANN training loss 0.185407\n",
      ">> Epoch 391 finished \tANN training loss 0.125219\n",
      ">> Epoch 392 finished \tANN training loss 0.083025\n",
      ">> Epoch 393 finished \tANN training loss 0.076903\n",
      ">> Epoch 394 finished \tANN training loss 0.068536\n",
      ">> Epoch 395 finished \tANN training loss 0.075566\n",
      ">> Epoch 396 finished \tANN training loss 0.086303\n",
      ">> Epoch 397 finished \tANN training loss 0.086345\n",
      ">> Epoch 398 finished \tANN training loss 0.067689\n",
      ">> Epoch 399 finished \tANN training loss 0.083062\n",
      ">> Epoch 400 finished \tANN training loss 0.065610\n",
      ">> Epoch 401 finished \tANN training loss 0.095735\n",
      ">> Epoch 402 finished \tANN training loss 0.086226\n",
      ">> Epoch 403 finished \tANN training loss 0.078632\n",
      ">> Epoch 404 finished \tANN training loss 0.104650\n",
      ">> Epoch 405 finished \tANN training loss 0.073439\n",
      ">> Epoch 406 finished \tANN training loss 0.076412\n",
      ">> Epoch 407 finished \tANN training loss 0.070548\n",
      ">> Epoch 408 finished \tANN training loss 0.090793\n",
      ">> Epoch 409 finished \tANN training loss 0.076207\n",
      ">> Epoch 410 finished \tANN training loss 0.077138\n",
      ">> Epoch 411 finished \tANN training loss 0.068835\n",
      ">> Epoch 412 finished \tANN training loss 0.073270\n",
      ">> Epoch 413 finished \tANN training loss 0.080867\n",
      ">> Epoch 414 finished \tANN training loss 0.076011\n",
      ">> Epoch 415 finished \tANN training loss 0.090265\n",
      ">> Epoch 416 finished \tANN training loss 0.071262\n",
      ">> Epoch 417 finished \tANN training loss 0.080282\n",
      ">> Epoch 418 finished \tANN training loss 0.074042\n",
      ">> Epoch 419 finished \tANN training loss 0.077741\n",
      ">> Epoch 420 finished \tANN training loss 0.076049\n",
      ">> Epoch 421 finished \tANN training loss 0.089901\n",
      ">> Epoch 422 finished \tANN training loss 0.084121\n",
      ">> Epoch 423 finished \tANN training loss 0.166795\n",
      ">> Epoch 424 finished \tANN training loss 0.094746\n",
      ">> Epoch 425 finished \tANN training loss 0.079222\n",
      ">> Epoch 426 finished \tANN training loss 0.065546\n",
      ">> Epoch 427 finished \tANN training loss 0.095212\n",
      ">> Epoch 428 finished \tANN training loss 0.071572\n",
      ">> Epoch 429 finished \tANN training loss 0.073900\n",
      ">> Epoch 430 finished \tANN training loss 0.066604\n",
      ">> Epoch 431 finished \tANN training loss 0.080117\n",
      ">> Epoch 432 finished \tANN training loss 0.069387\n",
      ">> Epoch 433 finished \tANN training loss 0.073860\n",
      ">> Epoch 434 finished \tANN training loss 0.060843\n",
      ">> Epoch 435 finished \tANN training loss 0.105780\n",
      ">> Epoch 436 finished \tANN training loss 0.072747\n",
      ">> Epoch 437 finished \tANN training loss 0.068631\n",
      ">> Epoch 438 finished \tANN training loss 0.056329\n",
      ">> Epoch 439 finished \tANN training loss 0.060843\n",
      ">> Epoch 440 finished \tANN training loss 0.069522\n",
      ">> Epoch 441 finished \tANN training loss 0.058248\n",
      ">> Epoch 442 finished \tANN training loss 0.063930\n",
      ">> Epoch 443 finished \tANN training loss 0.066748\n",
      ">> Epoch 444 finished \tANN training loss 0.077122\n",
      ">> Epoch 445 finished \tANN training loss 0.082305\n",
      ">> Epoch 446 finished \tANN training loss 0.101875\n",
      ">> Epoch 447 finished \tANN training loss 0.080840\n",
      ">> Epoch 448 finished \tANN training loss 0.066829\n",
      ">> Epoch 449 finished \tANN training loss 0.063143\n",
      ">> Epoch 450 finished \tANN training loss 0.058187\n",
      ">> Epoch 451 finished \tANN training loss 0.057925\n",
      ">> Epoch 452 finished \tANN training loss 0.066848\n",
      ">> Epoch 453 finished \tANN training loss 0.068098\n",
      ">> Epoch 454 finished \tANN training loss 0.075888\n",
      ">> Epoch 455 finished \tANN training loss 0.079013\n",
      ">> Epoch 456 finished \tANN training loss 0.125413\n",
      ">> Epoch 457 finished \tANN training loss 0.096884\n",
      ">> Epoch 458 finished \tANN training loss 0.074256\n",
      ">> Epoch 459 finished \tANN training loss 0.056649\n",
      ">> Epoch 460 finished \tANN training loss 0.071966\n",
      ">> Epoch 461 finished \tANN training loss 0.070963\n",
      ">> Epoch 462 finished \tANN training loss 0.099783\n",
      ">> Epoch 463 finished \tANN training loss 0.069646\n",
      ">> Epoch 464 finished \tANN training loss 0.105301\n",
      ">> Epoch 465 finished \tANN training loss 0.062669\n",
      ">> Epoch 466 finished \tANN training loss 0.073261\n",
      ">> Epoch 467 finished \tANN training loss 0.074057\n",
      ">> Epoch 468 finished \tANN training loss 0.059103\n",
      ">> Epoch 469 finished \tANN training loss 0.076481\n",
      ">> Epoch 470 finished \tANN training loss 0.067610\n",
      ">> Epoch 471 finished \tANN training loss 0.064835\n",
      ">> Epoch 472 finished \tANN training loss 0.062363\n",
      ">> Epoch 473 finished \tANN training loss 0.058546\n",
      ">> Epoch 474 finished \tANN training loss 0.067828\n",
      ">> Epoch 475 finished \tANN training loss 0.064853\n",
      ">> Epoch 476 finished \tANN training loss 0.070785\n",
      ">> Epoch 477 finished \tANN training loss 0.066670\n",
      ">> Epoch 478 finished \tANN training loss 0.067129\n",
      ">> Epoch 479 finished \tANN training loss 0.070925\n",
      ">> Epoch 480 finished \tANN training loss 0.080434\n",
      ">> Epoch 481 finished \tANN training loss 0.147345\n",
      ">> Epoch 482 finished \tANN training loss 0.081248\n",
      ">> Epoch 483 finished \tANN training loss 0.074320\n",
      ">> Epoch 484 finished \tANN training loss 0.078743\n",
      ">> Epoch 485 finished \tANN training loss 0.071518\n",
      ">> Epoch 486 finished \tANN training loss 0.078319\n",
      ">> Epoch 487 finished \tANN training loss 0.081307\n",
      ">> Epoch 488 finished \tANN training loss 0.073109\n",
      ">> Epoch 489 finished \tANN training loss 0.071950\n",
      ">> Epoch 490 finished \tANN training loss 0.073413\n",
      ">> Epoch 491 finished \tANN training loss 0.069563\n",
      ">> Epoch 492 finished \tANN training loss 0.075166\n",
      ">> Epoch 493 finished \tANN training loss 0.062626\n",
      ">> Epoch 494 finished \tANN training loss 0.075466\n",
      ">> Epoch 495 finished \tANN training loss 0.065684\n",
      ">> Epoch 496 finished \tANN training loss 0.066230\n",
      ">> Epoch 497 finished \tANN training loss 0.072852\n",
      ">> Epoch 498 finished \tANN training loss 0.067672\n",
      ">> Epoch 499 finished \tANN training loss 0.079781\n",
      ">> Epoch 500 finished \tANN training loss 0.080568\n",
      ">> Epoch 501 finished \tANN training loss 0.064334\n",
      ">> Epoch 502 finished \tANN training loss 0.077717\n",
      ">> Epoch 503 finished \tANN training loss 0.070038\n",
      ">> Epoch 504 finished \tANN training loss 0.061960\n",
      ">> Epoch 505 finished \tANN training loss 0.069560\n",
      ">> Epoch 506 finished \tANN training loss 0.077948\n",
      ">> Epoch 507 finished \tANN training loss 0.138420\n",
      ">> Epoch 508 finished \tANN training loss 0.065835\n",
      ">> Epoch 509 finished \tANN training loss 0.085166\n",
      ">> Epoch 510 finished \tANN training loss 0.115267\n",
      ">> Epoch 511 finished \tANN training loss 0.071194\n",
      ">> Epoch 512 finished \tANN training loss 0.066672\n",
      ">> Epoch 513 finished \tANN training loss 0.067539\n",
      ">> Epoch 514 finished \tANN training loss 0.066409\n",
      ">> Epoch 515 finished \tANN training loss 0.064340\n",
      ">> Epoch 516 finished \tANN training loss 0.061585\n",
      ">> Epoch 517 finished \tANN training loss 0.063026\n",
      ">> Epoch 518 finished \tANN training loss 0.060826\n",
      ">> Epoch 519 finished \tANN training loss 0.066317\n",
      ">> Epoch 520 finished \tANN training loss 0.063270\n",
      ">> Epoch 521 finished \tANN training loss 0.106389\n",
      ">> Epoch 522 finished \tANN training loss 0.080096\n",
      ">> Epoch 523 finished \tANN training loss 0.067234\n",
      ">> Epoch 524 finished \tANN training loss 0.066101\n",
      ">> Epoch 525 finished \tANN training loss 0.071937\n",
      ">> Epoch 526 finished \tANN training loss 0.087707\n",
      ">> Epoch 527 finished \tANN training loss 0.073639\n",
      ">> Epoch 528 finished \tANN training loss 0.138773\n",
      ">> Epoch 529 finished \tANN training loss 0.065898\n",
      ">> Epoch 530 finished \tANN training loss 0.077225\n",
      ">> Epoch 531 finished \tANN training loss 0.137071\n",
      ">> Epoch 532 finished \tANN training loss 0.080645\n",
      ">> Epoch 533 finished \tANN training loss 0.071806\n",
      ">> Epoch 534 finished \tANN training loss 0.084612\n",
      ">> Epoch 535 finished \tANN training loss 0.073882\n",
      ">> Epoch 536 finished \tANN training loss 0.072485\n",
      ">> Epoch 537 finished \tANN training loss 0.058006\n",
      ">> Epoch 538 finished \tANN training loss 0.055284\n",
      ">> Epoch 539 finished \tANN training loss 0.066833\n",
      ">> Epoch 540 finished \tANN training loss 0.066459\n",
      ">> Epoch 541 finished \tANN training loss 0.072162\n",
      ">> Epoch 542 finished \tANN training loss 0.059197\n",
      ">> Epoch 543 finished \tANN training loss 0.082620\n",
      ">> Epoch 544 finished \tANN training loss 0.067427\n",
      ">> Epoch 545 finished \tANN training loss 0.066776\n",
      ">> Epoch 546 finished \tANN training loss 0.058889\n",
      ">> Epoch 547 finished \tANN training loss 0.064846\n",
      ">> Epoch 548 finished \tANN training loss 0.073523\n",
      ">> Epoch 549 finished \tANN training loss 0.063451\n",
      ">> Epoch 550 finished \tANN training loss 0.060499\n",
      ">> Epoch 551 finished \tANN training loss 0.064878\n",
      ">> Epoch 552 finished \tANN training loss 0.071879\n",
      ">> Epoch 553 finished \tANN training loss 0.077459\n",
      ">> Epoch 554 finished \tANN training loss 0.064764\n",
      ">> Epoch 555 finished \tANN training loss 0.064880\n",
      ">> Epoch 556 finished \tANN training loss 0.067564\n",
      ">> Epoch 557 finished \tANN training loss 0.090297\n",
      ">> Epoch 558 finished \tANN training loss 0.066403\n",
      ">> Epoch 559 finished \tANN training loss 0.090854\n",
      ">> Epoch 560 finished \tANN training loss 0.102215\n",
      ">> Epoch 561 finished \tANN training loss 0.100784\n",
      ">> Epoch 562 finished \tANN training loss 0.076729\n",
      ">> Epoch 563 finished \tANN training loss 0.067866\n",
      ">> Epoch 564 finished \tANN training loss 0.062276\n",
      ">> Epoch 565 finished \tANN training loss 0.073842\n",
      ">> Epoch 566 finished \tANN training loss 0.057119\n",
      ">> Epoch 567 finished \tANN training loss 0.074989\n",
      ">> Epoch 568 finished \tANN training loss 0.153962\n",
      ">> Epoch 569 finished \tANN training loss 0.065786\n",
      ">> Epoch 570 finished \tANN training loss 0.073621\n",
      ">> Epoch 571 finished \tANN training loss 0.073228\n",
      ">> Epoch 572 finished \tANN training loss 0.062645\n",
      ">> Epoch 573 finished \tANN training loss 0.088913\n",
      ">> Epoch 574 finished \tANN training loss 0.070770\n",
      ">> Epoch 575 finished \tANN training loss 0.150350\n",
      ">> Epoch 576 finished \tANN training loss 0.083659\n",
      ">> Epoch 577 finished \tANN training loss 0.078866\n",
      ">> Epoch 578 finished \tANN training loss 0.072758\n",
      ">> Epoch 579 finished \tANN training loss 0.067395\n",
      ">> Epoch 580 finished \tANN training loss 0.062127\n",
      ">> Epoch 581 finished \tANN training loss 0.063053\n",
      ">> Epoch 582 finished \tANN training loss 0.060038\n",
      ">> Epoch 583 finished \tANN training loss 0.060451\n",
      ">> Epoch 584 finished \tANN training loss 0.066180\n",
      ">> Epoch 585 finished \tANN training loss 0.068467\n",
      ">> Epoch 586 finished \tANN training loss 0.061045\n",
      ">> Epoch 587 finished \tANN training loss 0.063751\n",
      ">> Epoch 588 finished \tANN training loss 0.058053\n",
      ">> Epoch 589 finished \tANN training loss 0.079272\n",
      ">> Epoch 590 finished \tANN training loss 0.075692\n",
      ">> Epoch 591 finished \tANN training loss 0.061752\n",
      ">> Epoch 592 finished \tANN training loss 0.081732\n",
      ">> Epoch 593 finished \tANN training loss 0.079124\n",
      ">> Epoch 594 finished \tANN training loss 0.146903\n",
      ">> Epoch 595 finished \tANN training loss 0.094943\n",
      ">> Epoch 596 finished \tANN training loss 0.084278\n",
      ">> Epoch 597 finished \tANN training loss 0.068655\n",
      ">> Epoch 598 finished \tANN training loss 0.075965\n",
      ">> Epoch 599 finished \tANN training loss 0.088485\n",
      ">> Epoch 600 finished \tANN training loss 0.123555\n",
      ">> Epoch 601 finished \tANN training loss 0.065658\n",
      ">> Epoch 602 finished \tANN training loss 0.087071\n",
      ">> Epoch 603 finished \tANN training loss 0.068645\n",
      ">> Epoch 604 finished \tANN training loss 0.062794\n",
      ">> Epoch 605 finished \tANN training loss 0.071740\n",
      ">> Epoch 606 finished \tANN training loss 0.065908\n",
      ">> Epoch 607 finished \tANN training loss 0.066176\n",
      ">> Epoch 608 finished \tANN training loss 0.061759\n",
      ">> Epoch 609 finished \tANN training loss 0.068734\n",
      ">> Epoch 610 finished \tANN training loss 0.062070\n",
      ">> Epoch 611 finished \tANN training loss 0.118561\n",
      ">> Epoch 612 finished \tANN training loss 0.067710\n",
      ">> Epoch 613 finished \tANN training loss 0.069293\n",
      ">> Epoch 614 finished \tANN training loss 0.058773\n",
      ">> Epoch 615 finished \tANN training loss 0.057914\n",
      ">> Epoch 616 finished \tANN training loss 0.060853\n",
      ">> Epoch 617 finished \tANN training loss 0.059910\n",
      ">> Epoch 618 finished \tANN training loss 0.067257\n",
      ">> Epoch 619 finished \tANN training loss 0.068419\n",
      ">> Epoch 620 finished \tANN training loss 0.062733\n",
      ">> Epoch 621 finished \tANN training loss 0.057679\n",
      ">> Epoch 622 finished \tANN training loss 0.077388\n",
      ">> Epoch 623 finished \tANN training loss 0.094956\n",
      ">> Epoch 624 finished \tANN training loss 0.081326\n",
      ">> Epoch 625 finished \tANN training loss 0.063786\n",
      ">> Epoch 626 finished \tANN training loss 0.055379\n",
      ">> Epoch 627 finished \tANN training loss 0.061112\n",
      ">> Epoch 628 finished \tANN training loss 0.068752\n",
      ">> Epoch 629 finished \tANN training loss 0.061870\n",
      ">> Epoch 630 finished \tANN training loss 0.069503\n",
      ">> Epoch 631 finished \tANN training loss 0.069973\n",
      ">> Epoch 632 finished \tANN training loss 0.063877\n",
      ">> Epoch 633 finished \tANN training loss 0.052265\n",
      ">> Epoch 634 finished \tANN training loss 0.061586\n",
      ">> Epoch 635 finished \tANN training loss 0.056256\n",
      ">> Epoch 636 finished \tANN training loss 0.086852\n",
      ">> Epoch 637 finished \tANN training loss 0.056689\n",
      ">> Epoch 638 finished \tANN training loss 0.053470\n",
      ">> Epoch 639 finished \tANN training loss 0.062342\n",
      ">> Epoch 640 finished \tANN training loss 0.082043\n",
      ">> Epoch 641 finished \tANN training loss 0.057049\n",
      ">> Epoch 642 finished \tANN training loss 0.063274\n",
      ">> Epoch 643 finished \tANN training loss 0.067530\n",
      ">> Epoch 644 finished \tANN training loss 0.073290\n",
      ">> Epoch 645 finished \tANN training loss 0.083683\n",
      ">> Epoch 646 finished \tANN training loss 0.067476\n",
      ">> Epoch 647 finished \tANN training loss 0.063881\n",
      ">> Epoch 648 finished \tANN training loss 0.067149\n",
      ">> Epoch 649 finished \tANN training loss 0.066883\n",
      ">> Epoch 650 finished \tANN training loss 0.055567\n",
      ">> Epoch 651 finished \tANN training loss 0.050376\n",
      ">> Epoch 652 finished \tANN training loss 0.052164\n",
      ">> Epoch 653 finished \tANN training loss 0.058265\n",
      ">> Epoch 654 finished \tANN training loss 0.048071\n",
      ">> Epoch 655 finished \tANN training loss 0.049686\n",
      ">> Epoch 656 finished \tANN training loss 0.053599\n",
      ">> Epoch 657 finished \tANN training loss 0.061683\n",
      ">> Epoch 658 finished \tANN training loss 0.083034\n",
      ">> Epoch 659 finished \tANN training loss 0.087555\n",
      ">> Epoch 660 finished \tANN training loss 0.052705\n",
      ">> Epoch 661 finished \tANN training loss 0.062272\n",
      ">> Epoch 662 finished \tANN training loss 0.057067\n",
      ">> Epoch 663 finished \tANN training loss 0.056185\n",
      ">> Epoch 664 finished \tANN training loss 0.066802\n",
      ">> Epoch 665 finished \tANN training loss 0.063386\n",
      ">> Epoch 666 finished \tANN training loss 0.051097\n",
      ">> Epoch 667 finished \tANN training loss 0.056048\n",
      ">> Epoch 668 finished \tANN training loss 0.054996\n",
      ">> Epoch 669 finished \tANN training loss 0.064759\n",
      ">> Epoch 670 finished \tANN training loss 0.058022\n",
      ">> Epoch 671 finished \tANN training loss 0.052410\n",
      ">> Epoch 672 finished \tANN training loss 0.061353\n",
      ">> Epoch 673 finished \tANN training loss 0.081340\n",
      ">> Epoch 674 finished \tANN training loss 0.055983\n",
      ">> Epoch 675 finished \tANN training loss 0.057966\n",
      ">> Epoch 676 finished \tANN training loss 0.048098\n",
      ">> Epoch 677 finished \tANN training loss 0.056397\n",
      ">> Epoch 678 finished \tANN training loss 0.054042\n",
      ">> Epoch 679 finished \tANN training loss 0.053463\n",
      ">> Epoch 680 finished \tANN training loss 0.062308\n",
      ">> Epoch 681 finished \tANN training loss 0.053710\n",
      ">> Epoch 682 finished \tANN training loss 0.066125\n",
      ">> Epoch 683 finished \tANN training loss 0.084407\n",
      ">> Epoch 684 finished \tANN training loss 0.062931\n",
      ">> Epoch 685 finished \tANN training loss 0.058080\n",
      ">> Epoch 686 finished \tANN training loss 0.077806\n",
      ">> Epoch 687 finished \tANN training loss 0.067695\n",
      ">> Epoch 688 finished \tANN training loss 0.052654\n",
      ">> Epoch 689 finished \tANN training loss 0.065541\n",
      ">> Epoch 690 finished \tANN training loss 0.062261\n",
      ">> Epoch 691 finished \tANN training loss 0.074195\n",
      ">> Epoch 692 finished \tANN training loss 0.068263\n",
      ">> Epoch 693 finished \tANN training loss 0.092999\n",
      ">> Epoch 694 finished \tANN training loss 0.064562\n",
      ">> Epoch 695 finished \tANN training loss 0.060151\n",
      ">> Epoch 696 finished \tANN training loss 0.058540\n",
      ">> Epoch 697 finished \tANN training loss 0.065162\n",
      ">> Epoch 698 finished \tANN training loss 0.062412\n",
      ">> Epoch 699 finished \tANN training loss 0.060261\n",
      ">> Epoch 700 finished \tANN training loss 0.057411\n",
      ">> Epoch 701 finished \tANN training loss 0.070924\n",
      ">> Epoch 702 finished \tANN training loss 0.083713\n",
      ">> Epoch 703 finished \tANN training loss 0.064858\n",
      ">> Epoch 704 finished \tANN training loss 0.055281\n",
      ">> Epoch 705 finished \tANN training loss 0.067262\n",
      ">> Epoch 706 finished \tANN training loss 0.054405\n",
      ">> Epoch 707 finished \tANN training loss 0.054482\n",
      ">> Epoch 708 finished \tANN training loss 0.056744\n",
      ">> Epoch 709 finished \tANN training loss 0.054959\n",
      ">> Epoch 710 finished \tANN training loss 0.066303\n",
      ">> Epoch 711 finished \tANN training loss 0.074067\n",
      ">> Epoch 712 finished \tANN training loss 0.062794\n",
      ">> Epoch 713 finished \tANN training loss 0.063347\n",
      ">> Epoch 714 finished \tANN training loss 0.059670\n",
      ">> Epoch 715 finished \tANN training loss 0.063952\n",
      ">> Epoch 716 finished \tANN training loss 0.057833\n",
      ">> Epoch 717 finished \tANN training loss 0.058889\n",
      ">> Epoch 718 finished \tANN training loss 0.062602\n",
      ">> Epoch 719 finished \tANN training loss 0.058955\n",
      ">> Epoch 720 finished \tANN training loss 0.057103\n",
      ">> Epoch 721 finished \tANN training loss 0.057529\n",
      ">> Epoch 722 finished \tANN training loss 0.061115\n",
      ">> Epoch 723 finished \tANN training loss 0.051058\n",
      ">> Epoch 724 finished \tANN training loss 0.055448\n",
      ">> Epoch 725 finished \tANN training loss 0.051199\n",
      ">> Epoch 726 finished \tANN training loss 0.055734\n",
      ">> Epoch 727 finished \tANN training loss 0.095799\n",
      ">> Epoch 728 finished \tANN training loss 0.052150\n",
      ">> Epoch 729 finished \tANN training loss 0.055639\n",
      ">> Epoch 730 finished \tANN training loss 0.058113\n",
      ">> Epoch 731 finished \tANN training loss 0.062598\n",
      ">> Epoch 732 finished \tANN training loss 0.059960\n",
      ">> Epoch 733 finished \tANN training loss 0.061744\n",
      ">> Epoch 734 finished \tANN training loss 0.059315\n",
      ">> Epoch 735 finished \tANN training loss 0.062332\n",
      ">> Epoch 736 finished \tANN training loss 0.082263\n",
      ">> Epoch 737 finished \tANN training loss 0.079829\n",
      ">> Epoch 738 finished \tANN training loss 0.076429\n",
      ">> Epoch 739 finished \tANN training loss 0.082901\n",
      ">> Epoch 740 finished \tANN training loss 0.064612\n",
      ">> Epoch 741 finished \tANN training loss 0.057610\n",
      ">> Epoch 742 finished \tANN training loss 0.058336\n",
      ">> Epoch 743 finished \tANN training loss 0.053512\n",
      ">> Epoch 744 finished \tANN training loss 0.056781\n",
      ">> Epoch 745 finished \tANN training loss 0.057756\n",
      ">> Epoch 746 finished \tANN training loss 0.059685\n",
      ">> Epoch 747 finished \tANN training loss 0.055059\n",
      ">> Epoch 748 finished \tANN training loss 0.050817\n",
      ">> Epoch 749 finished \tANN training loss 0.129809\n",
      ">> Epoch 750 finished \tANN training loss 0.066776\n",
      ">> Epoch 751 finished \tANN training loss 0.063411\n",
      ">> Epoch 752 finished \tANN training loss 0.061322\n",
      ">> Epoch 753 finished \tANN training loss 0.069728\n",
      ">> Epoch 754 finished \tANN training loss 0.058645\n",
      ">> Epoch 755 finished \tANN training loss 0.060006\n",
      ">> Epoch 756 finished \tANN training loss 0.061725\n",
      ">> Epoch 757 finished \tANN training loss 0.060402\n",
      ">> Epoch 758 finished \tANN training loss 0.061379\n",
      ">> Epoch 759 finished \tANN training loss 0.057119\n",
      ">> Epoch 760 finished \tANN training loss 0.058166\n",
      ">> Epoch 761 finished \tANN training loss 0.089215\n",
      ">> Epoch 762 finished \tANN training loss 0.071529\n",
      ">> Epoch 763 finished \tANN training loss 0.091535\n",
      ">> Epoch 764 finished \tANN training loss 0.058214\n",
      ">> Epoch 765 finished \tANN training loss 0.057442\n",
      ">> Epoch 766 finished \tANN training loss 0.053845\n",
      ">> Epoch 767 finished \tANN training loss 0.071330\n",
      ">> Epoch 768 finished \tANN training loss 0.060930\n",
      ">> Epoch 769 finished \tANN training loss 0.060063\n",
      ">> Epoch 770 finished \tANN training loss 0.060204\n",
      ">> Epoch 771 finished \tANN training loss 0.050803\n",
      ">> Epoch 772 finished \tANN training loss 0.050638\n",
      ">> Epoch 773 finished \tANN training loss 0.047969\n",
      ">> Epoch 774 finished \tANN training loss 0.054105\n",
      ">> Epoch 775 finished \tANN training loss 0.071784\n",
      ">> Epoch 776 finished \tANN training loss 0.069316\n",
      ">> Epoch 777 finished \tANN training loss 0.066962\n",
      ">> Epoch 778 finished \tANN training loss 0.080277\n",
      ">> Epoch 779 finished \tANN training loss 0.068422\n",
      ">> Epoch 780 finished \tANN training loss 0.059414\n",
      ">> Epoch 781 finished \tANN training loss 0.063375\n",
      ">> Epoch 782 finished \tANN training loss 0.059880\n",
      ">> Epoch 783 finished \tANN training loss 0.057192\n",
      ">> Epoch 784 finished \tANN training loss 0.052992\n",
      ">> Epoch 785 finished \tANN training loss 0.050261\n",
      ">> Epoch 786 finished \tANN training loss 0.047246\n",
      ">> Epoch 787 finished \tANN training loss 0.092597\n",
      ">> Epoch 788 finished \tANN training loss 0.053278\n",
      ">> Epoch 789 finished \tANN training loss 0.048326\n",
      ">> Epoch 790 finished \tANN training loss 0.056400\n",
      ">> Epoch 791 finished \tANN training loss 0.061399\n",
      ">> Epoch 792 finished \tANN training loss 0.064487\n",
      ">> Epoch 793 finished \tANN training loss 0.051939\n",
      ">> Epoch 794 finished \tANN training loss 0.049914\n",
      ">> Epoch 795 finished \tANN training loss 0.056519\n",
      ">> Epoch 796 finished \tANN training loss 0.056702\n",
      ">> Epoch 797 finished \tANN training loss 0.054959\n",
      ">> Epoch 798 finished \tANN training loss 0.056323\n",
      ">> Epoch 799 finished \tANN training loss 0.056912\n",
      ">> Epoch 800 finished \tANN training loss 0.061629\n",
      ">> Epoch 801 finished \tANN training loss 0.051039\n",
      ">> Epoch 802 finished \tANN training loss 0.062037\n",
      ">> Epoch 803 finished \tANN training loss 0.056188\n",
      ">> Epoch 804 finished \tANN training loss 0.050744\n",
      ">> Epoch 805 finished \tANN training loss 0.052829\n",
      ">> Epoch 806 finished \tANN training loss 0.050967\n",
      ">> Epoch 807 finished \tANN training loss 0.056216\n",
      ">> Epoch 808 finished \tANN training loss 0.052473\n",
      ">> Epoch 809 finished \tANN training loss 0.054434\n",
      ">> Epoch 810 finished \tANN training loss 0.051653\n",
      ">> Epoch 811 finished \tANN training loss 0.051526\n",
      ">> Epoch 812 finished \tANN training loss 0.093029\n",
      ">> Epoch 813 finished \tANN training loss 0.102615\n",
      ">> Epoch 814 finished \tANN training loss 0.065930\n",
      ">> Epoch 815 finished \tANN training loss 0.059395\n",
      ">> Epoch 816 finished \tANN training loss 0.061118\n",
      ">> Epoch 817 finished \tANN training loss 0.061458\n",
      ">> Epoch 818 finished \tANN training loss 0.054196\n",
      ">> Epoch 819 finished \tANN training loss 0.061649\n",
      ">> Epoch 820 finished \tANN training loss 0.070647\n",
      ">> Epoch 821 finished \tANN training loss 0.071572\n",
      ">> Epoch 822 finished \tANN training loss 0.060846\n",
      ">> Epoch 823 finished \tANN training loss 0.064023\n",
      ">> Epoch 824 finished \tANN training loss 0.062533\n",
      ">> Epoch 825 finished \tANN training loss 0.060965\n",
      ">> Epoch 826 finished \tANN training loss 0.080700\n",
      ">> Epoch 827 finished \tANN training loss 0.066234\n",
      ">> Epoch 828 finished \tANN training loss 0.064898\n",
      ">> Epoch 829 finished \tANN training loss 0.056109\n",
      ">> Epoch 830 finished \tANN training loss 0.055596\n",
      ">> Epoch 831 finished \tANN training loss 0.065091\n",
      ">> Epoch 832 finished \tANN training loss 0.054497\n",
      ">> Epoch 833 finished \tANN training loss 0.053070\n",
      ">> Epoch 834 finished \tANN training loss 0.056515\n",
      ">> Epoch 835 finished \tANN training loss 0.066733\n",
      ">> Epoch 836 finished \tANN training loss 0.052172\n",
      ">> Epoch 837 finished \tANN training loss 0.056217\n",
      ">> Epoch 838 finished \tANN training loss 0.048428\n",
      ">> Epoch 839 finished \tANN training loss 0.094812\n",
      ">> Epoch 840 finished \tANN training loss 0.058146\n",
      ">> Epoch 841 finished \tANN training loss 0.063973\n",
      ">> Epoch 842 finished \tANN training loss 0.064705\n",
      ">> Epoch 843 finished \tANN training loss 0.055832\n",
      ">> Epoch 844 finished \tANN training loss 0.056412\n",
      ">> Epoch 845 finished \tANN training loss 0.058207\n",
      ">> Epoch 846 finished \tANN training loss 0.053598\n",
      ">> Epoch 847 finished \tANN training loss 0.143104\n",
      ">> Epoch 848 finished \tANN training loss 0.057919\n",
      ">> Epoch 849 finished \tANN training loss 0.065122\n",
      ">> Epoch 850 finished \tANN training loss 0.064984\n",
      ">> Epoch 851 finished \tANN training loss 0.080107\n",
      ">> Epoch 852 finished \tANN training loss 0.066606\n",
      ">> Epoch 853 finished \tANN training loss 0.061760\n",
      ">> Epoch 854 finished \tANN training loss 0.064774\n",
      ">> Epoch 855 finished \tANN training loss 0.063801\n",
      ">> Epoch 856 finished \tANN training loss 0.123311\n",
      ">> Epoch 857 finished \tANN training loss 0.066617\n",
      ">> Epoch 858 finished \tANN training loss 0.062499\n",
      ">> Epoch 859 finished \tANN training loss 0.074902\n",
      ">> Epoch 860 finished \tANN training loss 0.091812\n",
      ">> Epoch 861 finished \tANN training loss 0.071278\n",
      ">> Epoch 862 finished \tANN training loss 0.064396\n",
      ">> Epoch 863 finished \tANN training loss 0.062332\n",
      ">> Epoch 864 finished \tANN training loss 0.063668\n",
      ">> Epoch 865 finished \tANN training loss 0.065117\n",
      ">> Epoch 866 finished \tANN training loss 0.058363\n",
      ">> Epoch 867 finished \tANN training loss 0.079395\n",
      ">> Epoch 868 finished \tANN training loss 0.062214\n",
      ">> Epoch 869 finished \tANN training loss 0.103923\n",
      ">> Epoch 870 finished \tANN training loss 0.068266\n",
      ">> Epoch 871 finished \tANN training loss 0.060806\n",
      ">> Epoch 872 finished \tANN training loss 0.059081\n",
      ">> Epoch 873 finished \tANN training loss 0.074927\n",
      ">> Epoch 874 finished \tANN training loss 0.081187\n",
      ">> Epoch 875 finished \tANN training loss 0.064661\n",
      ">> Epoch 876 finished \tANN training loss 0.080149\n",
      ">> Epoch 877 finished \tANN training loss 0.070245\n",
      ">> Epoch 878 finished \tANN training loss 0.059423\n",
      ">> Epoch 879 finished \tANN training loss 0.059911\n",
      ">> Epoch 880 finished \tANN training loss 0.075605\n",
      ">> Epoch 881 finished \tANN training loss 0.054886\n",
      ">> Epoch 882 finished \tANN training loss 0.059639\n",
      ">> Epoch 883 finished \tANN training loss 0.071669\n",
      ">> Epoch 884 finished \tANN training loss 0.071992\n",
      ">> Epoch 885 finished \tANN training loss 0.084629\n",
      ">> Epoch 886 finished \tANN training loss 0.065257\n",
      ">> Epoch 887 finished \tANN training loss 0.058586\n",
      ">> Epoch 888 finished \tANN training loss 0.062988\n",
      ">> Epoch 889 finished \tANN training loss 0.066289\n",
      ">> Epoch 890 finished \tANN training loss 0.073106\n",
      ">> Epoch 891 finished \tANN training loss 0.069374\n",
      ">> Epoch 892 finished \tANN training loss 0.060665\n",
      ">> Epoch 893 finished \tANN training loss 0.067781\n",
      ">> Epoch 894 finished \tANN training loss 0.060718\n",
      ">> Epoch 895 finished \tANN training loss 0.056504\n",
      ">> Epoch 896 finished \tANN training loss 0.053175\n",
      ">> Epoch 897 finished \tANN training loss 0.074804\n",
      ">> Epoch 898 finished \tANN training loss 0.052876\n",
      ">> Epoch 899 finished \tANN training loss 0.053691\n",
      ">> Epoch 900 finished \tANN training loss 0.090734\n",
      ">> Epoch 901 finished \tANN training loss 0.054217\n",
      ">> Epoch 902 finished \tANN training loss 0.058890\n",
      ">> Epoch 903 finished \tANN training loss 0.056753\n",
      ">> Epoch 904 finished \tANN training loss 0.065624\n",
      ">> Epoch 905 finished \tANN training loss 0.056793\n",
      ">> Epoch 906 finished \tANN training loss 0.056778\n",
      ">> Epoch 907 finished \tANN training loss 0.059702\n",
      ">> Epoch 908 finished \tANN training loss 0.056474\n",
      ">> Epoch 909 finished \tANN training loss 0.057929\n",
      ">> Epoch 910 finished \tANN training loss 0.056426\n",
      ">> Epoch 911 finished \tANN training loss 0.068724\n",
      ">> Epoch 912 finished \tANN training loss 0.055438\n",
      ">> Epoch 913 finished \tANN training loss 0.049801\n",
      ">> Epoch 914 finished \tANN training loss 0.053590\n",
      ">> Epoch 915 finished \tANN training loss 0.067048\n",
      ">> Epoch 916 finished \tANN training loss 0.065709\n",
      ">> Epoch 917 finished \tANN training loss 0.057176\n",
      ">> Epoch 918 finished \tANN training loss 0.060486\n",
      ">> Epoch 919 finished \tANN training loss 0.066282\n",
      ">> Epoch 920 finished \tANN training loss 0.068678\n",
      ">> Epoch 921 finished \tANN training loss 0.057532\n",
      ">> Epoch 922 finished \tANN training loss 0.073039\n",
      ">> Epoch 923 finished \tANN training loss 0.057849\n",
      ">> Epoch 924 finished \tANN training loss 0.056993\n",
      ">> Epoch 925 finished \tANN training loss 0.055401\n",
      ">> Epoch 926 finished \tANN training loss 0.062741\n",
      ">> Epoch 927 finished \tANN training loss 0.053447\n",
      ">> Epoch 928 finished \tANN training loss 0.115807\n",
      ">> Epoch 929 finished \tANN training loss 0.056901\n",
      ">> Epoch 930 finished \tANN training loss 0.052698\n",
      ">> Epoch 931 finished \tANN training loss 0.050434\n",
      ">> Epoch 932 finished \tANN training loss 0.069113\n",
      ">> Epoch 933 finished \tANN training loss 0.053789\n",
      ">> Epoch 934 finished \tANN training loss 0.054465\n",
      ">> Epoch 935 finished \tANN training loss 0.049804\n",
      ">> Epoch 936 finished \tANN training loss 0.054023\n",
      ">> Epoch 937 finished \tANN training loss 0.061800\n",
      ">> Epoch 938 finished \tANN training loss 0.055626\n",
      ">> Epoch 939 finished \tANN training loss 0.056554\n",
      ">> Epoch 940 finished \tANN training loss 0.053756\n",
      ">> Epoch 941 finished \tANN training loss 0.052818\n",
      ">> Epoch 942 finished \tANN training loss 0.071289\n",
      ">> Epoch 943 finished \tANN training loss 0.075579\n",
      ">> Epoch 944 finished \tANN training loss 0.060236\n",
      ">> Epoch 945 finished \tANN training loss 0.068749\n",
      ">> Epoch 946 finished \tANN training loss 0.062809\n",
      ">> Epoch 947 finished \tANN training loss 0.053963\n",
      ">> Epoch 948 finished \tANN training loss 0.050893\n",
      ">> Epoch 949 finished \tANN training loss 0.053586\n",
      ">> Epoch 950 finished \tANN training loss 0.065772\n",
      ">> Epoch 951 finished \tANN training loss 0.057780\n",
      ">> Epoch 952 finished \tANN training loss 0.051010\n",
      ">> Epoch 953 finished \tANN training loss 0.055301\n",
      ">> Epoch 954 finished \tANN training loss 0.096173\n",
      ">> Epoch 955 finished \tANN training loss 0.062592\n",
      ">> Epoch 956 finished \tANN training loss 0.057735\n",
      ">> Epoch 957 finished \tANN training loss 0.053190\n",
      ">> Epoch 958 finished \tANN training loss 0.052247\n",
      ">> Epoch 959 finished \tANN training loss 0.054339\n",
      ">> Epoch 960 finished \tANN training loss 0.060837\n",
      ">> Epoch 961 finished \tANN training loss 0.058626\n",
      ">> Epoch 962 finished \tANN training loss 0.069913\n",
      ">> Epoch 963 finished \tANN training loss 0.061416\n",
      ">> Epoch 964 finished \tANN training loss 0.049168\n",
      ">> Epoch 965 finished \tANN training loss 0.056964\n",
      ">> Epoch 966 finished \tANN training loss 0.051104\n",
      ">> Epoch 967 finished \tANN training loss 0.046584\n",
      ">> Epoch 968 finished \tANN training loss 0.049148\n",
      ">> Epoch 969 finished \tANN training loss 0.046165\n",
      ">> Epoch 970 finished \tANN training loss 0.057866\n",
      ">> Epoch 971 finished \tANN training loss 0.053219\n",
      ">> Epoch 972 finished \tANN training loss 0.061592\n",
      ">> Epoch 973 finished \tANN training loss 0.076981\n",
      ">> Epoch 974 finished \tANN training loss 0.073925\n",
      ">> Epoch 975 finished \tANN training loss 0.059669\n",
      ">> Epoch 976 finished \tANN training loss 0.061447\n",
      ">> Epoch 977 finished \tANN training loss 0.061050\n",
      ">> Epoch 978 finished \tANN training loss 0.062913\n",
      ">> Epoch 979 finished \tANN training loss 0.056503\n",
      ">> Epoch 980 finished \tANN training loss 0.053634\n",
      ">> Epoch 981 finished \tANN training loss 0.051959\n",
      ">> Epoch 982 finished \tANN training loss 0.049304\n",
      ">> Epoch 983 finished \tANN training loss 0.055416\n",
      ">> Epoch 984 finished \tANN training loss 0.055631\n",
      ">> Epoch 985 finished \tANN training loss 0.052198\n",
      ">> Epoch 986 finished \tANN training loss 0.061910\n",
      ">> Epoch 987 finished \tANN training loss 0.102268\n",
      ">> Epoch 988 finished \tANN training loss 0.078318\n",
      ">> Epoch 989 finished \tANN training loss 0.064491\n",
      ">> Epoch 990 finished \tANN training loss 0.055634\n",
      ">> Epoch 991 finished \tANN training loss 0.059598\n",
      ">> Epoch 992 finished \tANN training loss 0.069247\n",
      ">> Epoch 993 finished \tANN training loss 0.055669\n",
      ">> Epoch 994 finished \tANN training loss 0.055528\n",
      ">> Epoch 995 finished \tANN training loss 0.055304\n",
      ">> Epoch 996 finished \tANN training loss 0.052354\n",
      ">> Epoch 997 finished \tANN training loss 0.050284\n",
      ">> Epoch 998 finished \tANN training loss 0.048266\n",
      ">> Epoch 999 finished \tANN training loss 0.048153\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 8.624595\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 8.432890\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 8.474347\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 8.358066\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 8.428870\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 8.363795\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 8.521246\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 8.436450\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 8.088045\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 7.734633\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 7.793622\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 7.901414\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 7.813156\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 7.965528\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 8.132035\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 8.163415\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 8.505245\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 8.328688\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 8.661448\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 8.686359\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 8.073325\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 11.979760\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 10.033398\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 10.945097\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 12.394855\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 11.665763\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 11.012309\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 10.081907\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 9.727846\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 10.596403\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 11.081095\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 11.023205\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 11.349392\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 11.007415\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 10.509401\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 10.271147\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 11.501799\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 10.579566\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 11.457087\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 11.048555\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.035618\n",
      ">> Epoch 1 finished \tANN training loss 0.885405\n",
      ">> Epoch 2 finished \tANN training loss 0.842006\n",
      ">> Epoch 3 finished \tANN training loss 0.899448\n",
      ">> Epoch 4 finished \tANN training loss 0.817537\n",
      ">> Epoch 5 finished \tANN training loss 0.764231\n",
      ">> Epoch 6 finished \tANN training loss 0.704533\n",
      ">> Epoch 7 finished \tANN training loss 0.675717\n",
      ">> Epoch 8 finished \tANN training loss 0.607785\n",
      ">> Epoch 9 finished \tANN training loss 0.644228\n",
      ">> Epoch 10 finished \tANN training loss 0.563015\n",
      ">> Epoch 11 finished \tANN training loss 0.565148\n",
      ">> Epoch 12 finished \tANN training loss 0.519979\n",
      ">> Epoch 13 finished \tANN training loss 0.505082\n",
      ">> Epoch 14 finished \tANN training loss 0.483338\n",
      ">> Epoch 15 finished \tANN training loss 0.480964\n",
      ">> Epoch 16 finished \tANN training loss 0.453856\n",
      ">> Epoch 17 finished \tANN training loss 0.558947\n",
      ">> Epoch 18 finished \tANN training loss 0.446858\n",
      ">> Epoch 19 finished \tANN training loss 0.449264\n",
      ">> Epoch 20 finished \tANN training loss 0.414807\n",
      ">> Epoch 21 finished \tANN training loss 0.484124\n",
      ">> Epoch 22 finished \tANN training loss 0.403549\n",
      ">> Epoch 23 finished \tANN training loss 0.574087\n",
      ">> Epoch 24 finished \tANN training loss 0.410838\n",
      ">> Epoch 25 finished \tANN training loss 0.383466\n",
      ">> Epoch 26 finished \tANN training loss 0.374305\n",
      ">> Epoch 27 finished \tANN training loss 0.389502\n",
      ">> Epoch 28 finished \tANN training loss 0.378340\n",
      ">> Epoch 29 finished \tANN training loss 0.361298\n",
      ">> Epoch 30 finished \tANN training loss 0.374418\n",
      ">> Epoch 31 finished \tANN training loss 0.357339\n",
      ">> Epoch 32 finished \tANN training loss 0.447984\n",
      ">> Epoch 33 finished \tANN training loss 0.369148\n",
      ">> Epoch 34 finished \tANN training loss 0.355530\n",
      ">> Epoch 35 finished \tANN training loss 0.324969\n",
      ">> Epoch 36 finished \tANN training loss 0.344237\n",
      ">> Epoch 37 finished \tANN training loss 0.342515\n",
      ">> Epoch 38 finished \tANN training loss 0.318667\n",
      ">> Epoch 39 finished \tANN training loss 0.352689\n",
      ">> Epoch 40 finished \tANN training loss 0.304239\n",
      ">> Epoch 41 finished \tANN training loss 0.296478\n",
      ">> Epoch 42 finished \tANN training loss 0.347157\n",
      ">> Epoch 43 finished \tANN training loss 0.291336\n",
      ">> Epoch 44 finished \tANN training loss 0.298397\n",
      ">> Epoch 45 finished \tANN training loss 0.367867\n",
      ">> Epoch 46 finished \tANN training loss 0.269090\n",
      ">> Epoch 47 finished \tANN training loss 0.259862\n",
      ">> Epoch 48 finished \tANN training loss 0.500910\n",
      ">> Epoch 49 finished \tANN training loss 0.297143\n",
      ">> Epoch 50 finished \tANN training loss 0.263634\n",
      ">> Epoch 51 finished \tANN training loss 0.253437\n",
      ">> Epoch 52 finished \tANN training loss 0.258136\n",
      ">> Epoch 53 finished \tANN training loss 0.225398\n",
      ">> Epoch 54 finished \tANN training loss 0.290182\n",
      ">> Epoch 55 finished \tANN training loss 0.241872\n",
      ">> Epoch 56 finished \tANN training loss 0.314427\n",
      ">> Epoch 57 finished \tANN training loss 0.323061\n",
      ">> Epoch 58 finished \tANN training loss 0.227055\n",
      ">> Epoch 59 finished \tANN training loss 0.226281\n",
      ">> Epoch 60 finished \tANN training loss 0.236012\n",
      ">> Epoch 61 finished \tANN training loss 0.235098\n",
      ">> Epoch 62 finished \tANN training loss 0.220798\n",
      ">> Epoch 63 finished \tANN training loss 0.227662\n",
      ">> Epoch 64 finished \tANN training loss 0.211522\n",
      ">> Epoch 65 finished \tANN training loss 0.205729\n",
      ">> Epoch 66 finished \tANN training loss 0.196777\n",
      ">> Epoch 67 finished \tANN training loss 0.428660\n",
      ">> Epoch 68 finished \tANN training loss 0.223727\n",
      ">> Epoch 69 finished \tANN training loss 0.217837\n",
      ">> Epoch 70 finished \tANN training loss 0.306422\n",
      ">> Epoch 71 finished \tANN training loss 0.269565\n",
      ">> Epoch 72 finished \tANN training loss 0.248308\n",
      ">> Epoch 73 finished \tANN training loss 0.220138\n",
      ">> Epoch 74 finished \tANN training loss 0.192939\n",
      ">> Epoch 75 finished \tANN training loss 0.206400\n",
      ">> Epoch 76 finished \tANN training loss 0.178965\n",
      ">> Epoch 77 finished \tANN training loss 0.284228\n",
      ">> Epoch 78 finished \tANN training loss 0.176826\n",
      ">> Epoch 79 finished \tANN training loss 0.201872\n",
      ">> Epoch 80 finished \tANN training loss 0.178780\n",
      ">> Epoch 81 finished \tANN training loss 0.175921\n",
      ">> Epoch 82 finished \tANN training loss 0.169685\n",
      ">> Epoch 83 finished \tANN training loss 0.166424\n",
      ">> Epoch 84 finished \tANN training loss 0.185176\n",
      ">> Epoch 85 finished \tANN training loss 0.169791\n",
      ">> Epoch 86 finished \tANN training loss 0.178506\n",
      ">> Epoch 87 finished \tANN training loss 0.179375\n",
      ">> Epoch 88 finished \tANN training loss 0.166648\n",
      ">> Epoch 89 finished \tANN training loss 0.180884\n",
      ">> Epoch 90 finished \tANN training loss 0.173922\n",
      ">> Epoch 91 finished \tANN training loss 0.235395\n",
      ">> Epoch 92 finished \tANN training loss 0.205213\n",
      ">> Epoch 93 finished \tANN training loss 0.163494\n",
      ">> Epoch 94 finished \tANN training loss 0.176922\n",
      ">> Epoch 95 finished \tANN training loss 0.182560\n",
      ">> Epoch 96 finished \tANN training loss 0.196861\n",
      ">> Epoch 97 finished \tANN training loss 0.229146\n",
      ">> Epoch 98 finished \tANN training loss 0.245291\n",
      ">> Epoch 99 finished \tANN training loss 0.166888\n",
      ">> Epoch 100 finished \tANN training loss 0.192055\n",
      ">> Epoch 101 finished \tANN training loss 0.173123\n",
      ">> Epoch 102 finished \tANN training loss 0.158635\n",
      ">> Epoch 103 finished \tANN training loss 0.177781\n",
      ">> Epoch 104 finished \tANN training loss 0.174036\n",
      ">> Epoch 105 finished \tANN training loss 0.275272\n",
      ">> Epoch 106 finished \tANN training loss 0.196378\n",
      ">> Epoch 107 finished \tANN training loss 0.171610\n",
      ">> Epoch 108 finished \tANN training loss 0.152043\n",
      ">> Epoch 109 finished \tANN training loss 0.161473\n",
      ">> Epoch 110 finished \tANN training loss 0.173411\n",
      ">> Epoch 111 finished \tANN training loss 0.185673\n",
      ">> Epoch 112 finished \tANN training loss 0.200978\n",
      ">> Epoch 113 finished \tANN training loss 0.176228\n",
      ">> Epoch 114 finished \tANN training loss 0.263629\n",
      ">> Epoch 115 finished \tANN training loss 0.157490\n",
      ">> Epoch 116 finished \tANN training loss 0.156154\n",
      ">> Epoch 117 finished \tANN training loss 0.135602\n",
      ">> Epoch 118 finished \tANN training loss 0.161137\n",
      ">> Epoch 119 finished \tANN training loss 0.136812\n",
      ">> Epoch 120 finished \tANN training loss 0.146260\n",
      ">> Epoch 121 finished \tANN training loss 0.158313\n",
      ">> Epoch 122 finished \tANN training loss 0.173898\n",
      ">> Epoch 123 finished \tANN training loss 0.178386\n",
      ">> Epoch 124 finished \tANN training loss 0.166080\n",
      ">> Epoch 125 finished \tANN training loss 0.136138\n",
      ">> Epoch 126 finished \tANN training loss 0.206653\n",
      ">> Epoch 127 finished \tANN training loss 0.140739\n",
      ">> Epoch 128 finished \tANN training loss 0.267891\n",
      ">> Epoch 129 finished \tANN training loss 0.149539\n",
      ">> Epoch 130 finished \tANN training loss 0.151712\n",
      ">> Epoch 131 finished \tANN training loss 0.136570\n",
      ">> Epoch 132 finished \tANN training loss 0.186516\n",
      ">> Epoch 133 finished \tANN training loss 0.141381\n",
      ">> Epoch 134 finished \tANN training loss 0.136015\n",
      ">> Epoch 135 finished \tANN training loss 0.139616\n",
      ">> Epoch 136 finished \tANN training loss 0.135909\n",
      ">> Epoch 137 finished \tANN training loss 0.179207\n",
      ">> Epoch 138 finished \tANN training loss 0.191388\n",
      ">> Epoch 139 finished \tANN training loss 0.147679\n",
      ">> Epoch 140 finished \tANN training loss 0.137398\n",
      ">> Epoch 141 finished \tANN training loss 0.146355\n",
      ">> Epoch 142 finished \tANN training loss 0.158742\n",
      ">> Epoch 143 finished \tANN training loss 0.138973\n",
      ">> Epoch 144 finished \tANN training loss 0.152000\n",
      ">> Epoch 145 finished \tANN training loss 0.130898\n",
      ">> Epoch 146 finished \tANN training loss 0.135549\n",
      ">> Epoch 147 finished \tANN training loss 0.149088\n",
      ">> Epoch 148 finished \tANN training loss 0.131933\n",
      ">> Epoch 149 finished \tANN training loss 0.122652\n",
      ">> Epoch 150 finished \tANN training loss 0.206660\n",
      ">> Epoch 151 finished \tANN training loss 0.170466\n",
      ">> Epoch 152 finished \tANN training loss 0.133528\n",
      ">> Epoch 153 finished \tANN training loss 0.126549\n",
      ">> Epoch 154 finished \tANN training loss 0.131581\n",
      ">> Epoch 155 finished \tANN training loss 0.152308\n",
      ">> Epoch 156 finished \tANN training loss 0.133168\n",
      ">> Epoch 157 finished \tANN training loss 0.126048\n",
      ">> Epoch 158 finished \tANN training loss 0.127480\n",
      ">> Epoch 159 finished \tANN training loss 0.113262\n",
      ">> Epoch 160 finished \tANN training loss 0.110964\n",
      ">> Epoch 161 finished \tANN training loss 0.200955\n",
      ">> Epoch 162 finished \tANN training loss 0.117721\n",
      ">> Epoch 163 finished \tANN training loss 0.145024\n",
      ">> Epoch 164 finished \tANN training loss 0.118890\n",
      ">> Epoch 165 finished \tANN training loss 0.158848\n",
      ">> Epoch 166 finished \tANN training loss 0.143558\n",
      ">> Epoch 167 finished \tANN training loss 0.124189\n",
      ">> Epoch 168 finished \tANN training loss 0.134817\n",
      ">> Epoch 169 finished \tANN training loss 0.150460\n",
      ">> Epoch 170 finished \tANN training loss 0.119098\n",
      ">> Epoch 171 finished \tANN training loss 0.174597\n",
      ">> Epoch 172 finished \tANN training loss 0.143246\n",
      ">> Epoch 173 finished \tANN training loss 0.253931\n",
      ">> Epoch 174 finished \tANN training loss 0.149202\n",
      ">> Epoch 175 finished \tANN training loss 0.120105\n",
      ">> Epoch 176 finished \tANN training loss 0.114664\n",
      ">> Epoch 177 finished \tANN training loss 0.163922\n",
      ">> Epoch 178 finished \tANN training loss 0.119973\n",
      ">> Epoch 179 finished \tANN training loss 0.153077\n",
      ">> Epoch 180 finished \tANN training loss 0.117353\n",
      ">> Epoch 181 finished \tANN training loss 0.110701\n",
      ">> Epoch 182 finished \tANN training loss 0.122516\n",
      ">> Epoch 183 finished \tANN training loss 0.126902\n",
      ">> Epoch 184 finished \tANN training loss 0.109475\n",
      ">> Epoch 185 finished \tANN training loss 0.109415\n",
      ">> Epoch 186 finished \tANN training loss 0.123029\n",
      ">> Epoch 187 finished \tANN training loss 0.127345\n",
      ">> Epoch 188 finished \tANN training loss 0.099613\n",
      ">> Epoch 189 finished \tANN training loss 0.106047\n",
      ">> Epoch 190 finished \tANN training loss 0.123415\n",
      ">> Epoch 191 finished \tANN training loss 0.113576\n",
      ">> Epoch 192 finished \tANN training loss 0.359779\n",
      ">> Epoch 193 finished \tANN training loss 0.138562\n",
      ">> Epoch 194 finished \tANN training loss 0.107684\n",
      ">> Epoch 195 finished \tANN training loss 0.266171\n",
      ">> Epoch 196 finished \tANN training loss 0.163049\n",
      ">> Epoch 197 finished \tANN training loss 0.139783\n",
      ">> Epoch 198 finished \tANN training loss 0.119454\n",
      ">> Epoch 199 finished \tANN training loss 0.107465\n",
      ">> Epoch 200 finished \tANN training loss 0.112519\n",
      ">> Epoch 201 finished \tANN training loss 0.141609\n",
      ">> Epoch 202 finished \tANN training loss 0.103073\n",
      ">> Epoch 203 finished \tANN training loss 0.106075\n",
      ">> Epoch 204 finished \tANN training loss 0.117893\n",
      ">> Epoch 205 finished \tANN training loss 0.141852\n",
      ">> Epoch 206 finished \tANN training loss 0.116063\n",
      ">> Epoch 207 finished \tANN training loss 0.116452\n",
      ">> Epoch 208 finished \tANN training loss 0.133680\n",
      ">> Epoch 209 finished \tANN training loss 0.129302\n",
      ">> Epoch 210 finished \tANN training loss 0.110503\n",
      ">> Epoch 211 finished \tANN training loss 0.103621\n",
      ">> Epoch 212 finished \tANN training loss 0.138611\n",
      ">> Epoch 213 finished \tANN training loss 0.109814\n",
      ">> Epoch 214 finished \tANN training loss 0.117799\n",
      ">> Epoch 215 finished \tANN training loss 0.099041\n",
      ">> Epoch 216 finished \tANN training loss 0.110441\n",
      ">> Epoch 217 finished \tANN training loss 0.109132\n",
      ">> Epoch 218 finished \tANN training loss 0.116941\n",
      ">> Epoch 219 finished \tANN training loss 0.111382\n",
      ">> Epoch 220 finished \tANN training loss 0.118033\n",
      ">> Epoch 221 finished \tANN training loss 0.100545\n",
      ">> Epoch 222 finished \tANN training loss 0.113418\n",
      ">> Epoch 223 finished \tANN training loss 0.095197\n",
      ">> Epoch 224 finished \tANN training loss 0.135001\n",
      ">> Epoch 225 finished \tANN training loss 0.103903\n",
      ">> Epoch 226 finished \tANN training loss 0.103779\n",
      ">> Epoch 227 finished \tANN training loss 0.087751\n",
      ">> Epoch 228 finished \tANN training loss 0.095057\n",
      ">> Epoch 229 finished \tANN training loss 0.100484\n",
      ">> Epoch 230 finished \tANN training loss 0.112220\n",
      ">> Epoch 231 finished \tANN training loss 0.118060\n",
      ">> Epoch 232 finished \tANN training loss 0.225583\n",
      ">> Epoch 233 finished \tANN training loss 0.127814\n",
      ">> Epoch 234 finished \tANN training loss 0.119841\n",
      ">> Epoch 235 finished \tANN training loss 0.109808\n",
      ">> Epoch 236 finished \tANN training loss 0.113605\n",
      ">> Epoch 237 finished \tANN training loss 0.126643\n",
      ">> Epoch 238 finished \tANN training loss 0.129039\n",
      ">> Epoch 239 finished \tANN training loss 0.133507\n",
      ">> Epoch 240 finished \tANN training loss 0.106762\n",
      ">> Epoch 241 finished \tANN training loss 0.172594\n",
      ">> Epoch 242 finished \tANN training loss 0.140486\n",
      ">> Epoch 243 finished \tANN training loss 0.135337\n",
      ">> Epoch 244 finished \tANN training loss 0.087877\n",
      ">> Epoch 245 finished \tANN training loss 0.088548\n",
      ">> Epoch 246 finished \tANN training loss 0.090304\n",
      ">> Epoch 247 finished \tANN training loss 0.112259\n",
      ">> Epoch 248 finished \tANN training loss 0.104266\n",
      ">> Epoch 249 finished \tANN training loss 0.091463\n",
      ">> Epoch 250 finished \tANN training loss 0.133897\n",
      ">> Epoch 251 finished \tANN training loss 0.104624\n",
      ">> Epoch 252 finished \tANN training loss 0.122181\n",
      ">> Epoch 253 finished \tANN training loss 0.135509\n",
      ">> Epoch 254 finished \tANN training loss 0.127222\n",
      ">> Epoch 255 finished \tANN training loss 0.105321\n",
      ">> Epoch 256 finished \tANN training loss 0.118125\n",
      ">> Epoch 257 finished \tANN training loss 0.103356\n",
      ">> Epoch 258 finished \tANN training loss 0.107322\n",
      ">> Epoch 259 finished \tANN training loss 0.098217\n",
      ">> Epoch 260 finished \tANN training loss 0.101983\n",
      ">> Epoch 261 finished \tANN training loss 0.189209\n",
      ">> Epoch 262 finished \tANN training loss 0.117715\n",
      ">> Epoch 263 finished \tANN training loss 0.127862\n",
      ">> Epoch 264 finished \tANN training loss 0.138933\n",
      ">> Epoch 265 finished \tANN training loss 0.105473\n",
      ">> Epoch 266 finished \tANN training loss 0.100341\n",
      ">> Epoch 267 finished \tANN training loss 0.140352\n",
      ">> Epoch 268 finished \tANN training loss 0.112399\n",
      ">> Epoch 269 finished \tANN training loss 0.105419\n",
      ">> Epoch 270 finished \tANN training loss 0.110931\n",
      ">> Epoch 271 finished \tANN training loss 0.102897\n",
      ">> Epoch 272 finished \tANN training loss 0.120623\n",
      ">> Epoch 273 finished \tANN training loss 0.104247\n",
      ">> Epoch 274 finished \tANN training loss 0.094983\n",
      ">> Epoch 275 finished \tANN training loss 0.162567\n",
      ">> Epoch 276 finished \tANN training loss 0.097920\n",
      ">> Epoch 277 finished \tANN training loss 0.100152\n",
      ">> Epoch 278 finished \tANN training loss 0.104788\n",
      ">> Epoch 279 finished \tANN training loss 0.108610\n",
      ">> Epoch 280 finished \tANN training loss 0.094081\n",
      ">> Epoch 281 finished \tANN training loss 0.103924\n",
      ">> Epoch 282 finished \tANN training loss 0.099498\n",
      ">> Epoch 283 finished \tANN training loss 0.111681\n",
      ">> Epoch 284 finished \tANN training loss 0.100897\n",
      ">> Epoch 285 finished \tANN training loss 0.091817\n",
      ">> Epoch 286 finished \tANN training loss 0.091069\n",
      ">> Epoch 287 finished \tANN training loss 0.095828\n",
      ">> Epoch 288 finished \tANN training loss 0.092629\n",
      ">> Epoch 289 finished \tANN training loss 0.099163\n",
      ">> Epoch 290 finished \tANN training loss 0.114457\n",
      ">> Epoch 291 finished \tANN training loss 0.109561\n",
      ">> Epoch 292 finished \tANN training loss 0.087973\n",
      ">> Epoch 293 finished \tANN training loss 0.086257\n",
      ">> Epoch 294 finished \tANN training loss 0.104596\n",
      ">> Epoch 295 finished \tANN training loss 0.092245\n",
      ">> Epoch 296 finished \tANN training loss 0.155934\n",
      ">> Epoch 297 finished \tANN training loss 0.107917\n",
      ">> Epoch 298 finished \tANN training loss 0.109150\n",
      ">> Epoch 299 finished \tANN training loss 0.093611\n",
      ">> Epoch 300 finished \tANN training loss 0.179975\n",
      ">> Epoch 301 finished \tANN training loss 0.092803\n",
      ">> Epoch 302 finished \tANN training loss 0.097434\n",
      ">> Epoch 303 finished \tANN training loss 0.090961\n",
      ">> Epoch 304 finished \tANN training loss 0.104515\n",
      ">> Epoch 305 finished \tANN training loss 0.097319\n",
      ">> Epoch 306 finished \tANN training loss 0.088002\n",
      ">> Epoch 307 finished \tANN training loss 0.090617\n",
      ">> Epoch 308 finished \tANN training loss 0.125742\n",
      ">> Epoch 309 finished \tANN training loss 0.091855\n",
      ">> Epoch 310 finished \tANN training loss 0.093522\n",
      ">> Epoch 311 finished \tANN training loss 0.087114\n",
      ">> Epoch 312 finished \tANN training loss 0.087365\n",
      ">> Epoch 313 finished \tANN training loss 0.087881\n",
      ">> Epoch 314 finished \tANN training loss 0.091244\n",
      ">> Epoch 315 finished \tANN training loss 0.098607\n",
      ">> Epoch 316 finished \tANN training loss 0.110235\n",
      ">> Epoch 317 finished \tANN training loss 0.120225\n",
      ">> Epoch 318 finished \tANN training loss 0.092017\n",
      ">> Epoch 319 finished \tANN training loss 0.087056\n",
      ">> Epoch 320 finished \tANN training loss 0.088165\n",
      ">> Epoch 321 finished \tANN training loss 0.073477\n",
      ">> Epoch 322 finished \tANN training loss 0.087605\n",
      ">> Epoch 323 finished \tANN training loss 0.095683\n",
      ">> Epoch 324 finished \tANN training loss 0.084342\n",
      ">> Epoch 325 finished \tANN training loss 0.078930\n",
      ">> Epoch 326 finished \tANN training loss 0.149377\n",
      ">> Epoch 327 finished \tANN training loss 0.094635\n",
      ">> Epoch 328 finished \tANN training loss 0.184268\n",
      ">> Epoch 329 finished \tANN training loss 0.111164\n",
      ">> Epoch 330 finished \tANN training loss 0.104269\n",
      ">> Epoch 331 finished \tANN training loss 0.106039\n",
      ">> Epoch 332 finished \tANN training loss 0.097866\n",
      ">> Epoch 333 finished \tANN training loss 0.100272\n",
      ">> Epoch 334 finished \tANN training loss 0.155142\n",
      ">> Epoch 335 finished \tANN training loss 0.109599\n",
      ">> Epoch 336 finished \tANN training loss 0.133957\n",
      ">> Epoch 337 finished \tANN training loss 0.095074\n",
      ">> Epoch 338 finished \tANN training loss 0.109232\n",
      ">> Epoch 339 finished \tANN training loss 0.093264\n",
      ">> Epoch 340 finished \tANN training loss 0.103519\n",
      ">> Epoch 341 finished \tANN training loss 0.143368\n",
      ">> Epoch 342 finished \tANN training loss 0.104922\n",
      ">> Epoch 343 finished \tANN training loss 0.112903\n",
      ">> Epoch 344 finished \tANN training loss 0.163439\n",
      ">> Epoch 345 finished \tANN training loss 0.102128\n",
      ">> Epoch 346 finished \tANN training loss 0.105387\n",
      ">> Epoch 347 finished \tANN training loss 0.113812\n",
      ">> Epoch 348 finished \tANN training loss 0.079036\n",
      ">> Epoch 349 finished \tANN training loss 0.085195\n",
      ">> Epoch 350 finished \tANN training loss 0.090381\n",
      ">> Epoch 351 finished \tANN training loss 0.177067\n",
      ">> Epoch 352 finished \tANN training loss 0.107007\n",
      ">> Epoch 353 finished \tANN training loss 0.133288\n",
      ">> Epoch 354 finished \tANN training loss 0.092338\n",
      ">> Epoch 355 finished \tANN training loss 0.109852\n",
      ">> Epoch 356 finished \tANN training loss 0.087322\n",
      ">> Epoch 357 finished \tANN training loss 0.128583\n",
      ">> Epoch 358 finished \tANN training loss 0.097306\n",
      ">> Epoch 359 finished \tANN training loss 0.084677\n",
      ">> Epoch 360 finished \tANN training loss 0.079187\n",
      ">> Epoch 361 finished \tANN training loss 0.093815\n",
      ">> Epoch 362 finished \tANN training loss 0.090028\n",
      ">> Epoch 363 finished \tANN training loss 0.092684\n",
      ">> Epoch 364 finished \tANN training loss 0.122522\n",
      ">> Epoch 365 finished \tANN training loss 0.100954\n",
      ">> Epoch 366 finished \tANN training loss 0.108022\n",
      ">> Epoch 367 finished \tANN training loss 0.090363\n",
      ">> Epoch 368 finished \tANN training loss 0.140739\n",
      ">> Epoch 369 finished \tANN training loss 0.127620\n",
      ">> Epoch 370 finished \tANN training loss 0.106213\n",
      ">> Epoch 371 finished \tANN training loss 0.104705\n",
      ">> Epoch 372 finished \tANN training loss 0.088468\n",
      ">> Epoch 373 finished \tANN training loss 0.099515\n",
      ">> Epoch 374 finished \tANN training loss 0.109169\n",
      ">> Epoch 375 finished \tANN training loss 0.124351\n",
      ">> Epoch 376 finished \tANN training loss 0.090953\n",
      ">> Epoch 377 finished \tANN training loss 0.075740\n",
      ">> Epoch 378 finished \tANN training loss 0.101099\n",
      ">> Epoch 379 finished \tANN training loss 0.107241\n",
      ">> Epoch 380 finished \tANN training loss 0.123143\n",
      ">> Epoch 381 finished \tANN training loss 0.145234\n",
      ">> Epoch 382 finished \tANN training loss 0.098901\n",
      ">> Epoch 383 finished \tANN training loss 0.131377\n",
      ">> Epoch 384 finished \tANN training loss 0.102006\n",
      ">> Epoch 385 finished \tANN training loss 0.117234\n",
      ">> Epoch 386 finished \tANN training loss 0.095966\n",
      ">> Epoch 387 finished \tANN training loss 0.118316\n",
      ">> Epoch 388 finished \tANN training loss 0.094834\n",
      ">> Epoch 389 finished \tANN training loss 0.087774\n",
      ">> Epoch 390 finished \tANN training loss 0.083816\n",
      ">> Epoch 391 finished \tANN training loss 0.087711\n",
      ">> Epoch 392 finished \tANN training loss 0.152972\n",
      ">> Epoch 393 finished \tANN training loss 0.103409\n",
      ">> Epoch 394 finished \tANN training loss 0.097101\n",
      ">> Epoch 395 finished \tANN training loss 0.096527\n",
      ">> Epoch 396 finished \tANN training loss 0.096303\n",
      ">> Epoch 397 finished \tANN training loss 0.088827\n",
      ">> Epoch 398 finished \tANN training loss 0.087965\n",
      ">> Epoch 399 finished \tANN training loss 0.085636\n",
      ">> Epoch 400 finished \tANN training loss 0.084646\n",
      ">> Epoch 401 finished \tANN training loss 0.095377\n",
      ">> Epoch 402 finished \tANN training loss 0.087603\n",
      ">> Epoch 403 finished \tANN training loss 0.111773\n",
      ">> Epoch 404 finished \tANN training loss 0.101472\n",
      ">> Epoch 405 finished \tANN training loss 0.083018\n",
      ">> Epoch 406 finished \tANN training loss 0.087923\n",
      ">> Epoch 407 finished \tANN training loss 0.082169\n",
      ">> Epoch 408 finished \tANN training loss 0.117705\n",
      ">> Epoch 409 finished \tANN training loss 0.155550\n",
      ">> Epoch 410 finished \tANN training loss 0.109266\n",
      ">> Epoch 411 finished \tANN training loss 0.121756\n",
      ">> Epoch 412 finished \tANN training loss 0.087647\n",
      ">> Epoch 413 finished \tANN training loss 0.341934\n",
      ">> Epoch 414 finished \tANN training loss 0.193816\n",
      ">> Epoch 415 finished \tANN training loss 0.112372\n",
      ">> Epoch 416 finished \tANN training loss 0.107872\n",
      ">> Epoch 417 finished \tANN training loss 0.095185\n",
      ">> Epoch 418 finished \tANN training loss 0.099476\n",
      ">> Epoch 419 finished \tANN training loss 0.102773\n",
      ">> Epoch 420 finished \tANN training loss 0.094855\n",
      ">> Epoch 421 finished \tANN training loss 0.108577\n",
      ">> Epoch 422 finished \tANN training loss 0.102414\n",
      ">> Epoch 423 finished \tANN training loss 0.110395\n",
      ">> Epoch 424 finished \tANN training loss 0.099994\n",
      ">> Epoch 425 finished \tANN training loss 0.159647\n",
      ">> Epoch 426 finished \tANN training loss 0.105278\n",
      ">> Epoch 427 finished \tANN training loss 0.093629\n",
      ">> Epoch 428 finished \tANN training loss 0.088837\n",
      ">> Epoch 429 finished \tANN training loss 0.080794\n",
      ">> Epoch 430 finished \tANN training loss 0.124877\n",
      ">> Epoch 431 finished \tANN training loss 0.083245\n",
      ">> Epoch 432 finished \tANN training loss 0.089125\n",
      ">> Epoch 433 finished \tANN training loss 0.092768\n",
      ">> Epoch 434 finished \tANN training loss 0.075658\n",
      ">> Epoch 435 finished \tANN training loss 0.074811\n",
      ">> Epoch 436 finished \tANN training loss 0.084368\n",
      ">> Epoch 437 finished \tANN training loss 0.074443\n",
      ">> Epoch 438 finished \tANN training loss 0.084894\n",
      ">> Epoch 439 finished \tANN training loss 0.077017\n",
      ">> Epoch 440 finished \tANN training loss 0.115238\n",
      ">> Epoch 441 finished \tANN training loss 0.119761\n",
      ">> Epoch 442 finished \tANN training loss 0.086629\n",
      ">> Epoch 443 finished \tANN training loss 0.115225\n",
      ">> Epoch 444 finished \tANN training loss 0.104503\n",
      ">> Epoch 445 finished \tANN training loss 0.087288\n",
      ">> Epoch 446 finished \tANN training loss 0.088533\n",
      ">> Epoch 447 finished \tANN training loss 0.095772\n",
      ">> Epoch 448 finished \tANN training loss 0.079212\n",
      ">> Epoch 449 finished \tANN training loss 0.078319\n",
      ">> Epoch 450 finished \tANN training loss 0.080996\n",
      ">> Epoch 451 finished \tANN training loss 0.077804\n",
      ">> Epoch 452 finished \tANN training loss 0.082279\n",
      ">> Epoch 453 finished \tANN training loss 0.094802\n",
      ">> Epoch 454 finished \tANN training loss 0.090895\n",
      ">> Epoch 455 finished \tANN training loss 0.097812\n",
      ">> Epoch 456 finished \tANN training loss 0.089536\n",
      ">> Epoch 457 finished \tANN training loss 0.073840\n",
      ">> Epoch 458 finished \tANN training loss 0.079403\n",
      ">> Epoch 459 finished \tANN training loss 0.086652\n",
      ">> Epoch 460 finished \tANN training loss 0.077035\n",
      ">> Epoch 461 finished \tANN training loss 0.094436\n",
      ">> Epoch 462 finished \tANN training loss 0.087744\n",
      ">> Epoch 463 finished \tANN training loss 0.080634\n",
      ">> Epoch 464 finished \tANN training loss 0.081600\n",
      ">> Epoch 465 finished \tANN training loss 0.094952\n",
      ">> Epoch 466 finished \tANN training loss 0.094853\n",
      ">> Epoch 467 finished \tANN training loss 0.099334\n",
      ">> Epoch 468 finished \tANN training loss 0.074655\n",
      ">> Epoch 469 finished \tANN training loss 0.075976\n",
      ">> Epoch 470 finished \tANN training loss 0.131391\n",
      ">> Epoch 471 finished \tANN training loss 0.079903\n",
      ">> Epoch 472 finished \tANN training loss 0.082515\n",
      ">> Epoch 473 finished \tANN training loss 0.077979\n",
      ">> Epoch 474 finished \tANN training loss 0.080264\n",
      ">> Epoch 475 finished \tANN training loss 0.080626\n",
      ">> Epoch 476 finished \tANN training loss 0.084391\n",
      ">> Epoch 477 finished \tANN training loss 0.103288\n",
      ">> Epoch 478 finished \tANN training loss 0.091024\n",
      ">> Epoch 479 finished \tANN training loss 0.078129\n",
      ">> Epoch 480 finished \tANN training loss 0.091366\n",
      ">> Epoch 481 finished \tANN training loss 0.103184\n",
      ">> Epoch 482 finished \tANN training loss 0.089371\n",
      ">> Epoch 483 finished \tANN training loss 0.076479\n",
      ">> Epoch 484 finished \tANN training loss 0.069778\n",
      ">> Epoch 485 finished \tANN training loss 0.084406\n",
      ">> Epoch 486 finished \tANN training loss 0.090082\n",
      ">> Epoch 487 finished \tANN training loss 0.075013\n",
      ">> Epoch 488 finished \tANN training loss 0.070298\n",
      ">> Epoch 489 finished \tANN training loss 0.084399\n",
      ">> Epoch 490 finished \tANN training loss 0.073325\n",
      ">> Epoch 491 finished \tANN training loss 0.151235\n",
      ">> Epoch 492 finished \tANN training loss 0.074068\n",
      ">> Epoch 493 finished \tANN training loss 0.082781\n",
      ">> Epoch 494 finished \tANN training loss 0.087279\n",
      ">> Epoch 495 finished \tANN training loss 0.083986\n",
      ">> Epoch 496 finished \tANN training loss 0.072120\n",
      ">> Epoch 497 finished \tANN training loss 0.071940\n",
      ">> Epoch 498 finished \tANN training loss 0.085004\n",
      ">> Epoch 499 finished \tANN training loss 0.080080\n",
      ">> Epoch 500 finished \tANN training loss 0.080908\n",
      ">> Epoch 501 finished \tANN training loss 0.095458\n",
      ">> Epoch 502 finished \tANN training loss 0.085252\n",
      ">> Epoch 503 finished \tANN training loss 0.100060\n",
      ">> Epoch 504 finished \tANN training loss 0.151836\n",
      ">> Epoch 505 finished \tANN training loss 0.101145\n",
      ">> Epoch 506 finished \tANN training loss 0.086756\n",
      ">> Epoch 507 finished \tANN training loss 0.105357\n",
      ">> Epoch 508 finished \tANN training loss 0.081015\n",
      ">> Epoch 509 finished \tANN training loss 0.129120\n",
      ">> Epoch 510 finished \tANN training loss 0.076474\n",
      ">> Epoch 511 finished \tANN training loss 0.064220\n",
      ">> Epoch 512 finished \tANN training loss 0.082765\n",
      ">> Epoch 513 finished \tANN training loss 0.093624\n",
      ">> Epoch 514 finished \tANN training loss 0.078364\n",
      ">> Epoch 515 finished \tANN training loss 0.086572\n",
      ">> Epoch 516 finished \tANN training loss 0.085206\n",
      ">> Epoch 517 finished \tANN training loss 0.093444\n",
      ">> Epoch 518 finished \tANN training loss 0.140565\n",
      ">> Epoch 519 finished \tANN training loss 0.076572\n",
      ">> Epoch 520 finished \tANN training loss 0.087174\n",
      ">> Epoch 521 finished \tANN training loss 0.097142\n",
      ">> Epoch 522 finished \tANN training loss 0.103791\n",
      ">> Epoch 523 finished \tANN training loss 0.076861\n",
      ">> Epoch 524 finished \tANN training loss 0.072803\n",
      ">> Epoch 525 finished \tANN training loss 0.121268\n",
      ">> Epoch 526 finished \tANN training loss 0.074982\n",
      ">> Epoch 527 finished \tANN training loss 0.095944\n",
      ">> Epoch 528 finished \tANN training loss 0.073286\n",
      ">> Epoch 529 finished \tANN training loss 0.092468\n",
      ">> Epoch 530 finished \tANN training loss 0.080780\n",
      ">> Epoch 531 finished \tANN training loss 0.078141\n",
      ">> Epoch 532 finished \tANN training loss 0.100653\n",
      ">> Epoch 533 finished \tANN training loss 0.076055\n",
      ">> Epoch 534 finished \tANN training loss 0.075108\n",
      ">> Epoch 535 finished \tANN training loss 0.078984\n",
      ">> Epoch 536 finished \tANN training loss 0.095302\n",
      ">> Epoch 537 finished \tANN training loss 0.105562\n",
      ">> Epoch 538 finished \tANN training loss 0.159033\n",
      ">> Epoch 539 finished \tANN training loss 0.110738\n",
      ">> Epoch 540 finished \tANN training loss 0.078352\n",
      ">> Epoch 541 finished \tANN training loss 0.075959\n",
      ">> Epoch 542 finished \tANN training loss 0.109040\n",
      ">> Epoch 543 finished \tANN training loss 0.077504\n",
      ">> Epoch 544 finished \tANN training loss 0.089589\n",
      ">> Epoch 545 finished \tANN training loss 0.084390\n",
      ">> Epoch 546 finished \tANN training loss 0.088811\n",
      ">> Epoch 547 finished \tANN training loss 0.124696\n",
      ">> Epoch 548 finished \tANN training loss 0.088246\n",
      ">> Epoch 549 finished \tANN training loss 0.085665\n",
      ">> Epoch 550 finished \tANN training loss 0.081140\n",
      ">> Epoch 551 finished \tANN training loss 0.077499\n",
      ">> Epoch 552 finished \tANN training loss 0.070763\n",
      ">> Epoch 553 finished \tANN training loss 0.120305\n",
      ">> Epoch 554 finished \tANN training loss 0.086345\n",
      ">> Epoch 555 finished \tANN training loss 0.074289\n",
      ">> Epoch 556 finished \tANN training loss 0.096895\n",
      ">> Epoch 557 finished \tANN training loss 0.069264\n",
      ">> Epoch 558 finished \tANN training loss 0.090962\n",
      ">> Epoch 559 finished \tANN training loss 0.080635\n",
      ">> Epoch 560 finished \tANN training loss 0.091209\n",
      ">> Epoch 561 finished \tANN training loss 0.152480\n",
      ">> Epoch 562 finished \tANN training loss 0.096734\n",
      ">> Epoch 563 finished \tANN training loss 0.094216\n",
      ">> Epoch 564 finished \tANN training loss 0.086997\n",
      ">> Epoch 565 finished \tANN training loss 0.084565\n",
      ">> Epoch 566 finished \tANN training loss 0.094603\n",
      ">> Epoch 567 finished \tANN training loss 0.093772\n",
      ">> Epoch 568 finished \tANN training loss 0.082073\n",
      ">> Epoch 569 finished \tANN training loss 0.075412\n",
      ">> Epoch 570 finished \tANN training loss 0.077310\n",
      ">> Epoch 571 finished \tANN training loss 0.076976\n",
      ">> Epoch 572 finished \tANN training loss 0.072243\n",
      ">> Epoch 573 finished \tANN training loss 0.074083\n",
      ">> Epoch 574 finished \tANN training loss 0.076529\n",
      ">> Epoch 575 finished \tANN training loss 0.094647\n",
      ">> Epoch 576 finished \tANN training loss 0.084321\n",
      ">> Epoch 577 finished \tANN training loss 0.091924\n",
      ">> Epoch 578 finished \tANN training loss 0.093136\n",
      ">> Epoch 579 finished \tANN training loss 0.089087\n",
      ">> Epoch 580 finished \tANN training loss 0.155261\n",
      ">> Epoch 581 finished \tANN training loss 0.089405\n",
      ">> Epoch 582 finished \tANN training loss 0.072953\n",
      ">> Epoch 583 finished \tANN training loss 0.084257\n",
      ">> Epoch 584 finished \tANN training loss 0.083829\n",
      ">> Epoch 585 finished \tANN training loss 0.109964\n",
      ">> Epoch 586 finished \tANN training loss 0.071758\n",
      ">> Epoch 587 finished \tANN training loss 0.069178\n",
      ">> Epoch 588 finished \tANN training loss 0.068512\n",
      ">> Epoch 589 finished \tANN training loss 0.084199\n",
      ">> Epoch 590 finished \tANN training loss 0.103464\n",
      ">> Epoch 591 finished \tANN training loss 0.075539\n",
      ">> Epoch 592 finished \tANN training loss 0.092708\n",
      ">> Epoch 593 finished \tANN training loss 0.076188\n",
      ">> Epoch 594 finished \tANN training loss 0.076616\n",
      ">> Epoch 595 finished \tANN training loss 0.091729\n",
      ">> Epoch 596 finished \tANN training loss 0.088867\n",
      ">> Epoch 597 finished \tANN training loss 0.089646\n",
      ">> Epoch 598 finished \tANN training loss 0.083497\n",
      ">> Epoch 599 finished \tANN training loss 0.072143\n",
      ">> Epoch 600 finished \tANN training loss 0.071694\n",
      ">> Epoch 601 finished \tANN training loss 0.088165\n",
      ">> Epoch 602 finished \tANN training loss 0.080376\n",
      ">> Epoch 603 finished \tANN training loss 0.062471\n",
      ">> Epoch 604 finished \tANN training loss 0.076980\n",
      ">> Epoch 605 finished \tANN training loss 0.067367\n",
      ">> Epoch 606 finished \tANN training loss 0.073993\n",
      ">> Epoch 607 finished \tANN training loss 0.067088\n",
      ">> Epoch 608 finished \tANN training loss 0.065962\n",
      ">> Epoch 609 finished \tANN training loss 0.069570\n",
      ">> Epoch 610 finished \tANN training loss 0.070957\n",
      ">> Epoch 611 finished \tANN training loss 0.077650\n",
      ">> Epoch 612 finished \tANN training loss 0.079053\n",
      ">> Epoch 613 finished \tANN training loss 0.084006\n",
      ">> Epoch 614 finished \tANN training loss 0.066825\n",
      ">> Epoch 615 finished \tANN training loss 0.077725\n",
      ">> Epoch 616 finished \tANN training loss 0.066096\n",
      ">> Epoch 617 finished \tANN training loss 0.078379\n",
      ">> Epoch 618 finished \tANN training loss 0.079732\n",
      ">> Epoch 619 finished \tANN training loss 0.069205\n",
      ">> Epoch 620 finished \tANN training loss 0.091184\n",
      ">> Epoch 621 finished \tANN training loss 0.078972\n",
      ">> Epoch 622 finished \tANN training loss 0.068182\n",
      ">> Epoch 623 finished \tANN training loss 0.089861\n",
      ">> Epoch 624 finished \tANN training loss 0.069621\n",
      ">> Epoch 625 finished \tANN training loss 0.065482\n",
      ">> Epoch 626 finished \tANN training loss 0.080530\n",
      ">> Epoch 627 finished \tANN training loss 0.084882\n",
      ">> Epoch 628 finished \tANN training loss 0.096844\n",
      ">> Epoch 629 finished \tANN training loss 0.076964\n",
      ">> Epoch 630 finished \tANN training loss 0.089458\n",
      ">> Epoch 631 finished \tANN training loss 0.070150\n",
      ">> Epoch 632 finished \tANN training loss 0.070477\n",
      ">> Epoch 633 finished \tANN training loss 0.113942\n",
      ">> Epoch 634 finished \tANN training loss 0.082600\n",
      ">> Epoch 635 finished \tANN training loss 0.069324\n",
      ">> Epoch 636 finished \tANN training loss 0.070733\n",
      ">> Epoch 637 finished \tANN training loss 0.077502\n",
      ">> Epoch 638 finished \tANN training loss 0.104971\n",
      ">> Epoch 639 finished \tANN training loss 0.097706\n",
      ">> Epoch 640 finished \tANN training loss 0.100893\n",
      ">> Epoch 641 finished \tANN training loss 0.100912\n",
      ">> Epoch 642 finished \tANN training loss 0.078854\n",
      ">> Epoch 643 finished \tANN training loss 0.068016\n",
      ">> Epoch 644 finished \tANN training loss 0.071526\n",
      ">> Epoch 645 finished \tANN training loss 0.067246\n",
      ">> Epoch 646 finished \tANN training loss 0.098761\n",
      ">> Epoch 647 finished \tANN training loss 0.081764\n",
      ">> Epoch 648 finished \tANN training loss 0.086040\n",
      ">> Epoch 649 finished \tANN training loss 0.079862\n",
      ">> Epoch 650 finished \tANN training loss 0.081955\n",
      ">> Epoch 651 finished \tANN training loss 0.075488\n",
      ">> Epoch 652 finished \tANN training loss 0.078025\n",
      ">> Epoch 653 finished \tANN training loss 0.078236\n",
      ">> Epoch 654 finished \tANN training loss 0.122925\n",
      ">> Epoch 655 finished \tANN training loss 0.066518\n",
      ">> Epoch 656 finished \tANN training loss 0.070201\n",
      ">> Epoch 657 finished \tANN training loss 0.084739\n",
      ">> Epoch 658 finished \tANN training loss 0.090908\n",
      ">> Epoch 659 finished \tANN training loss 0.067180\n",
      ">> Epoch 660 finished \tANN training loss 0.078449\n",
      ">> Epoch 661 finished \tANN training loss 0.075261\n",
      ">> Epoch 662 finished \tANN training loss 0.068891\n",
      ">> Epoch 663 finished \tANN training loss 0.060085\n",
      ">> Epoch 664 finished \tANN training loss 0.083702\n",
      ">> Epoch 665 finished \tANN training loss 0.078619\n",
      ">> Epoch 666 finished \tANN training loss 0.070822\n",
      ">> Epoch 667 finished \tANN training loss 0.075209\n",
      ">> Epoch 668 finished \tANN training loss 0.074617\n",
      ">> Epoch 669 finished \tANN training loss 0.073050\n",
      ">> Epoch 670 finished \tANN training loss 0.073039\n",
      ">> Epoch 671 finished \tANN training loss 0.062085\n",
      ">> Epoch 672 finished \tANN training loss 0.070893\n",
      ">> Epoch 673 finished \tANN training loss 0.065864\n",
      ">> Epoch 674 finished \tANN training loss 0.084866\n",
      ">> Epoch 675 finished \tANN training loss 0.083503\n",
      ">> Epoch 676 finished \tANN training loss 0.074076\n",
      ">> Epoch 677 finished \tANN training loss 0.075534\n",
      ">> Epoch 678 finished \tANN training loss 0.070968\n",
      ">> Epoch 679 finished \tANN training loss 0.075103\n",
      ">> Epoch 680 finished \tANN training loss 0.061849\n",
      ">> Epoch 681 finished \tANN training loss 0.064396\n",
      ">> Epoch 682 finished \tANN training loss 0.068575\n",
      ">> Epoch 683 finished \tANN training loss 0.068934\n",
      ">> Epoch 684 finished \tANN training loss 0.076365\n",
      ">> Epoch 685 finished \tANN training loss 0.064385\n",
      ">> Epoch 686 finished \tANN training loss 0.065697\n",
      ">> Epoch 687 finished \tANN training loss 0.058708\n",
      ">> Epoch 688 finished \tANN training loss 0.068479\n",
      ">> Epoch 689 finished \tANN training loss 0.060104\n",
      ">> Epoch 690 finished \tANN training loss 0.054276\n",
      ">> Epoch 691 finished \tANN training loss 0.051690\n",
      ">> Epoch 692 finished \tANN training loss 0.070248\n",
      ">> Epoch 693 finished \tANN training loss 0.094626\n",
      ">> Epoch 694 finished \tANN training loss 0.069797\n",
      ">> Epoch 695 finished \tANN training loss 0.059597\n",
      ">> Epoch 696 finished \tANN training loss 0.070556\n",
      ">> Epoch 697 finished \tANN training loss 0.056466\n",
      ">> Epoch 698 finished \tANN training loss 0.067165\n",
      ">> Epoch 699 finished \tANN training loss 0.069827\n",
      ">> Epoch 700 finished \tANN training loss 0.073969\n",
      ">> Epoch 701 finished \tANN training loss 0.074980\n",
      ">> Epoch 702 finished \tANN training loss 0.068048\n",
      ">> Epoch 703 finished \tANN training loss 0.082839\n",
      ">> Epoch 704 finished \tANN training loss 0.067294\n",
      ">> Epoch 705 finished \tANN training loss 0.073250\n",
      ">> Epoch 706 finished \tANN training loss 0.076203\n",
      ">> Epoch 707 finished \tANN training loss 0.083787\n",
      ">> Epoch 708 finished \tANN training loss 0.074285\n",
      ">> Epoch 709 finished \tANN training loss 0.059254\n",
      ">> Epoch 710 finished \tANN training loss 0.062055\n",
      ">> Epoch 711 finished \tANN training loss 0.068449\n",
      ">> Epoch 712 finished \tANN training loss 0.059163\n",
      ">> Epoch 713 finished \tANN training loss 0.058745\n",
      ">> Epoch 714 finished \tANN training loss 0.056490\n",
      ">> Epoch 715 finished \tANN training loss 0.065054\n",
      ">> Epoch 716 finished \tANN training loss 0.067779\n",
      ">> Epoch 717 finished \tANN training loss 0.067388\n",
      ">> Epoch 718 finished \tANN training loss 0.068845\n",
      ">> Epoch 719 finished \tANN training loss 0.064778\n",
      ">> Epoch 720 finished \tANN training loss 0.066725\n",
      ">> Epoch 721 finished \tANN training loss 0.089561\n",
      ">> Epoch 722 finished \tANN training loss 0.091442\n",
      ">> Epoch 723 finished \tANN training loss 0.119933\n",
      ">> Epoch 724 finished \tANN training loss 0.128156\n",
      ">> Epoch 725 finished \tANN training loss 0.071738\n",
      ">> Epoch 726 finished \tANN training loss 0.064306\n",
      ">> Epoch 727 finished \tANN training loss 0.065754\n",
      ">> Epoch 728 finished \tANN training loss 0.115941\n",
      ">> Epoch 729 finished \tANN training loss 0.061369\n",
      ">> Epoch 730 finished \tANN training loss 0.060724\n",
      ">> Epoch 731 finished \tANN training loss 0.065732\n",
      ">> Epoch 732 finished \tANN training loss 0.091936\n",
      ">> Epoch 733 finished \tANN training loss 0.077479\n",
      ">> Epoch 734 finished \tANN training loss 0.071975\n",
      ">> Epoch 735 finished \tANN training loss 0.065410\n",
      ">> Epoch 736 finished \tANN training loss 0.077741\n",
      ">> Epoch 737 finished \tANN training loss 0.063658\n",
      ">> Epoch 738 finished \tANN training loss 0.061369\n",
      ">> Epoch 739 finished \tANN training loss 0.064035\n",
      ">> Epoch 740 finished \tANN training loss 0.066726\n",
      ">> Epoch 741 finished \tANN training loss 0.062314\n",
      ">> Epoch 742 finished \tANN training loss 0.069752\n",
      ">> Epoch 743 finished \tANN training loss 0.087077\n",
      ">> Epoch 744 finished \tANN training loss 0.070263\n",
      ">> Epoch 745 finished \tANN training loss 0.093680\n",
      ">> Epoch 746 finished \tANN training loss 0.066374\n",
      ">> Epoch 747 finished \tANN training loss 0.068247\n",
      ">> Epoch 748 finished \tANN training loss 0.059820\n",
      ">> Epoch 749 finished \tANN training loss 0.072596\n",
      ">> Epoch 750 finished \tANN training loss 0.059222\n",
      ">> Epoch 751 finished \tANN training loss 0.062454\n",
      ">> Epoch 752 finished \tANN training loss 0.074012\n",
      ">> Epoch 753 finished \tANN training loss 0.076190\n",
      ">> Epoch 754 finished \tANN training loss 0.157511\n",
      ">> Epoch 755 finished \tANN training loss 0.085594\n",
      ">> Epoch 756 finished \tANN training loss 0.090216\n",
      ">> Epoch 757 finished \tANN training loss 0.068793\n",
      ">> Epoch 758 finished \tANN training loss 0.065230\n",
      ">> Epoch 759 finished \tANN training loss 0.060891\n",
      ">> Epoch 760 finished \tANN training loss 0.069301\n",
      ">> Epoch 761 finished \tANN training loss 0.065962\n",
      ">> Epoch 762 finished \tANN training loss 0.066080\n",
      ">> Epoch 763 finished \tANN training loss 0.063938\n",
      ">> Epoch 764 finished \tANN training loss 0.076211\n",
      ">> Epoch 765 finished \tANN training loss 0.085088\n",
      ">> Epoch 766 finished \tANN training loss 0.073359\n",
      ">> Epoch 767 finished \tANN training loss 0.068723\n",
      ">> Epoch 768 finished \tANN training loss 0.077269\n",
      ">> Epoch 769 finished \tANN training loss 0.069058\n",
      ">> Epoch 770 finished \tANN training loss 0.071579\n",
      ">> Epoch 771 finished \tANN training loss 0.073389\n",
      ">> Epoch 772 finished \tANN training loss 0.085036\n",
      ">> Epoch 773 finished \tANN training loss 0.058008\n",
      ">> Epoch 774 finished \tANN training loss 0.071777\n",
      ">> Epoch 775 finished \tANN training loss 0.072186\n",
      ">> Epoch 776 finished \tANN training loss 0.078689\n",
      ">> Epoch 777 finished \tANN training loss 0.076602\n",
      ">> Epoch 778 finished \tANN training loss 0.067794\n",
      ">> Epoch 779 finished \tANN training loss 0.068038\n",
      ">> Epoch 780 finished \tANN training loss 0.087556\n",
      ">> Epoch 781 finished \tANN training loss 0.061143\n",
      ">> Epoch 782 finished \tANN training loss 0.072058\n",
      ">> Epoch 783 finished \tANN training loss 0.068663\n",
      ">> Epoch 784 finished \tANN training loss 0.064009\n",
      ">> Epoch 785 finished \tANN training loss 0.076322\n",
      ">> Epoch 786 finished \tANN training loss 0.103260\n",
      ">> Epoch 787 finished \tANN training loss 0.114616\n",
      ">> Epoch 788 finished \tANN training loss 0.071400\n",
      ">> Epoch 789 finished \tANN training loss 0.059525\n",
      ">> Epoch 790 finished \tANN training loss 0.099648\n",
      ">> Epoch 791 finished \tANN training loss 0.097814\n",
      ">> Epoch 792 finished \tANN training loss 0.083179\n",
      ">> Epoch 793 finished \tANN training loss 0.083257\n",
      ">> Epoch 794 finished \tANN training loss 0.090581\n",
      ">> Epoch 795 finished \tANN training loss 0.084368\n",
      ">> Epoch 796 finished \tANN training loss 0.086319\n",
      ">> Epoch 797 finished \tANN training loss 0.071860\n",
      ">> Epoch 798 finished \tANN training loss 0.100054\n",
      ">> Epoch 799 finished \tANN training loss 0.073493\n",
      ">> Epoch 800 finished \tANN training loss 0.072917\n",
      ">> Epoch 801 finished \tANN training loss 0.071396\n",
      ">> Epoch 802 finished \tANN training loss 0.072618\n",
      ">> Epoch 803 finished \tANN training loss 0.069305\n",
      ">> Epoch 804 finished \tANN training loss 0.075312\n",
      ">> Epoch 805 finished \tANN training loss 0.072396\n",
      ">> Epoch 806 finished \tANN training loss 0.079240\n",
      ">> Epoch 807 finished \tANN training loss 0.076476\n",
      ">> Epoch 808 finished \tANN training loss 0.067006\n",
      ">> Epoch 809 finished \tANN training loss 0.106121\n",
      ">> Epoch 810 finished \tANN training loss 0.086757\n",
      ">> Epoch 811 finished \tANN training loss 0.119899\n",
      ">> Epoch 812 finished \tANN training loss 0.128081\n",
      ">> Epoch 813 finished \tANN training loss 0.082622\n",
      ">> Epoch 814 finished \tANN training loss 0.071995\n",
      ">> Epoch 815 finished \tANN training loss 0.118719\n",
      ">> Epoch 816 finished \tANN training loss 0.079184\n",
      ">> Epoch 817 finished \tANN training loss 0.070712\n",
      ">> Epoch 818 finished \tANN training loss 0.093199\n",
      ">> Epoch 819 finished \tANN training loss 0.060550\n",
      ">> Epoch 820 finished \tANN training loss 0.078447\n",
      ">> Epoch 821 finished \tANN training loss 0.063218\n",
      ">> Epoch 822 finished \tANN training loss 0.063686\n",
      ">> Epoch 823 finished \tANN training loss 0.089811\n",
      ">> Epoch 824 finished \tANN training loss 0.061337\n",
      ">> Epoch 825 finished \tANN training loss 0.069187\n",
      ">> Epoch 826 finished \tANN training loss 0.071421\n",
      ">> Epoch 827 finished \tANN training loss 0.077895\n",
      ">> Epoch 828 finished \tANN training loss 0.076245\n",
      ">> Epoch 829 finished \tANN training loss 0.082493\n",
      ">> Epoch 830 finished \tANN training loss 0.080916\n",
      ">> Epoch 831 finished \tANN training loss 0.070660\n",
      ">> Epoch 832 finished \tANN training loss 0.068770\n",
      ">> Epoch 833 finished \tANN training loss 0.060752\n",
      ">> Epoch 834 finished \tANN training loss 0.070198\n",
      ">> Epoch 835 finished \tANN training loss 0.114489\n",
      ">> Epoch 836 finished \tANN training loss 0.087901\n",
      ">> Epoch 837 finished \tANN training loss 0.062360\n",
      ">> Epoch 838 finished \tANN training loss 0.064504\n",
      ">> Epoch 839 finished \tANN training loss 0.062071\n",
      ">> Epoch 840 finished \tANN training loss 0.086818\n",
      ">> Epoch 841 finished \tANN training loss 0.089798\n",
      ">> Epoch 842 finished \tANN training loss 0.084853\n",
      ">> Epoch 843 finished \tANN training loss 0.069736\n",
      ">> Epoch 844 finished \tANN training loss 0.070663\n",
      ">> Epoch 845 finished \tANN training loss 0.064489\n",
      ">> Epoch 846 finished \tANN training loss 0.073966\n",
      ">> Epoch 847 finished \tANN training loss 0.119597\n",
      ">> Epoch 848 finished \tANN training loss 0.077211\n",
      ">> Epoch 849 finished \tANN training loss 0.103673\n",
      ">> Epoch 850 finished \tANN training loss 0.071672\n",
      ">> Epoch 851 finished \tANN training loss 0.194612\n",
      ">> Epoch 852 finished \tANN training loss 0.102090\n",
      ">> Epoch 853 finished \tANN training loss 0.101156\n",
      ">> Epoch 854 finished \tANN training loss 0.077476\n",
      ">> Epoch 855 finished \tANN training loss 0.073109\n",
      ">> Epoch 856 finished \tANN training loss 0.079552\n",
      ">> Epoch 857 finished \tANN training loss 0.061094\n",
      ">> Epoch 858 finished \tANN training loss 0.057252\n",
      ">> Epoch 859 finished \tANN training loss 0.072390\n",
      ">> Epoch 860 finished \tANN training loss 0.166537\n",
      ">> Epoch 861 finished \tANN training loss 0.065077\n",
      ">> Epoch 862 finished \tANN training loss 0.089673\n",
      ">> Epoch 863 finished \tANN training loss 0.063672\n",
      ">> Epoch 864 finished \tANN training loss 0.063089\n",
      ">> Epoch 865 finished \tANN training loss 0.074644\n",
      ">> Epoch 866 finished \tANN training loss 0.060362\n",
      ">> Epoch 867 finished \tANN training loss 0.057897\n",
      ">> Epoch 868 finished \tANN training loss 0.066662\n",
      ">> Epoch 869 finished \tANN training loss 0.071324\n",
      ">> Epoch 870 finished \tANN training loss 0.069015\n",
      ">> Epoch 871 finished \tANN training loss 0.069823\n",
      ">> Epoch 872 finished \tANN training loss 0.059048\n",
      ">> Epoch 873 finished \tANN training loss 0.053998\n",
      ">> Epoch 874 finished \tANN training loss 0.059526\n",
      ">> Epoch 875 finished \tANN training loss 0.060500\n",
      ">> Epoch 876 finished \tANN training loss 0.068767\n",
      ">> Epoch 877 finished \tANN training loss 0.062802\n",
      ">> Epoch 878 finished \tANN training loss 0.075186\n",
      ">> Epoch 879 finished \tANN training loss 0.066026\n",
      ">> Epoch 880 finished \tANN training loss 0.069445\n",
      ">> Epoch 881 finished \tANN training loss 0.075286\n",
      ">> Epoch 882 finished \tANN training loss 0.111930\n",
      ">> Epoch 883 finished \tANN training loss 0.075115\n",
      ">> Epoch 884 finished \tANN training loss 0.080957\n",
      ">> Epoch 885 finished \tANN training loss 0.068674\n",
      ">> Epoch 886 finished \tANN training loss 0.067879\n",
      ">> Epoch 887 finished \tANN training loss 0.077713\n",
      ">> Epoch 888 finished \tANN training loss 0.061872\n",
      ">> Epoch 889 finished \tANN training loss 0.059040\n",
      ">> Epoch 890 finished \tANN training loss 0.065140\n",
      ">> Epoch 891 finished \tANN training loss 0.066099\n",
      ">> Epoch 892 finished \tANN training loss 0.064004\n",
      ">> Epoch 893 finished \tANN training loss 0.065822\n",
      ">> Epoch 894 finished \tANN training loss 0.075378\n",
      ">> Epoch 895 finished \tANN training loss 0.063897\n",
      ">> Epoch 896 finished \tANN training loss 0.066078\n",
      ">> Epoch 897 finished \tANN training loss 0.085486\n",
      ">> Epoch 898 finished \tANN training loss 0.062176\n",
      ">> Epoch 899 finished \tANN training loss 0.061791\n",
      ">> Epoch 900 finished \tANN training loss 0.069634\n",
      ">> Epoch 901 finished \tANN training loss 0.070369\n",
      ">> Epoch 902 finished \tANN training loss 0.066309\n",
      ">> Epoch 903 finished \tANN training loss 0.062580\n",
      ">> Epoch 904 finished \tANN training loss 0.061348\n",
      ">> Epoch 905 finished \tANN training loss 0.063720\n",
      ">> Epoch 906 finished \tANN training loss 0.055571\n",
      ">> Epoch 907 finished \tANN training loss 0.057744\n",
      ">> Epoch 908 finished \tANN training loss 0.056647\n",
      ">> Epoch 909 finished \tANN training loss 0.069890\n",
      ">> Epoch 910 finished \tANN training loss 0.058960\n",
      ">> Epoch 911 finished \tANN training loss 0.071157\n",
      ">> Epoch 912 finished \tANN training loss 0.072933\n",
      ">> Epoch 913 finished \tANN training loss 0.056689\n",
      ">> Epoch 914 finished \tANN training loss 0.054081\n",
      ">> Epoch 915 finished \tANN training loss 0.062400\n",
      ">> Epoch 916 finished \tANN training loss 0.072825\n",
      ">> Epoch 917 finished \tANN training loss 0.061340\n",
      ">> Epoch 918 finished \tANN training loss 0.072194\n",
      ">> Epoch 919 finished \tANN training loss 0.058318\n",
      ">> Epoch 920 finished \tANN training loss 0.064060\n",
      ">> Epoch 921 finished \tANN training loss 0.051282\n",
      ">> Epoch 922 finished \tANN training loss 0.057117\n",
      ">> Epoch 923 finished \tANN training loss 0.064247\n",
      ">> Epoch 924 finished \tANN training loss 0.060625\n",
      ">> Epoch 925 finished \tANN training loss 0.068553\n",
      ">> Epoch 926 finished \tANN training loss 0.085549\n",
      ">> Epoch 927 finished \tANN training loss 0.060193\n",
      ">> Epoch 928 finished \tANN training loss 0.069418\n",
      ">> Epoch 929 finished \tANN training loss 0.063456\n",
      ">> Epoch 930 finished \tANN training loss 0.066292\n",
      ">> Epoch 931 finished \tANN training loss 0.055499\n",
      ">> Epoch 932 finished \tANN training loss 0.083751\n",
      ">> Epoch 933 finished \tANN training loss 0.068454\n",
      ">> Epoch 934 finished \tANN training loss 0.070031\n",
      ">> Epoch 935 finished \tANN training loss 0.067554\n",
      ">> Epoch 936 finished \tANN training loss 0.058135\n",
      ">> Epoch 937 finished \tANN training loss 0.057084\n",
      ">> Epoch 938 finished \tANN training loss 0.053410\n",
      ">> Epoch 939 finished \tANN training loss 0.089222\n",
      ">> Epoch 940 finished \tANN training loss 0.056726\n",
      ">> Epoch 941 finished \tANN training loss 0.060651\n",
      ">> Epoch 942 finished \tANN training loss 0.061707\n",
      ">> Epoch 943 finished \tANN training loss 0.052384\n",
      ">> Epoch 944 finished \tANN training loss 0.058832\n",
      ">> Epoch 945 finished \tANN training loss 0.062564\n",
      ">> Epoch 946 finished \tANN training loss 0.055626\n",
      ">> Epoch 947 finished \tANN training loss 0.071782\n",
      ">> Epoch 948 finished \tANN training loss 0.060157\n",
      ">> Epoch 949 finished \tANN training loss 0.070561\n",
      ">> Epoch 950 finished \tANN training loss 0.052515\n",
      ">> Epoch 951 finished \tANN training loss 0.053966\n",
      ">> Epoch 952 finished \tANN training loss 0.062799\n",
      ">> Epoch 953 finished \tANN training loss 0.060548\n",
      ">> Epoch 954 finished \tANN training loss 0.064066\n",
      ">> Epoch 955 finished \tANN training loss 0.067310\n",
      ">> Epoch 956 finished \tANN training loss 0.072259\n",
      ">> Epoch 957 finished \tANN training loss 0.069719\n",
      ">> Epoch 958 finished \tANN training loss 0.055984\n",
      ">> Epoch 959 finished \tANN training loss 0.053302\n",
      ">> Epoch 960 finished \tANN training loss 0.060123\n",
      ">> Epoch 961 finished \tANN training loss 0.056421\n",
      ">> Epoch 962 finished \tANN training loss 0.057024\n",
      ">> Epoch 963 finished \tANN training loss 0.059348\n",
      ">> Epoch 964 finished \tANN training loss 0.068288\n",
      ">> Epoch 965 finished \tANN training loss 0.057882\n",
      ">> Epoch 966 finished \tANN training loss 0.058934\n",
      ">> Epoch 967 finished \tANN training loss 0.051232\n",
      ">> Epoch 968 finished \tANN training loss 0.067880\n",
      ">> Epoch 969 finished \tANN training loss 0.063314\n",
      ">> Epoch 970 finished \tANN training loss 0.098895\n",
      ">> Epoch 971 finished \tANN training loss 0.066668\n",
      ">> Epoch 972 finished \tANN training loss 0.063512\n",
      ">> Epoch 973 finished \tANN training loss 0.064062\n",
      ">> Epoch 974 finished \tANN training loss 0.056026\n",
      ">> Epoch 975 finished \tANN training loss 0.069121\n",
      ">> Epoch 976 finished \tANN training loss 0.057954\n",
      ">> Epoch 977 finished \tANN training loss 0.061710\n",
      ">> Epoch 978 finished \tANN training loss 0.063720\n",
      ">> Epoch 979 finished \tANN training loss 0.079054\n",
      ">> Epoch 980 finished \tANN training loss 0.056620\n",
      ">> Epoch 981 finished \tANN training loss 0.043474\n",
      ">> Epoch 982 finished \tANN training loss 0.054944\n",
      ">> Epoch 983 finished \tANN training loss 0.069400\n",
      ">> Epoch 984 finished \tANN training loss 0.061480\n",
      ">> Epoch 985 finished \tANN training loss 0.057309\n",
      ">> Epoch 986 finished \tANN training loss 0.056699\n",
      ">> Epoch 987 finished \tANN training loss 0.052775\n",
      ">> Epoch 988 finished \tANN training loss 0.053548\n",
      ">> Epoch 989 finished \tANN training loss 0.053396\n",
      ">> Epoch 990 finished \tANN training loss 0.055109\n",
      ">> Epoch 991 finished \tANN training loss 0.057226\n",
      ">> Epoch 992 finished \tANN training loss 0.049450\n",
      ">> Epoch 993 finished \tANN training loss 0.049106\n",
      ">> Epoch 994 finished \tANN training loss 0.073808\n",
      ">> Epoch 995 finished \tANN training loss 0.069057\n",
      ">> Epoch 996 finished \tANN training loss 0.076149\n",
      ">> Epoch 997 finished \tANN training loss 0.086072\n",
      ">> Epoch 998 finished \tANN training loss 0.066830\n",
      ">> Epoch 999 finished \tANN training loss 0.071992\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 9.013428\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 10.290607\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 10.088646\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 10.563734\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 10.559401\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 9.867099\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 9.877441\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 9.570557\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 9.159077\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 8.459608\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 8.483793\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 8.373168\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 8.056385\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 8.161317\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 7.755428\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 7.922635\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 8.062221\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 8.117504\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 7.804533\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 8.137622\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.819118\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 6.322993\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 6.274534\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5.882557\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.715399\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 6.926053\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5.935875\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 7.385069\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 7.072820\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 6.551059\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 7.753427\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 6.477231\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 6.837061\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 6.323197\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 6.997414\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 6.876644\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 7.765156\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 8.263816\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 7.846244\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 7.925488\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.988297\n",
      ">> Epoch 1 finished \tANN training loss 0.980504\n",
      ">> Epoch 2 finished \tANN training loss 0.846440\n",
      ">> Epoch 3 finished \tANN training loss 0.857251\n",
      ">> Epoch 4 finished \tANN training loss 0.770869\n",
      ">> Epoch 5 finished \tANN training loss 0.715842\n",
      ">> Epoch 6 finished \tANN training loss 0.670924\n",
      ">> Epoch 7 finished \tANN training loss 0.667140\n",
      ">> Epoch 8 finished \tANN training loss 0.620203\n",
      ">> Epoch 9 finished \tANN training loss 0.619504\n",
      ">> Epoch 10 finished \tANN training loss 0.587964\n",
      ">> Epoch 11 finished \tANN training loss 0.573895\n",
      ">> Epoch 12 finished \tANN training loss 0.539558\n",
      ">> Epoch 13 finished \tANN training loss 0.537299\n",
      ">> Epoch 14 finished \tANN training loss 0.556330\n",
      ">> Epoch 15 finished \tANN training loss 0.504619\n",
      ">> Epoch 16 finished \tANN training loss 0.509821\n",
      ">> Epoch 17 finished \tANN training loss 0.653249\n",
      ">> Epoch 18 finished \tANN training loss 0.489530\n",
      ">> Epoch 19 finished \tANN training loss 0.458950\n",
      ">> Epoch 20 finished \tANN training loss 0.471580\n",
      ">> Epoch 21 finished \tANN training loss 0.448554\n",
      ">> Epoch 22 finished \tANN training loss 0.484861\n",
      ">> Epoch 23 finished \tANN training loss 0.453211\n",
      ">> Epoch 24 finished \tANN training loss 0.426107\n",
      ">> Epoch 25 finished \tANN training loss 0.476534\n",
      ">> Epoch 26 finished \tANN training loss 0.425094\n",
      ">> Epoch 27 finished \tANN training loss 0.459113\n",
      ">> Epoch 28 finished \tANN training loss 0.466059\n",
      ">> Epoch 29 finished \tANN training loss 0.497255\n",
      ">> Epoch 30 finished \tANN training loss 0.401490\n",
      ">> Epoch 31 finished \tANN training loss 0.406600\n",
      ">> Epoch 32 finished \tANN training loss 0.427717\n",
      ">> Epoch 33 finished \tANN training loss 0.394292\n",
      ">> Epoch 34 finished \tANN training loss 0.414529\n",
      ">> Epoch 35 finished \tANN training loss 0.373388\n",
      ">> Epoch 36 finished \tANN training loss 0.369101\n",
      ">> Epoch 37 finished \tANN training loss 0.384436\n",
      ">> Epoch 38 finished \tANN training loss 0.353762\n",
      ">> Epoch 39 finished \tANN training loss 0.350507\n",
      ">> Epoch 40 finished \tANN training loss 0.345578\n",
      ">> Epoch 41 finished \tANN training loss 0.360427\n",
      ">> Epoch 42 finished \tANN training loss 0.342101\n",
      ">> Epoch 43 finished \tANN training loss 0.339006\n",
      ">> Epoch 44 finished \tANN training loss 0.392924\n",
      ">> Epoch 45 finished \tANN training loss 0.352386\n",
      ">> Epoch 46 finished \tANN training loss 0.380172\n",
      ">> Epoch 47 finished \tANN training loss 0.336378\n",
      ">> Epoch 48 finished \tANN training loss 0.318850\n",
      ">> Epoch 49 finished \tANN training loss 0.330770\n",
      ">> Epoch 50 finished \tANN training loss 0.347477\n",
      ">> Epoch 51 finished \tANN training loss 0.307322\n",
      ">> Epoch 52 finished \tANN training loss 0.331913\n",
      ">> Epoch 53 finished \tANN training loss 0.293942\n",
      ">> Epoch 54 finished \tANN training loss 0.287131\n",
      ">> Epoch 55 finished \tANN training loss 0.296215\n",
      ">> Epoch 56 finished \tANN training loss 0.350597\n",
      ">> Epoch 57 finished \tANN training loss 0.271705\n",
      ">> Epoch 58 finished \tANN training loss 0.276957\n",
      ">> Epoch 59 finished \tANN training loss 0.308461\n",
      ">> Epoch 60 finished \tANN training loss 0.424512\n",
      ">> Epoch 61 finished \tANN training loss 0.269692\n",
      ">> Epoch 62 finished \tANN training loss 0.247021\n",
      ">> Epoch 63 finished \tANN training loss 0.346837\n",
      ">> Epoch 64 finished \tANN training loss 0.241963\n",
      ">> Epoch 65 finished \tANN training loss 0.283840\n",
      ">> Epoch 66 finished \tANN training loss 0.379766\n",
      ">> Epoch 67 finished \tANN training loss 0.261410\n",
      ">> Epoch 68 finished \tANN training loss 0.293646\n",
      ">> Epoch 69 finished \tANN training loss 0.272084\n",
      ">> Epoch 70 finished \tANN training loss 0.220020\n",
      ">> Epoch 71 finished \tANN training loss 0.236006\n",
      ">> Epoch 72 finished \tANN training loss 0.223178\n",
      ">> Epoch 73 finished \tANN training loss 0.304978\n",
      ">> Epoch 74 finished \tANN training loss 0.212983\n",
      ">> Epoch 75 finished \tANN training loss 0.269069\n",
      ">> Epoch 76 finished \tANN training loss 0.227598\n",
      ">> Epoch 77 finished \tANN training loss 0.223266\n",
      ">> Epoch 78 finished \tANN training loss 0.256705\n",
      ">> Epoch 79 finished \tANN training loss 0.226634\n",
      ">> Epoch 80 finished \tANN training loss 0.201646\n",
      ">> Epoch 81 finished \tANN training loss 0.219788\n",
      ">> Epoch 82 finished \tANN training loss 0.227122\n",
      ">> Epoch 83 finished \tANN training loss 0.216088\n",
      ">> Epoch 84 finished \tANN training loss 0.271125\n",
      ">> Epoch 85 finished \tANN training loss 0.239178\n",
      ">> Epoch 86 finished \tANN training loss 0.208446\n",
      ">> Epoch 87 finished \tANN training loss 0.188502\n",
      ">> Epoch 88 finished \tANN training loss 0.431945\n",
      ">> Epoch 89 finished \tANN training loss 0.248868\n",
      ">> Epoch 90 finished \tANN training loss 0.208432\n",
      ">> Epoch 91 finished \tANN training loss 0.199257\n",
      ">> Epoch 92 finished \tANN training loss 0.198460\n",
      ">> Epoch 93 finished \tANN training loss 0.196116\n",
      ">> Epoch 94 finished \tANN training loss 0.197342\n",
      ">> Epoch 95 finished \tANN training loss 0.184313\n",
      ">> Epoch 96 finished \tANN training loss 0.190823\n",
      ">> Epoch 97 finished \tANN training loss 0.193483\n",
      ">> Epoch 98 finished \tANN training loss 0.187048\n",
      ">> Epoch 99 finished \tANN training loss 0.253857\n",
      ">> Epoch 100 finished \tANN training loss 0.186907\n",
      ">> Epoch 101 finished \tANN training loss 0.549243\n",
      ">> Epoch 102 finished \tANN training loss 0.181301\n",
      ">> Epoch 103 finished \tANN training loss 0.167323\n",
      ">> Epoch 104 finished \tANN training loss 0.225241\n",
      ">> Epoch 105 finished \tANN training loss 0.166009\n",
      ">> Epoch 106 finished \tANN training loss 0.167327\n",
      ">> Epoch 107 finished \tANN training loss 0.217494\n",
      ">> Epoch 108 finished \tANN training loss 0.167469\n",
      ">> Epoch 109 finished \tANN training loss 0.175709\n",
      ">> Epoch 110 finished \tANN training loss 0.192341\n",
      ">> Epoch 111 finished \tANN training loss 0.179170\n",
      ">> Epoch 112 finished \tANN training loss 0.166421\n",
      ">> Epoch 113 finished \tANN training loss 0.166655\n",
      ">> Epoch 114 finished \tANN training loss 0.159260\n",
      ">> Epoch 115 finished \tANN training loss 0.158085\n",
      ">> Epoch 116 finished \tANN training loss 0.168218\n",
      ">> Epoch 117 finished \tANN training loss 0.159796\n",
      ">> Epoch 118 finished \tANN training loss 0.173542\n",
      ">> Epoch 119 finished \tANN training loss 0.250293\n",
      ">> Epoch 120 finished \tANN training loss 0.165981\n",
      ">> Epoch 121 finished \tANN training loss 0.147832\n",
      ">> Epoch 122 finished \tANN training loss 0.145810\n",
      ">> Epoch 123 finished \tANN training loss 0.258609\n",
      ">> Epoch 124 finished \tANN training loss 0.228968\n",
      ">> Epoch 125 finished \tANN training loss 0.167590\n",
      ">> Epoch 126 finished \tANN training loss 0.169734\n",
      ">> Epoch 127 finished \tANN training loss 0.238038\n",
      ">> Epoch 128 finished \tANN training loss 0.158960\n",
      ">> Epoch 129 finished \tANN training loss 0.152376\n",
      ">> Epoch 130 finished \tANN training loss 0.162666\n",
      ">> Epoch 131 finished \tANN training loss 0.230594\n",
      ">> Epoch 132 finished \tANN training loss 0.161933\n",
      ">> Epoch 133 finished \tANN training loss 0.156047\n",
      ">> Epoch 134 finished \tANN training loss 0.158311\n",
      ">> Epoch 135 finished \tANN training loss 0.139438\n",
      ">> Epoch 136 finished \tANN training loss 0.163445\n",
      ">> Epoch 137 finished \tANN training loss 0.187863\n",
      ">> Epoch 138 finished \tANN training loss 0.177985\n",
      ">> Epoch 139 finished \tANN training loss 0.177824\n",
      ">> Epoch 140 finished \tANN training loss 0.143815\n",
      ">> Epoch 141 finished \tANN training loss 0.145064\n",
      ">> Epoch 142 finished \tANN training loss 0.151149\n",
      ">> Epoch 143 finished \tANN training loss 0.130688\n",
      ">> Epoch 144 finished \tANN training loss 0.150093\n",
      ">> Epoch 145 finished \tANN training loss 0.178952\n",
      ">> Epoch 146 finished \tANN training loss 0.169011\n",
      ">> Epoch 147 finished \tANN training loss 0.134336\n",
      ">> Epoch 148 finished \tANN training loss 0.141308\n",
      ">> Epoch 149 finished \tANN training loss 0.150380\n",
      ">> Epoch 150 finished \tANN training loss 0.149743\n",
      ">> Epoch 151 finished \tANN training loss 0.185933\n",
      ">> Epoch 152 finished \tANN training loss 0.132426\n",
      ">> Epoch 153 finished \tANN training loss 0.134759\n",
      ">> Epoch 154 finished \tANN training loss 0.129426\n",
      ">> Epoch 155 finished \tANN training loss 0.131177\n",
      ">> Epoch 156 finished \tANN training loss 0.147030\n",
      ">> Epoch 157 finished \tANN training loss 0.125942\n",
      ">> Epoch 158 finished \tANN training loss 0.130773\n",
      ">> Epoch 159 finished \tANN training loss 0.116623\n",
      ">> Epoch 160 finished \tANN training loss 0.189410\n",
      ">> Epoch 161 finished \tANN training loss 0.153820\n",
      ">> Epoch 162 finished \tANN training loss 0.264793\n",
      ">> Epoch 163 finished \tANN training loss 0.167208\n",
      ">> Epoch 164 finished \tANN training loss 0.130269\n",
      ">> Epoch 165 finished \tANN training loss 0.127633\n",
      ">> Epoch 166 finished \tANN training loss 0.225979\n",
      ">> Epoch 167 finished \tANN training loss 0.136602\n",
      ">> Epoch 168 finished \tANN training loss 0.129717\n",
      ">> Epoch 169 finished \tANN training loss 0.171373\n",
      ">> Epoch 170 finished \tANN training loss 0.119881\n",
      ">> Epoch 171 finished \tANN training loss 0.142991\n",
      ">> Epoch 172 finished \tANN training loss 0.163281\n",
      ">> Epoch 173 finished \tANN training loss 0.111540\n",
      ">> Epoch 174 finished \tANN training loss 0.118722\n",
      ">> Epoch 175 finished \tANN training loss 0.108142\n",
      ">> Epoch 176 finished \tANN training loss 0.134323\n",
      ">> Epoch 177 finished \tANN training loss 0.103259\n",
      ">> Epoch 178 finished \tANN training loss 0.104724\n",
      ">> Epoch 179 finished \tANN training loss 0.113554\n",
      ">> Epoch 180 finished \tANN training loss 0.168557\n",
      ">> Epoch 181 finished \tANN training loss 0.128603\n",
      ">> Epoch 182 finished \tANN training loss 0.139252\n",
      ">> Epoch 183 finished \tANN training loss 0.142632\n",
      ">> Epoch 184 finished \tANN training loss 0.112382\n",
      ">> Epoch 185 finished \tANN training loss 0.118326\n",
      ">> Epoch 186 finished \tANN training loss 0.105631\n",
      ">> Epoch 187 finished \tANN training loss 0.185307\n",
      ">> Epoch 188 finished \tANN training loss 0.106077\n",
      ">> Epoch 189 finished \tANN training loss 0.105240\n",
      ">> Epoch 190 finished \tANN training loss 0.105702\n",
      ">> Epoch 191 finished \tANN training loss 0.117847\n",
      ">> Epoch 192 finished \tANN training loss 0.120750\n",
      ">> Epoch 193 finished \tANN training loss 0.149313\n",
      ">> Epoch 194 finished \tANN training loss 0.132422\n",
      ">> Epoch 195 finished \tANN training loss 0.110109\n",
      ">> Epoch 196 finished \tANN training loss 0.245338\n",
      ">> Epoch 197 finished \tANN training loss 0.140518\n",
      ">> Epoch 198 finished \tANN training loss 0.131243\n",
      ">> Epoch 199 finished \tANN training loss 0.115511\n",
      ">> Epoch 200 finished \tANN training loss 0.097617\n",
      ">> Epoch 201 finished \tANN training loss 0.111221\n",
      ">> Epoch 202 finished \tANN training loss 0.108148\n",
      ">> Epoch 203 finished \tANN training loss 0.126318\n",
      ">> Epoch 204 finished \tANN training loss 0.117286\n",
      ">> Epoch 205 finished \tANN training loss 0.096775\n",
      ">> Epoch 206 finished \tANN training loss 0.161204\n",
      ">> Epoch 207 finished \tANN training loss 0.097769\n",
      ">> Epoch 208 finished \tANN training loss 0.109550\n",
      ">> Epoch 209 finished \tANN training loss 0.095268\n",
      ">> Epoch 210 finished \tANN training loss 0.096046\n",
      ">> Epoch 211 finished \tANN training loss 0.103206\n",
      ">> Epoch 212 finished \tANN training loss 0.109440\n",
      ">> Epoch 213 finished \tANN training loss 0.095318\n",
      ">> Epoch 214 finished \tANN training loss 0.090224\n",
      ">> Epoch 215 finished \tANN training loss 0.101439\n",
      ">> Epoch 216 finished \tANN training loss 0.100759\n",
      ">> Epoch 217 finished \tANN training loss 0.167450\n",
      ">> Epoch 218 finished \tANN training loss 0.111084\n",
      ">> Epoch 219 finished \tANN training loss 0.103983\n",
      ">> Epoch 220 finished \tANN training loss 0.107953\n",
      ">> Epoch 221 finished \tANN training loss 0.131296\n",
      ">> Epoch 222 finished \tANN training loss 0.094215\n",
      ">> Epoch 223 finished \tANN training loss 0.115676\n",
      ">> Epoch 224 finished \tANN training loss 0.084661\n",
      ">> Epoch 225 finished \tANN training loss 0.088362\n",
      ">> Epoch 226 finished \tANN training loss 0.094946\n",
      ">> Epoch 227 finished \tANN training loss 0.131566\n",
      ">> Epoch 228 finished \tANN training loss 0.091621\n",
      ">> Epoch 229 finished \tANN training loss 0.101722\n",
      ">> Epoch 230 finished \tANN training loss 0.127201\n",
      ">> Epoch 231 finished \tANN training loss 0.128361\n",
      ">> Epoch 232 finished \tANN training loss 0.172092\n",
      ">> Epoch 233 finished \tANN training loss 0.105450\n",
      ">> Epoch 234 finished \tANN training loss 0.100359\n",
      ">> Epoch 235 finished \tANN training loss 0.090773\n",
      ">> Epoch 236 finished \tANN training loss 0.089798\n",
      ">> Epoch 237 finished \tANN training loss 0.083725\n",
      ">> Epoch 238 finished \tANN training loss 0.099697\n",
      ">> Epoch 239 finished \tANN training loss 0.097872\n",
      ">> Epoch 240 finished \tANN training loss 0.088536\n",
      ">> Epoch 241 finished \tANN training loss 0.101480\n",
      ">> Epoch 242 finished \tANN training loss 0.114074\n",
      ">> Epoch 243 finished \tANN training loss 0.118489\n",
      ">> Epoch 244 finished \tANN training loss 0.085337\n",
      ">> Epoch 245 finished \tANN training loss 0.090408\n",
      ">> Epoch 246 finished \tANN training loss 0.089664\n",
      ">> Epoch 247 finished \tANN training loss 0.100161\n",
      ">> Epoch 248 finished \tANN training loss 0.084983\n",
      ">> Epoch 249 finished \tANN training loss 0.086287\n",
      ">> Epoch 250 finished \tANN training loss 0.087995\n",
      ">> Epoch 251 finished \tANN training loss 0.102896\n",
      ">> Epoch 252 finished \tANN training loss 0.129815\n",
      ">> Epoch 253 finished \tANN training loss 0.087386\n",
      ">> Epoch 254 finished \tANN training loss 0.097123\n",
      ">> Epoch 255 finished \tANN training loss 0.097912\n",
      ">> Epoch 256 finished \tANN training loss 0.098084\n",
      ">> Epoch 257 finished \tANN training loss 0.126322\n",
      ">> Epoch 258 finished \tANN training loss 0.100748\n",
      ">> Epoch 259 finished \tANN training loss 0.090712\n",
      ">> Epoch 260 finished \tANN training loss 0.080245\n",
      ">> Epoch 261 finished \tANN training loss 0.102943\n",
      ">> Epoch 262 finished \tANN training loss 0.082246\n",
      ">> Epoch 263 finished \tANN training loss 0.080179\n",
      ">> Epoch 264 finished \tANN training loss 0.116850\n",
      ">> Epoch 265 finished \tANN training loss 0.095138\n",
      ">> Epoch 266 finished \tANN training loss 0.093712\n",
      ">> Epoch 267 finished \tANN training loss 0.167548\n",
      ">> Epoch 268 finished \tANN training loss 0.089488\n",
      ">> Epoch 269 finished \tANN training loss 0.103180\n",
      ">> Epoch 270 finished \tANN training loss 0.100496\n",
      ">> Epoch 271 finished \tANN training loss 0.092429\n",
      ">> Epoch 272 finished \tANN training loss 0.130449\n",
      ">> Epoch 273 finished \tANN training loss 0.094298\n",
      ">> Epoch 274 finished \tANN training loss 0.109439\n",
      ">> Epoch 275 finished \tANN training loss 0.092095\n",
      ">> Epoch 276 finished \tANN training loss 0.145582\n",
      ">> Epoch 277 finished \tANN training loss 0.083674\n",
      ">> Epoch 278 finished \tANN training loss 0.091111\n",
      ">> Epoch 279 finished \tANN training loss 0.137137\n",
      ">> Epoch 280 finished \tANN training loss 0.120429\n",
      ">> Epoch 281 finished \tANN training loss 0.122864\n",
      ">> Epoch 282 finished \tANN training loss 0.105707\n",
      ">> Epoch 283 finished \tANN training loss 0.152360\n",
      ">> Epoch 284 finished \tANN training loss 0.100715\n",
      ">> Epoch 285 finished \tANN training loss 0.092274\n",
      ">> Epoch 286 finished \tANN training loss 0.152257\n",
      ">> Epoch 287 finished \tANN training loss 0.133650\n",
      ">> Epoch 288 finished \tANN training loss 0.109414\n",
      ">> Epoch 289 finished \tANN training loss 0.109007\n",
      ">> Epoch 290 finished \tANN training loss 0.093162\n",
      ">> Epoch 291 finished \tANN training loss 0.122336\n",
      ">> Epoch 292 finished \tANN training loss 0.105568\n",
      ">> Epoch 293 finished \tANN training loss 0.092949\n",
      ">> Epoch 294 finished \tANN training loss 0.106777\n",
      ">> Epoch 295 finished \tANN training loss 0.095811\n",
      ">> Epoch 296 finished \tANN training loss 0.082761\n",
      ">> Epoch 297 finished \tANN training loss 0.086841\n",
      ">> Epoch 298 finished \tANN training loss 0.085183\n",
      ">> Epoch 299 finished \tANN training loss 0.095057\n",
      ">> Epoch 300 finished \tANN training loss 0.102174\n",
      ">> Epoch 301 finished \tANN training loss 0.076836\n",
      ">> Epoch 302 finished \tANN training loss 0.088599\n",
      ">> Epoch 303 finished \tANN training loss 0.089034\n",
      ">> Epoch 304 finished \tANN training loss 0.088749\n",
      ">> Epoch 305 finished \tANN training loss 0.083202\n",
      ">> Epoch 306 finished \tANN training loss 0.090927\n",
      ">> Epoch 307 finished \tANN training loss 0.078486\n",
      ">> Epoch 308 finished \tANN training loss 0.123980\n",
      ">> Epoch 309 finished \tANN training loss 0.105375\n",
      ">> Epoch 310 finished \tANN training loss 0.141157\n",
      ">> Epoch 311 finished \tANN training loss 0.086080\n",
      ">> Epoch 312 finished \tANN training loss 0.099152\n",
      ">> Epoch 313 finished \tANN training loss 0.089915\n",
      ">> Epoch 314 finished \tANN training loss 0.095676\n",
      ">> Epoch 315 finished \tANN training loss 0.089406\n",
      ">> Epoch 316 finished \tANN training loss 0.085594\n",
      ">> Epoch 317 finished \tANN training loss 0.090928\n",
      ">> Epoch 318 finished \tANN training loss 0.078592\n",
      ">> Epoch 319 finished \tANN training loss 0.088101\n",
      ">> Epoch 320 finished \tANN training loss 0.092179\n",
      ">> Epoch 321 finished \tANN training loss 0.100176\n",
      ">> Epoch 322 finished \tANN training loss 0.084619\n",
      ">> Epoch 323 finished \tANN training loss 0.159900\n",
      ">> Epoch 324 finished \tANN training loss 0.218481\n",
      ">> Epoch 325 finished \tANN training loss 0.097361\n",
      ">> Epoch 326 finished \tANN training loss 0.079630\n",
      ">> Epoch 327 finished \tANN training loss 0.079850\n",
      ">> Epoch 328 finished \tANN training loss 0.084350\n",
      ">> Epoch 329 finished \tANN training loss 0.087796\n",
      ">> Epoch 330 finished \tANN training loss 0.096226\n",
      ">> Epoch 331 finished \tANN training loss 0.072337\n",
      ">> Epoch 332 finished \tANN training loss 0.072763\n",
      ">> Epoch 333 finished \tANN training loss 0.106971\n",
      ">> Epoch 334 finished \tANN training loss 0.237417\n",
      ">> Epoch 335 finished \tANN training loss 0.081392\n",
      ">> Epoch 336 finished \tANN training loss 0.078274\n",
      ">> Epoch 337 finished \tANN training loss 0.079768\n",
      ">> Epoch 338 finished \tANN training loss 0.071102\n",
      ">> Epoch 339 finished \tANN training loss 0.074026\n",
      ">> Epoch 340 finished \tANN training loss 0.074774\n",
      ">> Epoch 341 finished \tANN training loss 0.087416\n",
      ">> Epoch 342 finished \tANN training loss 0.084534\n",
      ">> Epoch 343 finished \tANN training loss 0.068645\n",
      ">> Epoch 344 finished \tANN training loss 0.122687\n",
      ">> Epoch 345 finished \tANN training loss 0.138478\n",
      ">> Epoch 346 finished \tANN training loss 0.078917\n",
      ">> Epoch 347 finished \tANN training loss 0.095347\n",
      ">> Epoch 348 finished \tANN training loss 0.076215\n",
      ">> Epoch 349 finished \tANN training loss 0.081405\n",
      ">> Epoch 350 finished \tANN training loss 0.103101\n",
      ">> Epoch 351 finished \tANN training loss 0.211785\n",
      ">> Epoch 352 finished \tANN training loss 0.098844\n",
      ">> Epoch 353 finished \tANN training loss 0.074425\n",
      ">> Epoch 354 finished \tANN training loss 0.075977\n",
      ">> Epoch 355 finished \tANN training loss 0.078683\n",
      ">> Epoch 356 finished \tANN training loss 0.117288\n",
      ">> Epoch 357 finished \tANN training loss 0.092168\n",
      ">> Epoch 358 finished \tANN training loss 0.081105\n",
      ">> Epoch 359 finished \tANN training loss 0.093353\n",
      ">> Epoch 360 finished \tANN training loss 0.072270\n",
      ">> Epoch 361 finished \tANN training loss 0.073052\n",
      ">> Epoch 362 finished \tANN training loss 0.076544\n",
      ">> Epoch 363 finished \tANN training loss 0.066739\n",
      ">> Epoch 364 finished \tANN training loss 0.072200\n",
      ">> Epoch 365 finished \tANN training loss 0.070726\n",
      ">> Epoch 366 finished \tANN training loss 0.072468\n",
      ">> Epoch 367 finished \tANN training loss 0.095639\n",
      ">> Epoch 368 finished \tANN training loss 0.075358\n",
      ">> Epoch 369 finished \tANN training loss 0.089580\n",
      ">> Epoch 370 finished \tANN training loss 0.103640\n",
      ">> Epoch 371 finished \tANN training loss 0.094909\n",
      ">> Epoch 372 finished \tANN training loss 0.082399\n",
      ">> Epoch 373 finished \tANN training loss 0.080021\n",
      ">> Epoch 374 finished \tANN training loss 0.078423\n",
      ">> Epoch 375 finished \tANN training loss 0.183171\n",
      ">> Epoch 376 finished \tANN training loss 0.088599\n",
      ">> Epoch 377 finished \tANN training loss 0.085261\n",
      ">> Epoch 378 finished \tANN training loss 0.146227\n",
      ">> Epoch 379 finished \tANN training loss 0.083440\n",
      ">> Epoch 380 finished \tANN training loss 0.090730\n",
      ">> Epoch 381 finished \tANN training loss 0.082736\n",
      ">> Epoch 382 finished \tANN training loss 0.091520\n",
      ">> Epoch 383 finished \tANN training loss 0.082635\n",
      ">> Epoch 384 finished \tANN training loss 0.079581\n",
      ">> Epoch 385 finished \tANN training loss 0.072626\n",
      ">> Epoch 386 finished \tANN training loss 0.069875\n",
      ">> Epoch 387 finished \tANN training loss 0.087554\n",
      ">> Epoch 388 finished \tANN training loss 0.124150\n",
      ">> Epoch 389 finished \tANN training loss 0.093076\n",
      ">> Epoch 390 finished \tANN training loss 0.080274\n",
      ">> Epoch 391 finished \tANN training loss 0.076183\n",
      ">> Epoch 392 finished \tANN training loss 0.081252\n",
      ">> Epoch 393 finished \tANN training loss 0.085402\n",
      ">> Epoch 394 finished \tANN training loss 0.086846\n",
      ">> Epoch 395 finished \tANN training loss 0.130995\n",
      ">> Epoch 396 finished \tANN training loss 0.106668\n",
      ">> Epoch 397 finished \tANN training loss 0.085157\n",
      ">> Epoch 398 finished \tANN training loss 0.084785\n",
      ">> Epoch 399 finished \tANN training loss 0.085905\n",
      ">> Epoch 400 finished \tANN training loss 0.077871\n",
      ">> Epoch 401 finished \tANN training loss 0.097033\n",
      ">> Epoch 402 finished \tANN training loss 0.083089\n",
      ">> Epoch 403 finished \tANN training loss 0.096832\n",
      ">> Epoch 404 finished \tANN training loss 0.081171\n",
      ">> Epoch 405 finished \tANN training loss 0.073695\n",
      ">> Epoch 406 finished \tANN training loss 0.122957\n",
      ">> Epoch 407 finished \tANN training loss 0.080653\n",
      ">> Epoch 408 finished \tANN training loss 0.079416\n",
      ">> Epoch 409 finished \tANN training loss 0.076941\n",
      ">> Epoch 410 finished \tANN training loss 0.117761\n",
      ">> Epoch 411 finished \tANN training loss 0.078703\n",
      ">> Epoch 412 finished \tANN training loss 0.082182\n",
      ">> Epoch 413 finished \tANN training loss 0.102541\n",
      ">> Epoch 414 finished \tANN training loss 0.083182\n",
      ">> Epoch 415 finished \tANN training loss 0.081131\n",
      ">> Epoch 416 finished \tANN training loss 0.068827\n",
      ">> Epoch 417 finished \tANN training loss 0.066815\n",
      ">> Epoch 418 finished \tANN training loss 0.068472\n",
      ">> Epoch 419 finished \tANN training loss 0.080436\n",
      ">> Epoch 420 finished \tANN training loss 0.075703\n",
      ">> Epoch 421 finished \tANN training loss 0.085696\n",
      ">> Epoch 422 finished \tANN training loss 0.078493\n",
      ">> Epoch 423 finished \tANN training loss 0.077919\n",
      ">> Epoch 424 finished \tANN training loss 0.070744\n",
      ">> Epoch 425 finished \tANN training loss 0.112135\n",
      ">> Epoch 426 finished \tANN training loss 0.072898\n",
      ">> Epoch 427 finished \tANN training loss 0.068046\n",
      ">> Epoch 428 finished \tANN training loss 0.082165\n",
      ">> Epoch 429 finished \tANN training loss 0.069871\n",
      ">> Epoch 430 finished \tANN training loss 0.083156\n",
      ">> Epoch 431 finished \tANN training loss 0.087760\n",
      ">> Epoch 432 finished \tANN training loss 0.074665\n",
      ">> Epoch 433 finished \tANN training loss 0.131759\n",
      ">> Epoch 434 finished \tANN training loss 0.086918\n",
      ">> Epoch 435 finished \tANN training loss 0.082299\n",
      ">> Epoch 436 finished \tANN training loss 0.074534\n",
      ">> Epoch 437 finished \tANN training loss 0.109090\n",
      ">> Epoch 438 finished \tANN training loss 0.076360\n",
      ">> Epoch 439 finished \tANN training loss 0.081104\n",
      ">> Epoch 440 finished \tANN training loss 0.075381\n",
      ">> Epoch 441 finished \tANN training loss 0.146834\n",
      ">> Epoch 442 finished \tANN training loss 0.079101\n",
      ">> Epoch 443 finished \tANN training loss 0.077317\n",
      ">> Epoch 444 finished \tANN training loss 0.085690\n",
      ">> Epoch 445 finished \tANN training loss 0.085288\n",
      ">> Epoch 446 finished \tANN training loss 0.134183\n",
      ">> Epoch 447 finished \tANN training loss 0.078089\n",
      ">> Epoch 448 finished \tANN training loss 0.072899\n",
      ">> Epoch 449 finished \tANN training loss 0.088085\n",
      ">> Epoch 450 finished \tANN training loss 0.109884\n",
      ">> Epoch 451 finished \tANN training loss 0.093598\n",
      ">> Epoch 452 finished \tANN training loss 0.094299\n",
      ">> Epoch 453 finished \tANN training loss 0.081626\n",
      ">> Epoch 454 finished \tANN training loss 0.081405\n",
      ">> Epoch 455 finished \tANN training loss 0.080228\n",
      ">> Epoch 456 finished \tANN training loss 0.077955\n",
      ">> Epoch 457 finished \tANN training loss 0.075395\n",
      ">> Epoch 458 finished \tANN training loss 0.078310\n",
      ">> Epoch 459 finished \tANN training loss 0.079575\n",
      ">> Epoch 460 finished \tANN training loss 0.078103\n",
      ">> Epoch 461 finished \tANN training loss 0.084675\n",
      ">> Epoch 462 finished \tANN training loss 0.137198\n",
      ">> Epoch 463 finished \tANN training loss 0.148482\n",
      ">> Epoch 464 finished \tANN training loss 0.084911\n",
      ">> Epoch 465 finished \tANN training loss 0.093662\n",
      ">> Epoch 466 finished \tANN training loss 0.077625\n",
      ">> Epoch 467 finished \tANN training loss 0.080757\n",
      ">> Epoch 468 finished \tANN training loss 0.071154\n",
      ">> Epoch 469 finished \tANN training loss 0.089176\n",
      ">> Epoch 470 finished \tANN training loss 0.071089\n",
      ">> Epoch 471 finished \tANN training loss 0.084886\n",
      ">> Epoch 472 finished \tANN training loss 0.078955\n",
      ">> Epoch 473 finished \tANN training loss 0.068525\n",
      ">> Epoch 474 finished \tANN training loss 0.086060\n",
      ">> Epoch 475 finished \tANN training loss 0.079423\n",
      ">> Epoch 476 finished \tANN training loss 0.066554\n",
      ">> Epoch 477 finished \tANN training loss 0.082225\n",
      ">> Epoch 478 finished \tANN training loss 0.066625\n",
      ">> Epoch 479 finished \tANN training loss 0.115421\n",
      ">> Epoch 480 finished \tANN training loss 0.064179\n",
      ">> Epoch 481 finished \tANN training loss 0.061979\n",
      ">> Epoch 482 finished \tANN training loss 0.065884\n",
      ">> Epoch 483 finished \tANN training loss 0.081684\n",
      ">> Epoch 484 finished \tANN training loss 0.078183\n",
      ">> Epoch 485 finished \tANN training loss 0.062913\n",
      ">> Epoch 486 finished \tANN training loss 0.077563\n",
      ">> Epoch 487 finished \tANN training loss 0.076531\n",
      ">> Epoch 488 finished \tANN training loss 0.066506\n",
      ">> Epoch 489 finished \tANN training loss 0.070865\n",
      ">> Epoch 490 finished \tANN training loss 0.133054\n",
      ">> Epoch 491 finished \tANN training loss 0.074562\n",
      ">> Epoch 492 finished \tANN training loss 0.075442\n",
      ">> Epoch 493 finished \tANN training loss 0.070934\n",
      ">> Epoch 494 finished \tANN training loss 0.068005\n",
      ">> Epoch 495 finished \tANN training loss 0.094389\n",
      ">> Epoch 496 finished \tANN training loss 0.077611\n",
      ">> Epoch 497 finished \tANN training loss 0.077457\n",
      ">> Epoch 498 finished \tANN training loss 0.075631\n",
      ">> Epoch 499 finished \tANN training loss 0.063970\n",
      ">> Epoch 500 finished \tANN training loss 0.063146\n",
      ">> Epoch 501 finished \tANN training loss 0.061942\n",
      ">> Epoch 502 finished \tANN training loss 0.072647\n",
      ">> Epoch 503 finished \tANN training loss 0.078311\n",
      ">> Epoch 504 finished \tANN training loss 0.068568\n",
      ">> Epoch 505 finished \tANN training loss 0.066411\n",
      ">> Epoch 506 finished \tANN training loss 0.063533\n",
      ">> Epoch 507 finished \tANN training loss 0.063221\n",
      ">> Epoch 508 finished \tANN training loss 0.104159\n",
      ">> Epoch 509 finished \tANN training loss 0.072346\n",
      ">> Epoch 510 finished \tANN training loss 0.060550\n",
      ">> Epoch 511 finished \tANN training loss 0.069732\n",
      ">> Epoch 512 finished \tANN training loss 0.070563\n",
      ">> Epoch 513 finished \tANN training loss 0.065641\n",
      ">> Epoch 514 finished \tANN training loss 0.066509\n",
      ">> Epoch 515 finished \tANN training loss 0.058193\n",
      ">> Epoch 516 finished \tANN training loss 0.063206\n",
      ">> Epoch 517 finished \tANN training loss 0.065795\n",
      ">> Epoch 518 finished \tANN training loss 0.068547\n",
      ">> Epoch 519 finished \tANN training loss 0.064688\n",
      ">> Epoch 520 finished \tANN training loss 0.068950\n",
      ">> Epoch 521 finished \tANN training loss 0.068689\n",
      ">> Epoch 522 finished \tANN training loss 0.103243\n",
      ">> Epoch 523 finished \tANN training loss 0.077122\n",
      ">> Epoch 524 finished \tANN training loss 0.084871\n",
      ">> Epoch 525 finished \tANN training loss 0.081332\n",
      ">> Epoch 526 finished \tANN training loss 0.080164\n",
      ">> Epoch 527 finished \tANN training loss 0.064937\n",
      ">> Epoch 528 finished \tANN training loss 0.071047\n",
      ">> Epoch 529 finished \tANN training loss 0.063156\n",
      ">> Epoch 530 finished \tANN training loss 0.068092\n",
      ">> Epoch 531 finished \tANN training loss 0.068117\n",
      ">> Epoch 532 finished \tANN training loss 0.070965\n",
      ">> Epoch 533 finished \tANN training loss 0.069818\n",
      ">> Epoch 534 finished \tANN training loss 0.065079\n",
      ">> Epoch 535 finished \tANN training loss 0.064521\n",
      ">> Epoch 536 finished \tANN training loss 0.075752\n",
      ">> Epoch 537 finished \tANN training loss 0.071497\n",
      ">> Epoch 538 finished \tANN training loss 0.093511\n",
      ">> Epoch 539 finished \tANN training loss 0.089525\n",
      ">> Epoch 540 finished \tANN training loss 0.075656\n",
      ">> Epoch 541 finished \tANN training loss 0.087769\n",
      ">> Epoch 542 finished \tANN training loss 0.072060\n",
      ">> Epoch 543 finished \tANN training loss 0.064279\n",
      ">> Epoch 544 finished \tANN training loss 0.066833\n",
      ">> Epoch 545 finished \tANN training loss 0.069742\n",
      ">> Epoch 546 finished \tANN training loss 0.060180\n",
      ">> Epoch 547 finished \tANN training loss 0.057122\n",
      ">> Epoch 548 finished \tANN training loss 0.070946\n",
      ">> Epoch 549 finished \tANN training loss 0.066473\n",
      ">> Epoch 550 finished \tANN training loss 0.066681\n",
      ">> Epoch 551 finished \tANN training loss 0.067958\n",
      ">> Epoch 552 finished \tANN training loss 0.075385\n",
      ">> Epoch 553 finished \tANN training loss 0.069148\n",
      ">> Epoch 554 finished \tANN training loss 0.069143\n",
      ">> Epoch 555 finished \tANN training loss 0.064688\n",
      ">> Epoch 556 finished \tANN training loss 0.067384\n",
      ">> Epoch 557 finished \tANN training loss 0.059800\n",
      ">> Epoch 558 finished \tANN training loss 0.060336\n",
      ">> Epoch 559 finished \tANN training loss 0.062568\n",
      ">> Epoch 560 finished \tANN training loss 0.060546\n",
      ">> Epoch 561 finished \tANN training loss 0.055956\n",
      ">> Epoch 562 finished \tANN training loss 0.056691\n",
      ">> Epoch 563 finished \tANN training loss 0.057411\n",
      ">> Epoch 564 finished \tANN training loss 0.057805\n",
      ">> Epoch 565 finished \tANN training loss 0.133069\n",
      ">> Epoch 566 finished \tANN training loss 0.064101\n",
      ">> Epoch 567 finished \tANN training loss 0.064255\n",
      ">> Epoch 568 finished \tANN training loss 0.059413\n",
      ">> Epoch 569 finished \tANN training loss 0.059439\n",
      ">> Epoch 570 finished \tANN training loss 0.056500\n",
      ">> Epoch 571 finished \tANN training loss 0.062830\n",
      ">> Epoch 572 finished \tANN training loss 0.075086\n",
      ">> Epoch 573 finished \tANN training loss 0.061395\n",
      ">> Epoch 574 finished \tANN training loss 0.063030\n",
      ">> Epoch 575 finished \tANN training loss 0.070287\n",
      ">> Epoch 576 finished \tANN training loss 0.065194\n",
      ">> Epoch 577 finished \tANN training loss 0.083432\n",
      ">> Epoch 578 finished \tANN training loss 0.074719\n",
      ">> Epoch 579 finished \tANN training loss 0.118293\n",
      ">> Epoch 580 finished \tANN training loss 0.075803\n",
      ">> Epoch 581 finished \tANN training loss 0.066872\n",
      ">> Epoch 582 finished \tANN training loss 0.214023\n",
      ">> Epoch 583 finished \tANN training loss 0.097908\n",
      ">> Epoch 584 finished \tANN training loss 0.079968\n",
      ">> Epoch 585 finished \tANN training loss 0.067080\n",
      ">> Epoch 586 finished \tANN training loss 0.111571\n",
      ">> Epoch 587 finished \tANN training loss 0.070049\n",
      ">> Epoch 588 finished \tANN training loss 0.076614\n",
      ">> Epoch 589 finished \tANN training loss 0.082598\n",
      ">> Epoch 590 finished \tANN training loss 0.066414\n",
      ">> Epoch 591 finished \tANN training loss 0.062502\n",
      ">> Epoch 592 finished \tANN training loss 0.068855\n",
      ">> Epoch 593 finished \tANN training loss 0.058634\n",
      ">> Epoch 594 finished \tANN training loss 0.106623\n",
      ">> Epoch 595 finished \tANN training loss 0.077417\n",
      ">> Epoch 596 finished \tANN training loss 0.059146\n",
      ">> Epoch 597 finished \tANN training loss 0.063096\n",
      ">> Epoch 598 finished \tANN training loss 0.055484\n",
      ">> Epoch 599 finished \tANN training loss 0.082654\n",
      ">> Epoch 600 finished \tANN training loss 0.063461\n",
      ">> Epoch 601 finished \tANN training loss 0.079145\n",
      ">> Epoch 602 finished \tANN training loss 0.108380\n",
      ">> Epoch 603 finished \tANN training loss 0.072874\n",
      ">> Epoch 604 finished \tANN training loss 0.063409\n",
      ">> Epoch 605 finished \tANN training loss 0.068654\n",
      ">> Epoch 606 finished \tANN training loss 0.109554\n",
      ">> Epoch 607 finished \tANN training loss 0.080369\n",
      ">> Epoch 608 finished \tANN training loss 0.066150\n",
      ">> Epoch 609 finished \tANN training loss 0.064212\n",
      ">> Epoch 610 finished \tANN training loss 0.059237\n",
      ">> Epoch 611 finished \tANN training loss 0.058981\n",
      ">> Epoch 612 finished \tANN training loss 0.094188\n",
      ">> Epoch 613 finished \tANN training loss 0.083462\n",
      ">> Epoch 614 finished \tANN training loss 0.064736\n",
      ">> Epoch 615 finished \tANN training loss 0.060127\n",
      ">> Epoch 616 finished \tANN training loss 0.064692\n",
      ">> Epoch 617 finished \tANN training loss 0.061968\n",
      ">> Epoch 618 finished \tANN training loss 0.083732\n",
      ">> Epoch 619 finished \tANN training loss 0.066194\n",
      ">> Epoch 620 finished \tANN training loss 0.096180\n",
      ">> Epoch 621 finished \tANN training loss 0.071001\n",
      ">> Epoch 622 finished \tANN training loss 0.063207\n",
      ">> Epoch 623 finished \tANN training loss 0.059690\n",
      ">> Epoch 624 finished \tANN training loss 0.069281\n",
      ">> Epoch 625 finished \tANN training loss 0.055703\n",
      ">> Epoch 626 finished \tANN training loss 0.066757\n",
      ">> Epoch 627 finished \tANN training loss 0.068350\n",
      ">> Epoch 628 finished \tANN training loss 0.067417\n",
      ">> Epoch 629 finished \tANN training loss 0.068922\n",
      ">> Epoch 630 finished \tANN training loss 0.071782\n",
      ">> Epoch 631 finished \tANN training loss 0.059706\n",
      ">> Epoch 632 finished \tANN training loss 0.059589\n",
      ">> Epoch 633 finished \tANN training loss 0.072021\n",
      ">> Epoch 634 finished \tANN training loss 0.065052\n",
      ">> Epoch 635 finished \tANN training loss 0.062831\n",
      ">> Epoch 636 finished \tANN training loss 0.064953\n",
      ">> Epoch 637 finished \tANN training loss 0.065974\n",
      ">> Epoch 638 finished \tANN training loss 0.060820\n",
      ">> Epoch 639 finished \tANN training loss 0.066121\n",
      ">> Epoch 640 finished \tANN training loss 0.053872\n",
      ">> Epoch 641 finished \tANN training loss 0.048802\n",
      ">> Epoch 642 finished \tANN training loss 0.063189\n",
      ">> Epoch 643 finished \tANN training loss 0.062029\n",
      ">> Epoch 644 finished \tANN training loss 0.068285\n",
      ">> Epoch 645 finished \tANN training loss 0.066551\n",
      ">> Epoch 646 finished \tANN training loss 0.058147\n",
      ">> Epoch 647 finished \tANN training loss 0.088832\n",
      ">> Epoch 648 finished \tANN training loss 0.068463\n",
      ">> Epoch 649 finished \tANN training loss 0.071017\n",
      ">> Epoch 650 finished \tANN training loss 0.074761\n",
      ">> Epoch 651 finished \tANN training loss 0.063667\n",
      ">> Epoch 652 finished \tANN training loss 0.064918\n",
      ">> Epoch 653 finished \tANN training loss 0.057836\n",
      ">> Epoch 654 finished \tANN training loss 0.066315\n",
      ">> Epoch 655 finished \tANN training loss 0.066134\n",
      ">> Epoch 656 finished \tANN training loss 0.066026\n",
      ">> Epoch 657 finished \tANN training loss 0.063948\n",
      ">> Epoch 658 finished \tANN training loss 0.059000\n",
      ">> Epoch 659 finished \tANN training loss 0.075635\n",
      ">> Epoch 660 finished \tANN training loss 0.081344\n",
      ">> Epoch 661 finished \tANN training loss 0.072136\n",
      ">> Epoch 662 finished \tANN training loss 0.064244\n",
      ">> Epoch 663 finished \tANN training loss 0.066169\n",
      ">> Epoch 664 finished \tANN training loss 0.076473\n",
      ">> Epoch 665 finished \tANN training loss 0.067230\n",
      ">> Epoch 666 finished \tANN training loss 0.075188\n",
      ">> Epoch 667 finished \tANN training loss 0.085272\n",
      ">> Epoch 668 finished \tANN training loss 0.063403\n",
      ">> Epoch 669 finished \tANN training loss 0.067684\n",
      ">> Epoch 670 finished \tANN training loss 0.067627\n",
      ">> Epoch 671 finished \tANN training loss 0.055038\n",
      ">> Epoch 672 finished \tANN training loss 0.065948\n",
      ">> Epoch 673 finished \tANN training loss 0.085878\n",
      ">> Epoch 674 finished \tANN training loss 0.069401\n",
      ">> Epoch 675 finished \tANN training loss 0.072769\n",
      ">> Epoch 676 finished \tANN training loss 0.065891\n",
      ">> Epoch 677 finished \tANN training loss 0.057709\n",
      ">> Epoch 678 finished \tANN training loss 0.064824\n",
      ">> Epoch 679 finished \tANN training loss 0.067527\n",
      ">> Epoch 680 finished \tANN training loss 0.081818\n",
      ">> Epoch 681 finished \tANN training loss 0.103834\n",
      ">> Epoch 682 finished \tANN training loss 0.076204\n",
      ">> Epoch 683 finished \tANN training loss 0.067611\n",
      ">> Epoch 684 finished \tANN training loss 0.071236\n",
      ">> Epoch 685 finished \tANN training loss 0.084517\n",
      ">> Epoch 686 finished \tANN training loss 0.070668\n",
      ">> Epoch 687 finished \tANN training loss 0.055067\n",
      ">> Epoch 688 finished \tANN training loss 0.061607\n",
      ">> Epoch 689 finished \tANN training loss 0.063135\n",
      ">> Epoch 690 finished \tANN training loss 0.062718\n",
      ">> Epoch 691 finished \tANN training loss 0.081646\n",
      ">> Epoch 692 finished \tANN training loss 0.115357\n",
      ">> Epoch 693 finished \tANN training loss 0.063957\n",
      ">> Epoch 694 finished \tANN training loss 0.061498\n",
      ">> Epoch 695 finished \tANN training loss 0.058968\n",
      ">> Epoch 696 finished \tANN training loss 0.062074\n",
      ">> Epoch 697 finished \tANN training loss 0.062216\n",
      ">> Epoch 698 finished \tANN training loss 0.079939\n",
      ">> Epoch 699 finished \tANN training loss 0.053708\n",
      ">> Epoch 700 finished \tANN training loss 0.055289\n",
      ">> Epoch 701 finished \tANN training loss 0.064012\n",
      ">> Epoch 702 finished \tANN training loss 0.059823\n",
      ">> Epoch 703 finished \tANN training loss 0.057964\n",
      ">> Epoch 704 finished \tANN training loss 0.054740\n",
      ">> Epoch 705 finished \tANN training loss 0.056884\n",
      ">> Epoch 706 finished \tANN training loss 0.059008\n",
      ">> Epoch 707 finished \tANN training loss 0.057077\n",
      ">> Epoch 708 finished \tANN training loss 0.049061\n",
      ">> Epoch 709 finished \tANN training loss 0.048024\n",
      ">> Epoch 710 finished \tANN training loss 0.055922\n",
      ">> Epoch 711 finished \tANN training loss 0.053850\n",
      ">> Epoch 712 finished \tANN training loss 0.052135\n",
      ">> Epoch 713 finished \tANN training loss 0.068165\n",
      ">> Epoch 714 finished \tANN training loss 0.057035\n",
      ">> Epoch 715 finished \tANN training loss 0.082885\n",
      ">> Epoch 716 finished \tANN training loss 0.056700\n",
      ">> Epoch 717 finished \tANN training loss 0.086749\n",
      ">> Epoch 718 finished \tANN training loss 0.064011\n",
      ">> Epoch 719 finished \tANN training loss 0.058477\n",
      ">> Epoch 720 finished \tANN training loss 0.067482\n",
      ">> Epoch 721 finished \tANN training loss 0.059275\n",
      ">> Epoch 722 finished \tANN training loss 0.069599\n",
      ">> Epoch 723 finished \tANN training loss 0.059806\n",
      ">> Epoch 724 finished \tANN training loss 0.070539\n",
      ">> Epoch 725 finished \tANN training loss 0.053209\n",
      ">> Epoch 726 finished \tANN training loss 0.056240\n",
      ">> Epoch 727 finished \tANN training loss 0.090778\n",
      ">> Epoch 728 finished \tANN training loss 0.070882\n",
      ">> Epoch 729 finished \tANN training loss 0.109058\n",
      ">> Epoch 730 finished \tANN training loss 0.064182\n",
      ">> Epoch 731 finished \tANN training loss 0.067445\n",
      ">> Epoch 732 finished \tANN training loss 0.058120\n",
      ">> Epoch 733 finished \tANN training loss 0.060839\n",
      ">> Epoch 734 finished \tANN training loss 0.056852\n",
      ">> Epoch 735 finished \tANN training loss 0.061532\n",
      ">> Epoch 736 finished \tANN training loss 0.065771\n",
      ">> Epoch 737 finished \tANN training loss 0.063671\n",
      ">> Epoch 738 finished \tANN training loss 0.056037\n",
      ">> Epoch 739 finished \tANN training loss 0.057189\n",
      ">> Epoch 740 finished \tANN training loss 0.058906\n",
      ">> Epoch 741 finished \tANN training loss 0.092748\n",
      ">> Epoch 742 finished \tANN training loss 0.060265\n",
      ">> Epoch 743 finished \tANN training loss 0.057358\n",
      ">> Epoch 744 finished \tANN training loss 0.059921\n",
      ">> Epoch 745 finished \tANN training loss 0.058418\n",
      ">> Epoch 746 finished \tANN training loss 0.063382\n",
      ">> Epoch 747 finished \tANN training loss 0.062295\n",
      ">> Epoch 748 finished \tANN training loss 0.057616\n",
      ">> Epoch 749 finished \tANN training loss 0.066752\n",
      ">> Epoch 750 finished \tANN training loss 0.059239\n",
      ">> Epoch 751 finished \tANN training loss 0.063002\n",
      ">> Epoch 752 finished \tANN training loss 0.053479\n",
      ">> Epoch 753 finished \tANN training loss 0.053400\n",
      ">> Epoch 754 finished \tANN training loss 0.063637\n",
      ">> Epoch 755 finished \tANN training loss 0.068212\n",
      ">> Epoch 756 finished \tANN training loss 0.072779\n",
      ">> Epoch 757 finished \tANN training loss 0.068141\n",
      ">> Epoch 758 finished \tANN training loss 0.073231\n",
      ">> Epoch 759 finished \tANN training loss 0.062985\n",
      ">> Epoch 760 finished \tANN training loss 0.064186\n",
      ">> Epoch 761 finished \tANN training loss 0.055043\n",
      ">> Epoch 762 finished \tANN training loss 0.071581\n",
      ">> Epoch 763 finished \tANN training loss 0.112541\n",
      ">> Epoch 764 finished \tANN training loss 0.063645\n",
      ">> Epoch 765 finished \tANN training loss 0.052756\n",
      ">> Epoch 766 finished \tANN training loss 0.061215\n",
      ">> Epoch 767 finished \tANN training loss 0.054879\n",
      ">> Epoch 768 finished \tANN training loss 0.054858\n",
      ">> Epoch 769 finished \tANN training loss 0.047018\n",
      ">> Epoch 770 finished \tANN training loss 0.070214\n",
      ">> Epoch 771 finished \tANN training loss 0.080028\n",
      ">> Epoch 772 finished \tANN training loss 0.058139\n",
      ">> Epoch 773 finished \tANN training loss 0.057106\n",
      ">> Epoch 774 finished \tANN training loss 0.094005\n",
      ">> Epoch 775 finished \tANN training loss 0.079162\n",
      ">> Epoch 776 finished \tANN training loss 0.070890\n",
      ">> Epoch 777 finished \tANN training loss 0.072132\n",
      ">> Epoch 778 finished \tANN training loss 0.112477\n",
      ">> Epoch 779 finished \tANN training loss 0.081663\n",
      ">> Epoch 780 finished \tANN training loss 0.078798\n",
      ">> Epoch 781 finished \tANN training loss 0.068937\n",
      ">> Epoch 782 finished \tANN training loss 0.066492\n",
      ">> Epoch 783 finished \tANN training loss 0.058104\n",
      ">> Epoch 784 finished \tANN training loss 0.054634\n",
      ">> Epoch 785 finished \tANN training loss 0.053730\n",
      ">> Epoch 786 finished \tANN training loss 0.053114\n",
      ">> Epoch 787 finished \tANN training loss 0.066352\n",
      ">> Epoch 788 finished \tANN training loss 0.062411\n",
      ">> Epoch 789 finished \tANN training loss 0.064427\n",
      ">> Epoch 790 finished \tANN training loss 0.070736\n",
      ">> Epoch 791 finished \tANN training loss 0.064565\n",
      ">> Epoch 792 finished \tANN training loss 0.060080\n",
      ">> Epoch 793 finished \tANN training loss 0.052239\n",
      ">> Epoch 794 finished \tANN training loss 0.062333\n",
      ">> Epoch 795 finished \tANN training loss 0.056824\n",
      ">> Epoch 796 finished \tANN training loss 0.063653\n",
      ">> Epoch 797 finished \tANN training loss 0.054120\n",
      ">> Epoch 798 finished \tANN training loss 0.051244\n",
      ">> Epoch 799 finished \tANN training loss 0.050941\n",
      ">> Epoch 800 finished \tANN training loss 0.062693\n",
      ">> Epoch 801 finished \tANN training loss 0.067734\n",
      ">> Epoch 802 finished \tANN training loss 0.063076\n",
      ">> Epoch 803 finished \tANN training loss 0.058142\n",
      ">> Epoch 804 finished \tANN training loss 0.061612\n",
      ">> Epoch 805 finished \tANN training loss 0.063998\n",
      ">> Epoch 806 finished \tANN training loss 0.056347\n",
      ">> Epoch 807 finished \tANN training loss 0.075550\n",
      ">> Epoch 808 finished \tANN training loss 0.069744\n",
      ">> Epoch 809 finished \tANN training loss 0.063994\n",
      ">> Epoch 810 finished \tANN training loss 0.061462\n",
      ">> Epoch 811 finished \tANN training loss 0.051844\n",
      ">> Epoch 812 finished \tANN training loss 0.050266\n",
      ">> Epoch 813 finished \tANN training loss 0.058027\n",
      ">> Epoch 814 finished \tANN training loss 0.045662\n",
      ">> Epoch 815 finished \tANN training loss 0.052707\n",
      ">> Epoch 816 finished \tANN training loss 0.054599\n",
      ">> Epoch 817 finished \tANN training loss 0.052567\n",
      ">> Epoch 818 finished \tANN training loss 0.057121\n",
      ">> Epoch 819 finished \tANN training loss 0.050154\n",
      ">> Epoch 820 finished \tANN training loss 0.060900\n",
      ">> Epoch 821 finished \tANN training loss 0.067257\n",
      ">> Epoch 822 finished \tANN training loss 0.071534\n",
      ">> Epoch 823 finished \tANN training loss 0.058687\n",
      ">> Epoch 824 finished \tANN training loss 0.059970\n",
      ">> Epoch 825 finished \tANN training loss 0.081930\n",
      ">> Epoch 826 finished \tANN training loss 0.052382\n",
      ">> Epoch 827 finished \tANN training loss 0.048390\n",
      ">> Epoch 828 finished \tANN training loss 0.059094\n",
      ">> Epoch 829 finished \tANN training loss 0.049225\n",
      ">> Epoch 830 finished \tANN training loss 0.055987\n",
      ">> Epoch 831 finished \tANN training loss 0.064437\n",
      ">> Epoch 832 finished \tANN training loss 0.062962\n",
      ">> Epoch 833 finished \tANN training loss 0.055360\n",
      ">> Epoch 834 finished \tANN training loss 0.058155\n",
      ">> Epoch 835 finished \tANN training loss 0.048321\n",
      ">> Epoch 836 finished \tANN training loss 0.053602\n",
      ">> Epoch 837 finished \tANN training loss 0.058127\n",
      ">> Epoch 838 finished \tANN training loss 0.053519\n",
      ">> Epoch 839 finished \tANN training loss 0.045968\n",
      ">> Epoch 840 finished \tANN training loss 0.043175\n",
      ">> Epoch 841 finished \tANN training loss 0.047962\n",
      ">> Epoch 842 finished \tANN training loss 0.044496\n",
      ">> Epoch 843 finished \tANN training loss 0.048741\n",
      ">> Epoch 844 finished \tANN training loss 0.053630\n",
      ">> Epoch 845 finished \tANN training loss 0.060832\n",
      ">> Epoch 846 finished \tANN training loss 0.060310\n",
      ">> Epoch 847 finished \tANN training loss 0.062422\n",
      ">> Epoch 848 finished \tANN training loss 0.087353\n",
      ">> Epoch 849 finished \tANN training loss 0.076769\n",
      ">> Epoch 850 finished \tANN training loss 0.070841\n",
      ">> Epoch 851 finished \tANN training loss 0.063808\n",
      ">> Epoch 852 finished \tANN training loss 0.058594\n",
      ">> Epoch 853 finished \tANN training loss 0.057248\n",
      ">> Epoch 854 finished \tANN training loss 0.054537\n",
      ">> Epoch 855 finished \tANN training loss 0.057886\n",
      ">> Epoch 856 finished \tANN training loss 0.101849\n",
      ">> Epoch 857 finished \tANN training loss 0.060327\n",
      ">> Epoch 858 finished \tANN training loss 0.059254\n",
      ">> Epoch 859 finished \tANN training loss 0.092004\n",
      ">> Epoch 860 finished \tANN training loss 0.049887\n",
      ">> Epoch 861 finished \tANN training loss 0.057704\n",
      ">> Epoch 862 finished \tANN training loss 0.054042\n",
      ">> Epoch 863 finished \tANN training loss 0.051941\n",
      ">> Epoch 864 finished \tANN training loss 0.049863\n",
      ">> Epoch 865 finished \tANN training loss 0.085846\n",
      ">> Epoch 866 finished \tANN training loss 0.062337\n",
      ">> Epoch 867 finished \tANN training loss 0.056109\n",
      ">> Epoch 868 finished \tANN training loss 0.054317\n",
      ">> Epoch 869 finished \tANN training loss 0.053822\n",
      ">> Epoch 870 finished \tANN training loss 0.078651\n",
      ">> Epoch 871 finished \tANN training loss 0.055115\n",
      ">> Epoch 872 finished \tANN training loss 0.055189\n",
      ">> Epoch 873 finished \tANN training loss 0.055700\n",
      ">> Epoch 874 finished \tANN training loss 0.099327\n",
      ">> Epoch 875 finished \tANN training loss 0.078871\n",
      ">> Epoch 876 finished \tANN training loss 0.061846\n",
      ">> Epoch 877 finished \tANN training loss 0.111264\n",
      ">> Epoch 878 finished \tANN training loss 0.053982\n",
      ">> Epoch 879 finished \tANN training loss 0.053440\n",
      ">> Epoch 880 finished \tANN training loss 0.052662\n",
      ">> Epoch 881 finished \tANN training loss 0.058390\n",
      ">> Epoch 882 finished \tANN training loss 0.053575\n",
      ">> Epoch 883 finished \tANN training loss 0.051821\n",
      ">> Epoch 884 finished \tANN training loss 0.053187\n",
      ">> Epoch 885 finished \tANN training loss 0.056690\n",
      ">> Epoch 886 finished \tANN training loss 0.043208\n",
      ">> Epoch 887 finished \tANN training loss 0.044316\n",
      ">> Epoch 888 finished \tANN training loss 0.056363\n",
      ">> Epoch 889 finished \tANN training loss 0.048961\n",
      ">> Epoch 890 finished \tANN training loss 0.051201\n",
      ">> Epoch 891 finished \tANN training loss 0.079118\n",
      ">> Epoch 892 finished \tANN training loss 0.060932\n",
      ">> Epoch 893 finished \tANN training loss 0.056155\n",
      ">> Epoch 894 finished \tANN training loss 0.051492\n",
      ">> Epoch 895 finished \tANN training loss 0.054383\n",
      ">> Epoch 896 finished \tANN training loss 0.047292\n",
      ">> Epoch 897 finished \tANN training loss 0.062926\n",
      ">> Epoch 898 finished \tANN training loss 0.053144\n",
      ">> Epoch 899 finished \tANN training loss 0.058179\n",
      ">> Epoch 900 finished \tANN training loss 0.054909\n",
      ">> Epoch 901 finished \tANN training loss 0.062836\n",
      ">> Epoch 902 finished \tANN training loss 0.057493\n",
      ">> Epoch 903 finished \tANN training loss 0.053339\n",
      ">> Epoch 904 finished \tANN training loss 0.052401\n",
      ">> Epoch 905 finished \tANN training loss 0.053218\n",
      ">> Epoch 906 finished \tANN training loss 0.046018\n",
      ">> Epoch 907 finished \tANN training loss 0.049566\n",
      ">> Epoch 908 finished \tANN training loss 0.070780\n",
      ">> Epoch 909 finished \tANN training loss 0.054812\n",
      ">> Epoch 910 finished \tANN training loss 0.048043\n",
      ">> Epoch 911 finished \tANN training loss 0.049842\n",
      ">> Epoch 912 finished \tANN training loss 0.072657\n",
      ">> Epoch 913 finished \tANN training loss 0.060130\n",
      ">> Epoch 914 finished \tANN training loss 0.049701\n",
      ">> Epoch 915 finished \tANN training loss 0.071836\n",
      ">> Epoch 916 finished \tANN training loss 0.053912\n",
      ">> Epoch 917 finished \tANN training loss 0.059575\n",
      ">> Epoch 918 finished \tANN training loss 0.050740\n",
      ">> Epoch 919 finished \tANN training loss 0.120382\n",
      ">> Epoch 920 finished \tANN training loss 0.067442\n",
      ">> Epoch 921 finished \tANN training loss 0.088114\n",
      ">> Epoch 922 finished \tANN training loss 0.070113\n",
      ">> Epoch 923 finished \tANN training loss 0.058773\n",
      ">> Epoch 924 finished \tANN training loss 0.066798\n",
      ">> Epoch 925 finished \tANN training loss 0.053303\n",
      ">> Epoch 926 finished \tANN training loss 0.052589\n",
      ">> Epoch 927 finished \tANN training loss 0.065070\n",
      ">> Epoch 928 finished \tANN training loss 0.063316\n",
      ">> Epoch 929 finished \tANN training loss 0.096875\n",
      ">> Epoch 930 finished \tANN training loss 0.058534\n",
      ">> Epoch 931 finished \tANN training loss 0.054278\n",
      ">> Epoch 932 finished \tANN training loss 0.052954\n",
      ">> Epoch 933 finished \tANN training loss 0.052223\n",
      ">> Epoch 934 finished \tANN training loss 0.053198\n",
      ">> Epoch 935 finished \tANN training loss 0.051561\n",
      ">> Epoch 936 finished \tANN training loss 0.054500\n",
      ">> Epoch 937 finished \tANN training loss 0.050861\n",
      ">> Epoch 938 finished \tANN training loss 0.055750\n",
      ">> Epoch 939 finished \tANN training loss 0.053095\n",
      ">> Epoch 940 finished \tANN training loss 0.051080\n",
      ">> Epoch 941 finished \tANN training loss 0.049611\n",
      ">> Epoch 942 finished \tANN training loss 0.060671\n",
      ">> Epoch 943 finished \tANN training loss 0.062252\n",
      ">> Epoch 944 finished \tANN training loss 0.068657\n",
      ">> Epoch 945 finished \tANN training loss 0.073510\n",
      ">> Epoch 946 finished \tANN training loss 0.061539\n",
      ">> Epoch 947 finished \tANN training loss 0.048597\n",
      ">> Epoch 948 finished \tANN training loss 0.050173\n",
      ">> Epoch 949 finished \tANN training loss 0.063580\n",
      ">> Epoch 950 finished \tANN training loss 0.049521\n",
      ">> Epoch 951 finished \tANN training loss 0.055849\n",
      ">> Epoch 952 finished \tANN training loss 0.050028\n",
      ">> Epoch 953 finished \tANN training loss 0.055543\n",
      ">> Epoch 954 finished \tANN training loss 0.057236\n",
      ">> Epoch 955 finished \tANN training loss 0.058160\n",
      ">> Epoch 956 finished \tANN training loss 0.060523\n",
      ">> Epoch 957 finished \tANN training loss 0.064047\n",
      ">> Epoch 958 finished \tANN training loss 0.062575\n",
      ">> Epoch 959 finished \tANN training loss 0.087829\n",
      ">> Epoch 960 finished \tANN training loss 0.077986\n",
      ">> Epoch 961 finished \tANN training loss 0.062560\n",
      ">> Epoch 962 finished \tANN training loss 0.058506\n",
      ">> Epoch 963 finished \tANN training loss 0.058741\n",
      ">> Epoch 964 finished \tANN training loss 0.055478\n",
      ">> Epoch 965 finished \tANN training loss 0.060842\n",
      ">> Epoch 966 finished \tANN training loss 0.059253\n",
      ">> Epoch 967 finished \tANN training loss 0.056684\n",
      ">> Epoch 968 finished \tANN training loss 0.052753\n",
      ">> Epoch 969 finished \tANN training loss 0.056516\n",
      ">> Epoch 970 finished \tANN training loss 0.053703\n",
      ">> Epoch 971 finished \tANN training loss 0.047152\n",
      ">> Epoch 972 finished \tANN training loss 0.049038\n",
      ">> Epoch 973 finished \tANN training loss 0.047934\n",
      ">> Epoch 974 finished \tANN training loss 0.047876\n",
      ">> Epoch 975 finished \tANN training loss 0.053351\n",
      ">> Epoch 976 finished \tANN training loss 0.043658\n",
      ">> Epoch 977 finished \tANN training loss 0.048865\n",
      ">> Epoch 978 finished \tANN training loss 0.049152\n",
      ">> Epoch 979 finished \tANN training loss 0.048079\n",
      ">> Epoch 980 finished \tANN training loss 0.042142\n",
      ">> Epoch 981 finished \tANN training loss 0.048782\n",
      ">> Epoch 982 finished \tANN training loss 0.044213\n",
      ">> Epoch 983 finished \tANN training loss 0.047416\n",
      ">> Epoch 984 finished \tANN training loss 0.058117\n",
      ">> Epoch 985 finished \tANN training loss 0.050571\n",
      ">> Epoch 986 finished \tANN training loss 0.056765\n",
      ">> Epoch 987 finished \tANN training loss 0.054925\n",
      ">> Epoch 988 finished \tANN training loss 0.055945\n",
      ">> Epoch 989 finished \tANN training loss 0.052993\n",
      ">> Epoch 990 finished \tANN training loss 0.058824\n",
      ">> Epoch 991 finished \tANN training loss 0.055326\n",
      ">> Epoch 992 finished \tANN training loss 0.051865\n",
      ">> Epoch 993 finished \tANN training loss 0.058978\n",
      ">> Epoch 994 finished \tANN training loss 0.049039\n",
      ">> Epoch 995 finished \tANN training loss 0.059518\n",
      ">> Epoch 996 finished \tANN training loss 0.051823\n",
      ">> Epoch 997 finished \tANN training loss 0.063617\n",
      ">> Epoch 998 finished \tANN training loss 0.051222\n",
      ">> Epoch 999 finished \tANN training loss 0.054008\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 8.091782\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 10.885195\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 10.966335\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 10.181576\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 9.789098\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 9.139126\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 8.940580\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 8.225638\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 7.783572\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 7.234603\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 7.052249\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 7.109640\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 6.977748\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 7.222653\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 7.049780\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 6.871920\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 7.073777\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 7.028566\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 7.245629\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 7.727318\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.418355\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 9.114535\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 8.202976\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 6.912064\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.948762\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5.566360\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5.653837\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 6.220471\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 6.667620\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 6.051269\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 5.710641\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 5.327462\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 5.099159\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 5.021621\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 5.030920\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 5.213264\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 5.801877\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 5.638838\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 6.082617\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 6.142948\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.946396\n",
      ">> Epoch 1 finished \tANN training loss 0.907009\n",
      ">> Epoch 2 finished \tANN training loss 0.905256\n",
      ">> Epoch 3 finished \tANN training loss 0.869959\n",
      ">> Epoch 4 finished \tANN training loss 0.865192\n",
      ">> Epoch 5 finished \tANN training loss 0.851066\n",
      ">> Epoch 6 finished \tANN training loss 0.809206\n",
      ">> Epoch 7 finished \tANN training loss 0.771988\n",
      ">> Epoch 8 finished \tANN training loss 0.717324\n",
      ">> Epoch 9 finished \tANN training loss 0.704774\n",
      ">> Epoch 10 finished \tANN training loss 0.687234\n",
      ">> Epoch 11 finished \tANN training loss 0.641223\n",
      ">> Epoch 12 finished \tANN training loss 0.613373\n",
      ">> Epoch 13 finished \tANN training loss 0.619672\n",
      ">> Epoch 14 finished \tANN training loss 0.566907\n",
      ">> Epoch 15 finished \tANN training loss 0.540519\n",
      ">> Epoch 16 finished \tANN training loss 0.516378\n",
      ">> Epoch 17 finished \tANN training loss 0.521101\n",
      ">> Epoch 18 finished \tANN training loss 0.473249\n",
      ">> Epoch 19 finished \tANN training loss 0.469112\n",
      ">> Epoch 20 finished \tANN training loss 0.494858\n",
      ">> Epoch 21 finished \tANN training loss 0.442931\n",
      ">> Epoch 22 finished \tANN training loss 0.425279\n",
      ">> Epoch 23 finished \tANN training loss 0.459196\n",
      ">> Epoch 24 finished \tANN training loss 0.432571\n",
      ">> Epoch 25 finished \tANN training loss 0.395285\n",
      ">> Epoch 26 finished \tANN training loss 0.372849\n",
      ">> Epoch 27 finished \tANN training loss 0.379275\n",
      ">> Epoch 28 finished \tANN training loss 0.386934\n",
      ">> Epoch 29 finished \tANN training loss 0.346885\n",
      ">> Epoch 30 finished \tANN training loss 0.429279\n",
      ">> Epoch 31 finished \tANN training loss 0.340090\n",
      ">> Epoch 32 finished \tANN training loss 0.335017\n",
      ">> Epoch 33 finished \tANN training loss 0.345066\n",
      ">> Epoch 34 finished \tANN training loss 0.535975\n",
      ">> Epoch 35 finished \tANN training loss 0.388143\n",
      ">> Epoch 36 finished \tANN training loss 0.297269\n",
      ">> Epoch 37 finished \tANN training loss 0.363014\n",
      ">> Epoch 38 finished \tANN training loss 0.326266\n",
      ">> Epoch 39 finished \tANN training loss 0.294232\n",
      ">> Epoch 40 finished \tANN training loss 0.319424\n",
      ">> Epoch 41 finished \tANN training loss 0.298013\n",
      ">> Epoch 42 finished \tANN training loss 0.260805\n",
      ">> Epoch 43 finished \tANN training loss 0.236902\n",
      ">> Epoch 44 finished \tANN training loss 0.233660\n",
      ">> Epoch 45 finished \tANN training loss 0.268738\n",
      ">> Epoch 46 finished \tANN training loss 0.259008\n",
      ">> Epoch 47 finished \tANN training loss 0.238022\n",
      ">> Epoch 48 finished \tANN training loss 0.242056\n",
      ">> Epoch 49 finished \tANN training loss 0.331447\n",
      ">> Epoch 50 finished \tANN training loss 0.227547\n",
      ">> Epoch 51 finished \tANN training loss 0.238769\n",
      ">> Epoch 52 finished \tANN training loss 0.217694\n",
      ">> Epoch 53 finished \tANN training loss 0.284912\n",
      ">> Epoch 54 finished \tANN training loss 0.222273\n",
      ">> Epoch 55 finished \tANN training loss 0.220275\n",
      ">> Epoch 56 finished \tANN training loss 0.213961\n",
      ">> Epoch 57 finished \tANN training loss 0.216814\n",
      ">> Epoch 58 finished \tANN training loss 0.233636\n",
      ">> Epoch 59 finished \tANN training loss 0.216451\n",
      ">> Epoch 60 finished \tANN training loss 0.205797\n",
      ">> Epoch 61 finished \tANN training loss 0.208452\n",
      ">> Epoch 62 finished \tANN training loss 0.264332\n",
      ">> Epoch 63 finished \tANN training loss 0.248369\n",
      ">> Epoch 64 finished \tANN training loss 0.192850\n",
      ">> Epoch 65 finished \tANN training loss 0.191713\n",
      ">> Epoch 66 finished \tANN training loss 0.185662\n",
      ">> Epoch 67 finished \tANN training loss 0.289630\n",
      ">> Epoch 68 finished \tANN training loss 0.201689\n",
      ">> Epoch 69 finished \tANN training loss 0.207888\n",
      ">> Epoch 70 finished \tANN training loss 0.185309\n",
      ">> Epoch 71 finished \tANN training loss 0.201579\n",
      ">> Epoch 72 finished \tANN training loss 0.210704\n",
      ">> Epoch 73 finished \tANN training loss 0.175283\n",
      ">> Epoch 74 finished \tANN training loss 0.208375\n",
      ">> Epoch 75 finished \tANN training loss 0.182845\n",
      ">> Epoch 76 finished \tANN training loss 0.173932\n",
      ">> Epoch 77 finished \tANN training loss 0.203442\n",
      ">> Epoch 78 finished \tANN training loss 0.182358\n",
      ">> Epoch 79 finished \tANN training loss 0.194762\n",
      ">> Epoch 80 finished \tANN training loss 0.188670\n",
      ">> Epoch 81 finished \tANN training loss 0.231810\n",
      ">> Epoch 82 finished \tANN training loss 0.192041\n",
      ">> Epoch 83 finished \tANN training loss 0.168911\n",
      ">> Epoch 84 finished \tANN training loss 0.186020\n",
      ">> Epoch 85 finished \tANN training loss 0.193754\n",
      ">> Epoch 86 finished \tANN training loss 0.218068\n",
      ">> Epoch 87 finished \tANN training loss 0.179369\n",
      ">> Epoch 88 finished \tANN training loss 0.177916\n",
      ">> Epoch 89 finished \tANN training loss 0.175662\n",
      ">> Epoch 90 finished \tANN training loss 0.192803\n",
      ">> Epoch 91 finished \tANN training loss 0.173192\n",
      ">> Epoch 92 finished \tANN training loss 0.172307\n",
      ">> Epoch 93 finished \tANN training loss 0.164949\n",
      ">> Epoch 94 finished \tANN training loss 0.178483\n",
      ">> Epoch 95 finished \tANN training loss 0.159457\n",
      ">> Epoch 96 finished \tANN training loss 0.150070\n",
      ">> Epoch 97 finished \tANN training loss 0.155993\n",
      ">> Epoch 98 finished \tANN training loss 0.139824\n",
      ">> Epoch 99 finished \tANN training loss 0.150304\n",
      ">> Epoch 100 finished \tANN training loss 0.154305\n",
      ">> Epoch 101 finished \tANN training loss 0.164807\n",
      ">> Epoch 102 finished \tANN training loss 0.152406\n",
      ">> Epoch 103 finished \tANN training loss 0.150379\n",
      ">> Epoch 104 finished \tANN training loss 0.152443\n",
      ">> Epoch 105 finished \tANN training loss 0.165315\n",
      ">> Epoch 106 finished \tANN training loss 0.169564\n",
      ">> Epoch 107 finished \tANN training loss 0.137633\n",
      ">> Epoch 108 finished \tANN training loss 0.137355\n",
      ">> Epoch 109 finished \tANN training loss 0.167047\n",
      ">> Epoch 110 finished \tANN training loss 0.418761\n",
      ">> Epoch 111 finished \tANN training loss 0.176965\n",
      ">> Epoch 112 finished \tANN training loss 0.149878\n",
      ">> Epoch 113 finished \tANN training loss 0.210761\n",
      ">> Epoch 114 finished \tANN training loss 0.240586\n",
      ">> Epoch 115 finished \tANN training loss 0.295053\n",
      ">> Epoch 116 finished \tANN training loss 0.176894\n",
      ">> Epoch 117 finished \tANN training loss 0.193012\n",
      ">> Epoch 118 finished \tANN training loss 0.246070\n",
      ">> Epoch 119 finished \tANN training loss 0.192411\n",
      ">> Epoch 120 finished \tANN training loss 0.172667\n",
      ">> Epoch 121 finished \tANN training loss 0.222464\n",
      ">> Epoch 122 finished \tANN training loss 0.141828\n",
      ">> Epoch 123 finished \tANN training loss 0.146349\n",
      ">> Epoch 124 finished \tANN training loss 0.131521\n",
      ">> Epoch 125 finished \tANN training loss 0.145255\n",
      ">> Epoch 126 finished \tANN training loss 0.139707\n",
      ">> Epoch 127 finished \tANN training loss 0.302961\n",
      ">> Epoch 128 finished \tANN training loss 0.137286\n",
      ">> Epoch 129 finished \tANN training loss 0.145559\n",
      ">> Epoch 130 finished \tANN training loss 0.143240\n",
      ">> Epoch 131 finished \tANN training loss 0.127708\n",
      ">> Epoch 132 finished \tANN training loss 0.134144\n",
      ">> Epoch 133 finished \tANN training loss 0.115215\n",
      ">> Epoch 134 finished \tANN training loss 0.147270\n",
      ">> Epoch 135 finished \tANN training loss 0.135616\n",
      ">> Epoch 136 finished \tANN training loss 0.140027\n",
      ">> Epoch 137 finished \tANN training loss 0.112624\n",
      ">> Epoch 138 finished \tANN training loss 0.196340\n",
      ">> Epoch 139 finished \tANN training loss 0.123097\n",
      ">> Epoch 140 finished \tANN training loss 0.128979\n",
      ">> Epoch 141 finished \tANN training loss 0.128364\n",
      ">> Epoch 142 finished \tANN training loss 0.127067\n",
      ">> Epoch 143 finished \tANN training loss 0.119557\n",
      ">> Epoch 144 finished \tANN training loss 0.141047\n",
      ">> Epoch 145 finished \tANN training loss 0.147536\n",
      ">> Epoch 146 finished \tANN training loss 0.116004\n",
      ">> Epoch 147 finished \tANN training loss 0.129197\n",
      ">> Epoch 148 finished \tANN training loss 0.104104\n",
      ">> Epoch 149 finished \tANN training loss 0.105191\n",
      ">> Epoch 150 finished \tANN training loss 0.107815\n",
      ">> Epoch 151 finished \tANN training loss 0.116219\n",
      ">> Epoch 152 finished \tANN training loss 0.111285\n",
      ">> Epoch 153 finished \tANN training loss 0.147819\n",
      ">> Epoch 154 finished \tANN training loss 0.155981\n",
      ">> Epoch 155 finished \tANN training loss 0.109063\n",
      ">> Epoch 156 finished \tANN training loss 0.118822\n",
      ">> Epoch 157 finished \tANN training loss 0.203921\n",
      ">> Epoch 158 finished \tANN training loss 0.119004\n",
      ">> Epoch 159 finished \tANN training loss 0.112320\n",
      ">> Epoch 160 finished \tANN training loss 0.365472\n",
      ">> Epoch 161 finished \tANN training loss 0.169334\n",
      ">> Epoch 162 finished \tANN training loss 0.118923\n",
      ">> Epoch 163 finished \tANN training loss 0.125067\n",
      ">> Epoch 164 finished \tANN training loss 0.165050\n",
      ">> Epoch 165 finished \tANN training loss 0.116091\n",
      ">> Epoch 166 finished \tANN training loss 0.115715\n",
      ">> Epoch 167 finished \tANN training loss 0.110988\n",
      ">> Epoch 168 finished \tANN training loss 0.128987\n",
      ">> Epoch 169 finished \tANN training loss 0.125263\n",
      ">> Epoch 170 finished \tANN training loss 0.099177\n",
      ">> Epoch 171 finished \tANN training loss 0.123750\n",
      ">> Epoch 172 finished \tANN training loss 0.108344\n",
      ">> Epoch 173 finished \tANN training loss 0.138457\n",
      ">> Epoch 174 finished \tANN training loss 0.117273\n",
      ">> Epoch 175 finished \tANN training loss 0.114410\n",
      ">> Epoch 176 finished \tANN training loss 0.110661\n",
      ">> Epoch 177 finished \tANN training loss 0.126174\n",
      ">> Epoch 178 finished \tANN training loss 0.144864\n",
      ">> Epoch 179 finished \tANN training loss 0.151672\n",
      ">> Epoch 180 finished \tANN training loss 0.107341\n",
      ">> Epoch 181 finished \tANN training loss 0.101784\n",
      ">> Epoch 182 finished \tANN training loss 0.111253\n",
      ">> Epoch 183 finished \tANN training loss 0.108834\n",
      ">> Epoch 184 finished \tANN training loss 0.103626\n",
      ">> Epoch 185 finished \tANN training loss 0.122107\n",
      ">> Epoch 186 finished \tANN training loss 0.108737\n",
      ">> Epoch 187 finished \tANN training loss 0.104664\n",
      ">> Epoch 188 finished \tANN training loss 0.116971\n",
      ">> Epoch 189 finished \tANN training loss 0.114497\n",
      ">> Epoch 190 finished \tANN training loss 0.132425\n",
      ">> Epoch 191 finished \tANN training loss 0.102310\n",
      ">> Epoch 192 finished \tANN training loss 0.102882\n",
      ">> Epoch 193 finished \tANN training loss 0.142735\n",
      ">> Epoch 194 finished \tANN training loss 0.113070\n",
      ">> Epoch 195 finished \tANN training loss 0.195762\n",
      ">> Epoch 196 finished \tANN training loss 0.127039\n",
      ">> Epoch 197 finished \tANN training loss 0.121363\n",
      ">> Epoch 198 finished \tANN training loss 0.166869\n",
      ">> Epoch 199 finished \tANN training loss 0.170368\n",
      ">> Epoch 200 finished \tANN training loss 0.113990\n",
      ">> Epoch 201 finished \tANN training loss 0.104751\n",
      ">> Epoch 202 finished \tANN training loss 0.158227\n",
      ">> Epoch 203 finished \tANN training loss 0.101875\n",
      ">> Epoch 204 finished \tANN training loss 0.089286\n",
      ">> Epoch 205 finished \tANN training loss 0.095063\n",
      ">> Epoch 206 finished \tANN training loss 0.134996\n",
      ">> Epoch 207 finished \tANN training loss 0.152359\n",
      ">> Epoch 208 finished \tANN training loss 0.104035\n",
      ">> Epoch 209 finished \tANN training loss 0.099325\n",
      ">> Epoch 210 finished \tANN training loss 0.157044\n",
      ">> Epoch 211 finished \tANN training loss 0.133267\n",
      ">> Epoch 212 finished \tANN training loss 0.163831\n",
      ">> Epoch 213 finished \tANN training loss 0.108871\n",
      ">> Epoch 214 finished \tANN training loss 0.130675\n",
      ">> Epoch 215 finished \tANN training loss 0.097224\n",
      ">> Epoch 216 finished \tANN training loss 0.096537\n",
      ">> Epoch 217 finished \tANN training loss 0.087517\n",
      ">> Epoch 218 finished \tANN training loss 0.100081\n",
      ">> Epoch 219 finished \tANN training loss 0.099351\n",
      ">> Epoch 220 finished \tANN training loss 0.092052\n",
      ">> Epoch 221 finished \tANN training loss 0.082203\n",
      ">> Epoch 222 finished \tANN training loss 0.121622\n",
      ">> Epoch 223 finished \tANN training loss 0.102371\n",
      ">> Epoch 224 finished \tANN training loss 0.103623\n",
      ">> Epoch 225 finished \tANN training loss 0.088843\n",
      ">> Epoch 226 finished \tANN training loss 0.078799\n",
      ">> Epoch 227 finished \tANN training loss 0.114888\n",
      ">> Epoch 228 finished \tANN training loss 0.085934\n",
      ">> Epoch 229 finished \tANN training loss 0.091001\n",
      ">> Epoch 230 finished \tANN training loss 0.128559\n",
      ">> Epoch 231 finished \tANN training loss 0.088368\n",
      ">> Epoch 232 finished \tANN training loss 0.077327\n",
      ">> Epoch 233 finished \tANN training loss 0.073962\n",
      ">> Epoch 234 finished \tANN training loss 0.071606\n",
      ">> Epoch 235 finished \tANN training loss 0.075614\n",
      ">> Epoch 236 finished \tANN training loss 0.090105\n",
      ">> Epoch 237 finished \tANN training loss 0.097538\n",
      ">> Epoch 238 finished \tANN training loss 0.094081\n",
      ">> Epoch 239 finished \tANN training loss 0.082041\n",
      ">> Epoch 240 finished \tANN training loss 0.136215\n",
      ">> Epoch 241 finished \tANN training loss 0.089291\n",
      ">> Epoch 242 finished \tANN training loss 0.094745\n",
      ">> Epoch 243 finished \tANN training loss 0.108875\n",
      ">> Epoch 244 finished \tANN training loss 0.074796\n",
      ">> Epoch 245 finished \tANN training loss 0.087627\n",
      ">> Epoch 246 finished \tANN training loss 0.169384\n",
      ">> Epoch 247 finished \tANN training loss 0.122944\n",
      ">> Epoch 248 finished \tANN training loss 0.180974\n",
      ">> Epoch 249 finished \tANN training loss 0.089784\n",
      ">> Epoch 250 finished \tANN training loss 0.103758\n",
      ">> Epoch 251 finished \tANN training loss 0.104396\n",
      ">> Epoch 252 finished \tANN training loss 0.090104\n",
      ">> Epoch 253 finished \tANN training loss 0.088130\n",
      ">> Epoch 254 finished \tANN training loss 0.085801\n",
      ">> Epoch 255 finished \tANN training loss 0.075317\n",
      ">> Epoch 256 finished \tANN training loss 0.244861\n",
      ">> Epoch 257 finished \tANN training loss 0.120878\n",
      ">> Epoch 258 finished \tANN training loss 0.079726\n",
      ">> Epoch 259 finished \tANN training loss 0.081663\n",
      ">> Epoch 260 finished \tANN training loss 0.354507\n",
      ">> Epoch 261 finished \tANN training loss 0.111676\n",
      ">> Epoch 262 finished \tANN training loss 0.115094\n",
      ">> Epoch 263 finished \tANN training loss 0.079120\n",
      ">> Epoch 264 finished \tANN training loss 0.112938\n",
      ">> Epoch 265 finished \tANN training loss 0.086321\n",
      ">> Epoch 266 finished \tANN training loss 0.081019\n",
      ">> Epoch 267 finished \tANN training loss 0.143189\n",
      ">> Epoch 268 finished \tANN training loss 0.129172\n",
      ">> Epoch 269 finished \tANN training loss 0.106147\n",
      ">> Epoch 270 finished \tANN training loss 0.084223\n",
      ">> Epoch 271 finished \tANN training loss 0.088616\n",
      ">> Epoch 272 finished \tANN training loss 0.092579\n",
      ">> Epoch 273 finished \tANN training loss 0.086781\n",
      ">> Epoch 274 finished \tANN training loss 0.066787\n",
      ">> Epoch 275 finished \tANN training loss 0.083158\n",
      ">> Epoch 276 finished \tANN training loss 0.084251\n",
      ">> Epoch 277 finished \tANN training loss 0.105869\n",
      ">> Epoch 278 finished \tANN training loss 0.096671\n",
      ">> Epoch 279 finished \tANN training loss 0.288516\n",
      ">> Epoch 280 finished \tANN training loss 0.151189\n",
      ">> Epoch 281 finished \tANN training loss 0.092958\n",
      ">> Epoch 282 finished \tANN training loss 0.082769\n",
      ">> Epoch 283 finished \tANN training loss 0.115004\n",
      ">> Epoch 284 finished \tANN training loss 0.117998\n",
      ">> Epoch 285 finished \tANN training loss 0.116604\n",
      ">> Epoch 286 finished \tANN training loss 0.107863\n",
      ">> Epoch 287 finished \tANN training loss 0.079270\n",
      ">> Epoch 288 finished \tANN training loss 0.145769\n",
      ">> Epoch 289 finished \tANN training loss 0.119994\n",
      ">> Epoch 290 finished \tANN training loss 0.091235\n",
      ">> Epoch 291 finished \tANN training loss 0.092374\n",
      ">> Epoch 292 finished \tANN training loss 0.074383\n",
      ">> Epoch 293 finished \tANN training loss 0.090019\n",
      ">> Epoch 294 finished \tANN training loss 0.102181\n",
      ">> Epoch 295 finished \tANN training loss 0.086896\n",
      ">> Epoch 296 finished \tANN training loss 0.104740\n",
      ">> Epoch 297 finished \tANN training loss 0.102609\n",
      ">> Epoch 298 finished \tANN training loss 0.107110\n",
      ">> Epoch 299 finished \tANN training loss 0.085240\n",
      ">> Epoch 300 finished \tANN training loss 0.081972\n",
      ">> Epoch 301 finished \tANN training loss 0.067033\n",
      ">> Epoch 302 finished \tANN training loss 0.140597\n",
      ">> Epoch 303 finished \tANN training loss 0.088683\n",
      ">> Epoch 304 finished \tANN training loss 0.087115\n",
      ">> Epoch 305 finished \tANN training loss 0.086894\n",
      ">> Epoch 306 finished \tANN training loss 0.068270\n",
      ">> Epoch 307 finished \tANN training loss 0.113329\n",
      ">> Epoch 308 finished \tANN training loss 0.091027\n",
      ">> Epoch 309 finished \tANN training loss 0.099107\n",
      ">> Epoch 310 finished \tANN training loss 0.084369\n",
      ">> Epoch 311 finished \tANN training loss 0.096351\n",
      ">> Epoch 312 finished \tANN training loss 0.085794\n",
      ">> Epoch 313 finished \tANN training loss 0.071637\n",
      ">> Epoch 314 finished \tANN training loss 0.076565\n",
      ">> Epoch 315 finished \tANN training loss 0.097894\n",
      ">> Epoch 316 finished \tANN training loss 0.081704\n",
      ">> Epoch 317 finished \tANN training loss 0.078424\n",
      ">> Epoch 318 finished \tANN training loss 0.117296\n",
      ">> Epoch 319 finished \tANN training loss 0.089994\n",
      ">> Epoch 320 finished \tANN training loss 0.143961\n",
      ">> Epoch 321 finished \tANN training loss 0.076470\n",
      ">> Epoch 322 finished \tANN training loss 0.075009\n",
      ">> Epoch 323 finished \tANN training loss 0.092455\n",
      ">> Epoch 324 finished \tANN training loss 0.132187\n",
      ">> Epoch 325 finished \tANN training loss 0.084238\n",
      ">> Epoch 326 finished \tANN training loss 0.090530\n",
      ">> Epoch 327 finished \tANN training loss 0.080642\n",
      ">> Epoch 328 finished \tANN training loss 0.086011\n",
      ">> Epoch 329 finished \tANN training loss 0.085921\n",
      ">> Epoch 330 finished \tANN training loss 0.069779\n",
      ">> Epoch 331 finished \tANN training loss 0.079293\n",
      ">> Epoch 332 finished \tANN training loss 0.081308\n",
      ">> Epoch 333 finished \tANN training loss 0.081949\n",
      ">> Epoch 334 finished \tANN training loss 0.078338\n",
      ">> Epoch 335 finished \tANN training loss 0.122249\n",
      ">> Epoch 336 finished \tANN training loss 0.088579\n",
      ">> Epoch 337 finished \tANN training loss 0.091899\n",
      ">> Epoch 338 finished \tANN training loss 0.092651\n",
      ">> Epoch 339 finished \tANN training loss 0.103001\n",
      ">> Epoch 340 finished \tANN training loss 0.100458\n",
      ">> Epoch 341 finished \tANN training loss 0.085964\n",
      ">> Epoch 342 finished \tANN training loss 0.072972\n",
      ">> Epoch 343 finished \tANN training loss 0.087378\n",
      ">> Epoch 344 finished \tANN training loss 0.087762\n",
      ">> Epoch 345 finished \tANN training loss 0.071112\n",
      ">> Epoch 346 finished \tANN training loss 0.069403\n",
      ">> Epoch 347 finished \tANN training loss 0.078574\n",
      ">> Epoch 348 finished \tANN training loss 0.069797\n",
      ">> Epoch 349 finished \tANN training loss 0.080827\n",
      ">> Epoch 350 finished \tANN training loss 0.087780\n",
      ">> Epoch 351 finished \tANN training loss 0.075484\n",
      ">> Epoch 352 finished \tANN training loss 0.136584\n",
      ">> Epoch 353 finished \tANN training loss 0.078340\n",
      ">> Epoch 354 finished \tANN training loss 0.097201\n",
      ">> Epoch 355 finished \tANN training loss 0.079710\n",
      ">> Epoch 356 finished \tANN training loss 0.078252\n",
      ">> Epoch 357 finished \tANN training loss 0.106863\n",
      ">> Epoch 358 finished \tANN training loss 0.070790\n",
      ">> Epoch 359 finished \tANN training loss 0.083063\n",
      ">> Epoch 360 finished \tANN training loss 0.084560\n",
      ">> Epoch 361 finished \tANN training loss 0.083757\n",
      ">> Epoch 362 finished \tANN training loss 0.086072\n",
      ">> Epoch 363 finished \tANN training loss 0.075090\n",
      ">> Epoch 364 finished \tANN training loss 0.096830\n",
      ">> Epoch 365 finished \tANN training loss 0.071099\n",
      ">> Epoch 366 finished \tANN training loss 0.074900\n",
      ">> Epoch 367 finished \tANN training loss 0.067751\n",
      ">> Epoch 368 finished \tANN training loss 0.064591\n",
      ">> Epoch 369 finished \tANN training loss 0.089188\n",
      ">> Epoch 370 finished \tANN training loss 0.063044\n",
      ">> Epoch 371 finished \tANN training loss 0.062876\n",
      ">> Epoch 372 finished \tANN training loss 0.067843\n",
      ">> Epoch 373 finished \tANN training loss 0.069120\n",
      ">> Epoch 374 finished \tANN training loss 0.074859\n",
      ">> Epoch 375 finished \tANN training loss 0.088317\n",
      ">> Epoch 376 finished \tANN training loss 0.077890\n",
      ">> Epoch 377 finished \tANN training loss 0.086132\n",
      ">> Epoch 378 finished \tANN training loss 0.077159\n",
      ">> Epoch 379 finished \tANN training loss 0.070158\n",
      ">> Epoch 380 finished \tANN training loss 0.067843\n",
      ">> Epoch 381 finished \tANN training loss 0.074432\n",
      ">> Epoch 382 finished \tANN training loss 0.074119\n",
      ">> Epoch 383 finished \tANN training loss 0.074287\n",
      ">> Epoch 384 finished \tANN training loss 0.083432\n",
      ">> Epoch 385 finished \tANN training loss 0.101794\n",
      ">> Epoch 386 finished \tANN training loss 0.084992\n",
      ">> Epoch 387 finished \tANN training loss 0.067043\n",
      ">> Epoch 388 finished \tANN training loss 0.064771\n",
      ">> Epoch 389 finished \tANN training loss 0.070711\n",
      ">> Epoch 390 finished \tANN training loss 0.114120\n",
      ">> Epoch 391 finished \tANN training loss 0.070321\n",
      ">> Epoch 392 finished \tANN training loss 0.083270\n",
      ">> Epoch 393 finished \tANN training loss 0.082352\n",
      ">> Epoch 394 finished \tANN training loss 0.085499\n",
      ">> Epoch 395 finished \tANN training loss 0.073145\n",
      ">> Epoch 396 finished \tANN training loss 0.068576\n",
      ">> Epoch 397 finished \tANN training loss 0.073002\n",
      ">> Epoch 398 finished \tANN training loss 0.095181\n",
      ">> Epoch 399 finished \tANN training loss 0.062664\n",
      ">> Epoch 400 finished \tANN training loss 0.063241\n",
      ">> Epoch 401 finished \tANN training loss 0.082031\n",
      ">> Epoch 402 finished \tANN training loss 0.062629\n",
      ">> Epoch 403 finished \tANN training loss 0.069109\n",
      ">> Epoch 404 finished \tANN training loss 0.094316\n",
      ">> Epoch 405 finished \tANN training loss 0.090283\n",
      ">> Epoch 406 finished \tANN training loss 0.087867\n",
      ">> Epoch 407 finished \tANN training loss 0.072516\n",
      ">> Epoch 408 finished \tANN training loss 0.073916\n",
      ">> Epoch 409 finished \tANN training loss 0.061072\n",
      ">> Epoch 410 finished \tANN training loss 0.067053\n",
      ">> Epoch 411 finished \tANN training loss 0.073117\n",
      ">> Epoch 412 finished \tANN training loss 0.080139\n",
      ">> Epoch 413 finished \tANN training loss 0.080403\n",
      ">> Epoch 414 finished \tANN training loss 0.072170\n",
      ">> Epoch 415 finished \tANN training loss 0.069056\n",
      ">> Epoch 416 finished \tANN training loss 0.060512\n",
      ">> Epoch 417 finished \tANN training loss 0.108657\n",
      ">> Epoch 418 finished \tANN training loss 0.066162\n",
      ">> Epoch 419 finished \tANN training loss 0.067960\n",
      ">> Epoch 420 finished \tANN training loss 0.070722\n",
      ">> Epoch 421 finished \tANN training loss 0.068296\n",
      ">> Epoch 422 finished \tANN training loss 0.076603\n",
      ">> Epoch 423 finished \tANN training loss 0.067580\n",
      ">> Epoch 424 finished \tANN training loss 0.071576\n",
      ">> Epoch 425 finished \tANN training loss 0.072185\n",
      ">> Epoch 426 finished \tANN training loss 0.067606\n",
      ">> Epoch 427 finished \tANN training loss 0.073761\n",
      ">> Epoch 428 finished \tANN training loss 0.067700\n",
      ">> Epoch 429 finished \tANN training loss 0.094988\n",
      ">> Epoch 430 finished \tANN training loss 0.101584\n",
      ">> Epoch 431 finished \tANN training loss 0.073631\n",
      ">> Epoch 432 finished \tANN training loss 0.102801\n",
      ">> Epoch 433 finished \tANN training loss 0.084103\n",
      ">> Epoch 434 finished \tANN training loss 0.064866\n",
      ">> Epoch 435 finished \tANN training loss 0.092373\n",
      ">> Epoch 436 finished \tANN training loss 0.108609\n",
      ">> Epoch 437 finished \tANN training loss 0.081924\n",
      ">> Epoch 438 finished \tANN training loss 0.068706\n",
      ">> Epoch 439 finished \tANN training loss 0.065704\n",
      ">> Epoch 440 finished \tANN training loss 0.065573\n",
      ">> Epoch 441 finished \tANN training loss 0.067207\n",
      ">> Epoch 442 finished \tANN training loss 0.069096\n",
      ">> Epoch 443 finished \tANN training loss 0.071290\n",
      ">> Epoch 444 finished \tANN training loss 0.062002\n",
      ">> Epoch 445 finished \tANN training loss 0.068730\n",
      ">> Epoch 446 finished \tANN training loss 0.075038\n",
      ">> Epoch 447 finished \tANN training loss 0.061782\n",
      ">> Epoch 448 finished \tANN training loss 0.075838\n",
      ">> Epoch 449 finished \tANN training loss 0.070612\n",
      ">> Epoch 450 finished \tANN training loss 0.062819\n",
      ">> Epoch 451 finished \tANN training loss 0.059190\n",
      ">> Epoch 452 finished \tANN training loss 0.061546\n",
      ">> Epoch 453 finished \tANN training loss 0.092460\n",
      ">> Epoch 454 finished \tANN training loss 0.069930\n",
      ">> Epoch 455 finished \tANN training loss 0.068353\n",
      ">> Epoch 456 finished \tANN training loss 0.065851\n",
      ">> Epoch 457 finished \tANN training loss 0.058682\n",
      ">> Epoch 458 finished \tANN training loss 0.118329\n",
      ">> Epoch 459 finished \tANN training loss 0.116199\n",
      ">> Epoch 460 finished \tANN training loss 0.078655\n",
      ">> Epoch 461 finished \tANN training loss 0.076409\n",
      ">> Epoch 462 finished \tANN training loss 0.073649\n",
      ">> Epoch 463 finished \tANN training loss 0.067862\n",
      ">> Epoch 464 finished \tANN training loss 0.060589\n",
      ">> Epoch 465 finished \tANN training loss 0.064988\n",
      ">> Epoch 466 finished \tANN training loss 0.076426\n",
      ">> Epoch 467 finished \tANN training loss 0.066685\n",
      ">> Epoch 468 finished \tANN training loss 0.078659\n",
      ">> Epoch 469 finished \tANN training loss 0.066525\n",
      ">> Epoch 470 finished \tANN training loss 0.092142\n",
      ">> Epoch 471 finished \tANN training loss 0.086908\n",
      ">> Epoch 472 finished \tANN training loss 0.070774\n",
      ">> Epoch 473 finished \tANN training loss 0.065643\n",
      ">> Epoch 474 finished \tANN training loss 0.092153\n",
      ">> Epoch 475 finished \tANN training loss 0.086181\n",
      ">> Epoch 476 finished \tANN training loss 0.072565\n",
      ">> Epoch 477 finished \tANN training loss 0.070319\n",
      ">> Epoch 478 finished \tANN training loss 0.058139\n",
      ">> Epoch 479 finished \tANN training loss 0.059689\n",
      ">> Epoch 480 finished \tANN training loss 0.061906\n",
      ">> Epoch 481 finished \tANN training loss 0.064031\n",
      ">> Epoch 482 finished \tANN training loss 0.057283\n",
      ">> Epoch 483 finished \tANN training loss 0.056520\n",
      ">> Epoch 484 finished \tANN training loss 0.051392\n",
      ">> Epoch 485 finished \tANN training loss 0.065972\n",
      ">> Epoch 486 finished \tANN training loss 0.062908\n",
      ">> Epoch 487 finished \tANN training loss 0.055747\n",
      ">> Epoch 488 finished \tANN training loss 0.054358\n",
      ">> Epoch 489 finished \tANN training loss 0.074698\n",
      ">> Epoch 490 finished \tANN training loss 0.068299\n",
      ">> Epoch 491 finished \tANN training loss 0.066179\n",
      ">> Epoch 492 finished \tANN training loss 0.068857\n",
      ">> Epoch 493 finished \tANN training loss 0.080758\n",
      ">> Epoch 494 finished \tANN training loss 0.094790\n",
      ">> Epoch 495 finished \tANN training loss 0.077400\n",
      ">> Epoch 496 finished \tANN training loss 0.055734\n",
      ">> Epoch 497 finished \tANN training loss 0.064297\n",
      ">> Epoch 498 finished \tANN training loss 0.068891\n",
      ">> Epoch 499 finished \tANN training loss 0.059491\n",
      ">> Epoch 500 finished \tANN training loss 0.091955\n",
      ">> Epoch 501 finished \tANN training loss 0.071422\n",
      ">> Epoch 502 finished \tANN training loss 0.067571\n",
      ">> Epoch 503 finished \tANN training loss 0.067944\n",
      ">> Epoch 504 finished \tANN training loss 0.058513\n",
      ">> Epoch 505 finished \tANN training loss 0.056513\n",
      ">> Epoch 506 finished \tANN training loss 0.063220\n",
      ">> Epoch 507 finished \tANN training loss 0.093231\n",
      ">> Epoch 508 finished \tANN training loss 0.064931\n",
      ">> Epoch 509 finished \tANN training loss 0.059693\n",
      ">> Epoch 510 finished \tANN training loss 0.078873\n",
      ">> Epoch 511 finished \tANN training loss 0.065504\n",
      ">> Epoch 512 finished \tANN training loss 0.065291\n",
      ">> Epoch 513 finished \tANN training loss 0.104827\n",
      ">> Epoch 514 finished \tANN training loss 0.096013\n",
      ">> Epoch 515 finished \tANN training loss 0.094892\n",
      ">> Epoch 516 finished \tANN training loss 0.066181\n",
      ">> Epoch 517 finished \tANN training loss 0.061342\n",
      ">> Epoch 518 finished \tANN training loss 0.054396\n",
      ">> Epoch 519 finished \tANN training loss 0.055295\n",
      ">> Epoch 520 finished \tANN training loss 0.054231\n",
      ">> Epoch 521 finished \tANN training loss 0.061404\n",
      ">> Epoch 522 finished \tANN training loss 0.065662\n",
      ">> Epoch 523 finished \tANN training loss 0.092456\n",
      ">> Epoch 524 finished \tANN training loss 0.057839\n",
      ">> Epoch 525 finished \tANN training loss 0.075319\n",
      ">> Epoch 526 finished \tANN training loss 0.085570\n",
      ">> Epoch 527 finished \tANN training loss 0.065676\n",
      ">> Epoch 528 finished \tANN training loss 0.157876\n",
      ">> Epoch 529 finished \tANN training loss 0.072645\n",
      ">> Epoch 530 finished \tANN training loss 0.143956\n",
      ">> Epoch 531 finished \tANN training loss 0.073616\n",
      ">> Epoch 532 finished \tANN training loss 0.083762\n",
      ">> Epoch 533 finished \tANN training loss 0.067480\n",
      ">> Epoch 534 finished \tANN training loss 0.068777\n",
      ">> Epoch 535 finished \tANN training loss 0.069604\n",
      ">> Epoch 536 finished \tANN training loss 0.063936\n",
      ">> Epoch 537 finished \tANN training loss 0.066472\n",
      ">> Epoch 538 finished \tANN training loss 0.068354\n",
      ">> Epoch 539 finished \tANN training loss 0.065249\n",
      ">> Epoch 540 finished \tANN training loss 0.064244\n",
      ">> Epoch 541 finished \tANN training loss 0.091099\n",
      ">> Epoch 542 finished \tANN training loss 0.059258\n",
      ">> Epoch 543 finished \tANN training loss 0.088875\n",
      ">> Epoch 544 finished \tANN training loss 0.058378\n",
      ">> Epoch 545 finished \tANN training loss 0.063173\n",
      ">> Epoch 546 finished \tANN training loss 0.070063\n",
      ">> Epoch 547 finished \tANN training loss 0.061996\n",
      ">> Epoch 548 finished \tANN training loss 0.059039\n",
      ">> Epoch 549 finished \tANN training loss 0.062770\n",
      ">> Epoch 550 finished \tANN training loss 0.056988\n",
      ">> Epoch 551 finished \tANN training loss 0.071272\n",
      ">> Epoch 552 finished \tANN training loss 0.053912\n",
      ">> Epoch 553 finished \tANN training loss 0.061272\n",
      ">> Epoch 554 finished \tANN training loss 0.056330\n",
      ">> Epoch 555 finished \tANN training loss 0.060817\n",
      ">> Epoch 556 finished \tANN training loss 0.074521\n",
      ">> Epoch 557 finished \tANN training loss 0.061820\n",
      ">> Epoch 558 finished \tANN training loss 0.051599\n",
      ">> Epoch 559 finished \tANN training loss 0.062044\n",
      ">> Epoch 560 finished \tANN training loss 0.060135\n",
      ">> Epoch 561 finished \tANN training loss 0.064561\n",
      ">> Epoch 562 finished \tANN training loss 0.071958\n",
      ">> Epoch 563 finished \tANN training loss 0.070660\n",
      ">> Epoch 564 finished \tANN training loss 0.062963\n",
      ">> Epoch 565 finished \tANN training loss 0.061690\n",
      ">> Epoch 566 finished \tANN training loss 0.068693\n",
      ">> Epoch 567 finished \tANN training loss 0.063698\n",
      ">> Epoch 568 finished \tANN training loss 0.062137\n",
      ">> Epoch 569 finished \tANN training loss 0.058673\n",
      ">> Epoch 570 finished \tANN training loss 0.060334\n",
      ">> Epoch 571 finished \tANN training loss 0.123153\n",
      ">> Epoch 572 finished \tANN training loss 0.076178\n",
      ">> Epoch 573 finished \tANN training loss 0.065658\n",
      ">> Epoch 574 finished \tANN training loss 0.105094\n",
      ">> Epoch 575 finished \tANN training loss 0.064707\n",
      ">> Epoch 576 finished \tANN training loss 0.073363\n",
      ">> Epoch 577 finished \tANN training loss 0.059095\n",
      ">> Epoch 578 finished \tANN training loss 0.063405\n",
      ">> Epoch 579 finished \tANN training loss 0.054192\n",
      ">> Epoch 580 finished \tANN training loss 0.125289\n",
      ">> Epoch 581 finished \tANN training loss 0.056608\n",
      ">> Epoch 582 finished \tANN training loss 0.051563\n",
      ">> Epoch 583 finished \tANN training loss 0.050736\n",
      ">> Epoch 584 finished \tANN training loss 0.052425\n",
      ">> Epoch 585 finished \tANN training loss 0.080824\n",
      ">> Epoch 586 finished \tANN training loss 0.061248\n",
      ">> Epoch 587 finished \tANN training loss 0.060512\n",
      ">> Epoch 588 finished \tANN training loss 0.059612\n",
      ">> Epoch 589 finished \tANN training loss 0.063908\n",
      ">> Epoch 590 finished \tANN training loss 0.087009\n",
      ">> Epoch 591 finished \tANN training loss 0.062102\n",
      ">> Epoch 592 finished \tANN training loss 0.060610\n",
      ">> Epoch 593 finished \tANN training loss 0.053697\n",
      ">> Epoch 594 finished \tANN training loss 0.053355\n",
      ">> Epoch 595 finished \tANN training loss 0.070734\n",
      ">> Epoch 596 finished \tANN training loss 0.063880\n",
      ">> Epoch 597 finished \tANN training loss 0.093652\n",
      ">> Epoch 598 finished \tANN training loss 0.067575\n",
      ">> Epoch 599 finished \tANN training loss 0.064743\n",
      ">> Epoch 600 finished \tANN training loss 0.069739\n",
      ">> Epoch 601 finished \tANN training loss 0.081463\n",
      ">> Epoch 602 finished \tANN training loss 0.065819\n",
      ">> Epoch 603 finished \tANN training loss 0.060392\n",
      ">> Epoch 604 finished \tANN training loss 0.061016\n",
      ">> Epoch 605 finished \tANN training loss 0.062125\n",
      ">> Epoch 606 finished \tANN training loss 0.059871\n",
      ">> Epoch 607 finished \tANN training loss 0.064121\n",
      ">> Epoch 608 finished \tANN training loss 0.109910\n",
      ">> Epoch 609 finished \tANN training loss 0.076912\n",
      ">> Epoch 610 finished \tANN training loss 0.067965\n",
      ">> Epoch 611 finished \tANN training loss 0.078737\n",
      ">> Epoch 612 finished \tANN training loss 0.060758\n",
      ">> Epoch 613 finished \tANN training loss 0.062213\n",
      ">> Epoch 614 finished \tANN training loss 0.074166\n",
      ">> Epoch 615 finished \tANN training loss 0.062641\n",
      ">> Epoch 616 finished \tANN training loss 0.069998\n",
      ">> Epoch 617 finished \tANN training loss 0.063415\n",
      ">> Epoch 618 finished \tANN training loss 0.068109\n",
      ">> Epoch 619 finished \tANN training loss 0.050258\n",
      ">> Epoch 620 finished \tANN training loss 0.057410\n",
      ">> Epoch 621 finished \tANN training loss 0.059462\n",
      ">> Epoch 622 finished \tANN training loss 0.058031\n",
      ">> Epoch 623 finished \tANN training loss 0.057771\n",
      ">> Epoch 624 finished \tANN training loss 0.060134\n",
      ">> Epoch 625 finished \tANN training loss 0.048581\n",
      ">> Epoch 626 finished \tANN training loss 0.063107\n",
      ">> Epoch 627 finished \tANN training loss 0.060118\n",
      ">> Epoch 628 finished \tANN training loss 0.061252\n",
      ">> Epoch 629 finished \tANN training loss 0.067563\n",
      ">> Epoch 630 finished \tANN training loss 0.055800\n",
      ">> Epoch 631 finished \tANN training loss 0.050122\n",
      ">> Epoch 632 finished \tANN training loss 0.051900\n",
      ">> Epoch 633 finished \tANN training loss 0.057382\n",
      ">> Epoch 634 finished \tANN training loss 0.055501\n",
      ">> Epoch 635 finished \tANN training loss 0.051757\n",
      ">> Epoch 636 finished \tANN training loss 0.054372\n",
      ">> Epoch 637 finished \tANN training loss 0.049530\n",
      ">> Epoch 638 finished \tANN training loss 0.054341\n",
      ">> Epoch 639 finished \tANN training loss 0.059047\n",
      ">> Epoch 640 finished \tANN training loss 0.055748\n",
      ">> Epoch 641 finished \tANN training loss 0.075739\n",
      ">> Epoch 642 finished \tANN training loss 0.064389\n",
      ">> Epoch 643 finished \tANN training loss 0.055639\n",
      ">> Epoch 644 finished \tANN training loss 0.056494\n",
      ">> Epoch 645 finished \tANN training loss 0.057531\n",
      ">> Epoch 646 finished \tANN training loss 0.064151\n",
      ">> Epoch 647 finished \tANN training loss 0.063723\n",
      ">> Epoch 648 finished \tANN training loss 0.053927\n",
      ">> Epoch 649 finished \tANN training loss 0.055082\n",
      ">> Epoch 650 finished \tANN training loss 0.055588\n",
      ">> Epoch 651 finished \tANN training loss 0.058658\n",
      ">> Epoch 652 finished \tANN training loss 0.050278\n",
      ">> Epoch 653 finished \tANN training loss 0.051526\n",
      ">> Epoch 654 finished \tANN training loss 0.051241\n",
      ">> Epoch 655 finished \tANN training loss 0.046234\n",
      ">> Epoch 656 finished \tANN training loss 0.052308\n",
      ">> Epoch 657 finished \tANN training loss 0.047561\n",
      ">> Epoch 658 finished \tANN training loss 0.060482\n",
      ">> Epoch 659 finished \tANN training loss 0.052629\n",
      ">> Epoch 660 finished \tANN training loss 0.058650\n",
      ">> Epoch 661 finished \tANN training loss 0.067612\n",
      ">> Epoch 662 finished \tANN training loss 0.064200\n",
      ">> Epoch 663 finished \tANN training loss 0.069385\n",
      ">> Epoch 664 finished \tANN training loss 0.077509\n",
      ">> Epoch 665 finished \tANN training loss 0.068438\n",
      ">> Epoch 666 finished \tANN training loss 0.055914\n",
      ">> Epoch 667 finished \tANN training loss 0.052878\n",
      ">> Epoch 668 finished \tANN training loss 0.055150\n",
      ">> Epoch 669 finished \tANN training loss 0.047071\n",
      ">> Epoch 670 finished \tANN training loss 0.055189\n",
      ">> Epoch 671 finished \tANN training loss 0.051051\n",
      ">> Epoch 672 finished \tANN training loss 0.056979\n",
      ">> Epoch 673 finished \tANN training loss 0.047691\n",
      ">> Epoch 674 finished \tANN training loss 0.053505\n",
      ">> Epoch 675 finished \tANN training loss 0.053539\n",
      ">> Epoch 676 finished \tANN training loss 0.062897\n",
      ">> Epoch 677 finished \tANN training loss 0.068958\n",
      ">> Epoch 678 finished \tANN training loss 0.052181\n",
      ">> Epoch 679 finished \tANN training loss 0.064891\n",
      ">> Epoch 680 finished \tANN training loss 0.057019\n",
      ">> Epoch 681 finished \tANN training loss 0.057613\n",
      ">> Epoch 682 finished \tANN training loss 0.060753\n",
      ">> Epoch 683 finished \tANN training loss 0.056579\n",
      ">> Epoch 684 finished \tANN training loss 0.058256\n",
      ">> Epoch 685 finished \tANN training loss 0.058616\n",
      ">> Epoch 686 finished \tANN training loss 0.063226\n",
      ">> Epoch 687 finished \tANN training loss 0.052199\n",
      ">> Epoch 688 finished \tANN training loss 0.061348\n",
      ">> Epoch 689 finished \tANN training loss 0.053983\n",
      ">> Epoch 690 finished \tANN training loss 0.050070\n",
      ">> Epoch 691 finished \tANN training loss 0.058809\n",
      ">> Epoch 692 finished \tANN training loss 0.063154\n",
      ">> Epoch 693 finished \tANN training loss 0.052929\n",
      ">> Epoch 694 finished \tANN training loss 0.049056\n",
      ">> Epoch 695 finished \tANN training loss 0.042567\n",
      ">> Epoch 696 finished \tANN training loss 0.091595\n",
      ">> Epoch 697 finished \tANN training loss 0.051997\n",
      ">> Epoch 698 finished \tANN training loss 0.105517\n",
      ">> Epoch 699 finished \tANN training loss 0.050861\n",
      ">> Epoch 700 finished \tANN training loss 0.053363\n",
      ">> Epoch 701 finished \tANN training loss 0.048872\n",
      ">> Epoch 702 finished \tANN training loss 0.057451\n",
      ">> Epoch 703 finished \tANN training loss 0.055395\n",
      ">> Epoch 704 finished \tANN training loss 0.052329\n",
      ">> Epoch 705 finished \tANN training loss 0.064621\n",
      ">> Epoch 706 finished \tANN training loss 0.071947\n",
      ">> Epoch 707 finished \tANN training loss 0.052848\n",
      ">> Epoch 708 finished \tANN training loss 0.052160\n",
      ">> Epoch 709 finished \tANN training loss 0.047444\n",
      ">> Epoch 710 finished \tANN training loss 0.051069\n",
      ">> Epoch 711 finished \tANN training loss 0.067630\n",
      ">> Epoch 712 finished \tANN training loss 0.084330\n",
      ">> Epoch 713 finished \tANN training loss 0.058241\n",
      ">> Epoch 714 finished \tANN training loss 0.057995\n",
      ">> Epoch 715 finished \tANN training loss 0.059561\n",
      ">> Epoch 716 finished \tANN training loss 0.059741\n",
      ">> Epoch 717 finished \tANN training loss 0.062440\n",
      ">> Epoch 718 finished \tANN training loss 0.057313\n",
      ">> Epoch 719 finished \tANN training loss 0.112816\n",
      ">> Epoch 720 finished \tANN training loss 0.052341\n",
      ">> Epoch 721 finished \tANN training loss 0.048172\n",
      ">> Epoch 722 finished \tANN training loss 0.050091\n",
      ">> Epoch 723 finished \tANN training loss 0.055592\n",
      ">> Epoch 724 finished \tANN training loss 0.044862\n",
      ">> Epoch 725 finished \tANN training loss 0.044403\n",
      ">> Epoch 726 finished \tANN training loss 0.049957\n",
      ">> Epoch 727 finished \tANN training loss 0.055546\n",
      ">> Epoch 728 finished \tANN training loss 0.079070\n",
      ">> Epoch 729 finished \tANN training loss 0.057147\n",
      ">> Epoch 730 finished \tANN training loss 0.053003\n",
      ">> Epoch 731 finished \tANN training loss 0.057104\n",
      ">> Epoch 732 finished \tANN training loss 0.049276\n",
      ">> Epoch 733 finished \tANN training loss 0.044920\n",
      ">> Epoch 734 finished \tANN training loss 0.050515\n",
      ">> Epoch 735 finished \tANN training loss 0.049933\n",
      ">> Epoch 736 finished \tANN training loss 0.055198\n",
      ">> Epoch 737 finished \tANN training loss 0.069800\n",
      ">> Epoch 738 finished \tANN training loss 0.070855\n",
      ">> Epoch 739 finished \tANN training loss 0.061569\n",
      ">> Epoch 740 finished \tANN training loss 0.068172\n",
      ">> Epoch 741 finished \tANN training loss 0.071512\n",
      ">> Epoch 742 finished \tANN training loss 0.065753\n",
      ">> Epoch 743 finished \tANN training loss 0.066647\n",
      ">> Epoch 744 finished \tANN training loss 0.058409\n",
      ">> Epoch 745 finished \tANN training loss 0.062141\n",
      ">> Epoch 746 finished \tANN training loss 0.053501\n",
      ">> Epoch 747 finished \tANN training loss 0.052392\n",
      ">> Epoch 748 finished \tANN training loss 0.060458\n",
      ">> Epoch 749 finished \tANN training loss 0.052484\n",
      ">> Epoch 750 finished \tANN training loss 0.048639\n",
      ">> Epoch 751 finished \tANN training loss 0.051082\n",
      ">> Epoch 752 finished \tANN training loss 0.051715\n",
      ">> Epoch 753 finished \tANN training loss 0.057979\n",
      ">> Epoch 754 finished \tANN training loss 0.062649\n",
      ">> Epoch 755 finished \tANN training loss 0.057682\n",
      ">> Epoch 756 finished \tANN training loss 0.053942\n",
      ">> Epoch 757 finished \tANN training loss 0.044741\n",
      ">> Epoch 758 finished \tANN training loss 0.059395\n",
      ">> Epoch 759 finished \tANN training loss 0.056403\n",
      ">> Epoch 760 finished \tANN training loss 0.056575\n",
      ">> Epoch 761 finished \tANN training loss 0.065568\n",
      ">> Epoch 762 finished \tANN training loss 0.055988\n",
      ">> Epoch 763 finished \tANN training loss 0.050625\n",
      ">> Epoch 764 finished \tANN training loss 0.050415\n",
      ">> Epoch 765 finished \tANN training loss 0.051336\n",
      ">> Epoch 766 finished \tANN training loss 0.051491\n",
      ">> Epoch 767 finished \tANN training loss 0.056974\n",
      ">> Epoch 768 finished \tANN training loss 0.059108\n",
      ">> Epoch 769 finished \tANN training loss 0.066766\n",
      ">> Epoch 770 finished \tANN training loss 0.105537\n",
      ">> Epoch 771 finished \tANN training loss 0.069259\n",
      ">> Epoch 772 finished \tANN training loss 0.061923\n",
      ">> Epoch 773 finished \tANN training loss 0.058609\n",
      ">> Epoch 774 finished \tANN training loss 0.059005\n",
      ">> Epoch 775 finished \tANN training loss 0.056299\n",
      ">> Epoch 776 finished \tANN training loss 0.059966\n",
      ">> Epoch 777 finished \tANN training loss 0.056111\n",
      ">> Epoch 778 finished \tANN training loss 0.052340\n",
      ">> Epoch 779 finished \tANN training loss 0.052181\n",
      ">> Epoch 780 finished \tANN training loss 0.051025\n",
      ">> Epoch 781 finished \tANN training loss 0.050800\n",
      ">> Epoch 782 finished \tANN training loss 0.053405\n",
      ">> Epoch 783 finished \tANN training loss 0.068724\n",
      ">> Epoch 784 finished \tANN training loss 0.073409\n",
      ">> Epoch 785 finished \tANN training loss 0.068126\n",
      ">> Epoch 786 finished \tANN training loss 0.079147\n",
      ">> Epoch 787 finished \tANN training loss 0.055031\n",
      ">> Epoch 788 finished \tANN training loss 0.060458\n",
      ">> Epoch 789 finished \tANN training loss 0.058671\n",
      ">> Epoch 790 finished \tANN training loss 0.053485\n",
      ">> Epoch 791 finished \tANN training loss 0.057593\n",
      ">> Epoch 792 finished \tANN training loss 0.060458\n",
      ">> Epoch 793 finished \tANN training loss 0.064659\n",
      ">> Epoch 794 finished \tANN training loss 0.060996\n",
      ">> Epoch 795 finished \tANN training loss 0.062232\n",
      ">> Epoch 796 finished \tANN training loss 0.062730\n",
      ">> Epoch 797 finished \tANN training loss 0.066782\n",
      ">> Epoch 798 finished \tANN training loss 0.052251\n",
      ">> Epoch 799 finished \tANN training loss 0.074705\n",
      ">> Epoch 800 finished \tANN training loss 0.055707\n",
      ">> Epoch 801 finished \tANN training loss 0.056397\n",
      ">> Epoch 802 finished \tANN training loss 0.055616\n",
      ">> Epoch 803 finished \tANN training loss 0.056201\n",
      ">> Epoch 804 finished \tANN training loss 0.060886\n",
      ">> Epoch 805 finished \tANN training loss 0.064106\n",
      ">> Epoch 806 finished \tANN training loss 0.060489\n",
      ">> Epoch 807 finished \tANN training loss 0.075362\n",
      ">> Epoch 808 finished \tANN training loss 0.059805\n",
      ">> Epoch 809 finished \tANN training loss 0.051537\n",
      ">> Epoch 810 finished \tANN training loss 0.051772\n",
      ">> Epoch 811 finished \tANN training loss 0.053690\n",
      ">> Epoch 812 finished \tANN training loss 0.062994\n",
      ">> Epoch 813 finished \tANN training loss 0.054197\n",
      ">> Epoch 814 finished \tANN training loss 0.073224\n",
      ">> Epoch 815 finished \tANN training loss 0.050422\n",
      ">> Epoch 816 finished \tANN training loss 0.054822\n",
      ">> Epoch 817 finished \tANN training loss 0.050555\n",
      ">> Epoch 818 finished \tANN training loss 0.053341\n",
      ">> Epoch 819 finished \tANN training loss 0.065898\n",
      ">> Epoch 820 finished \tANN training loss 0.065968\n",
      ">> Epoch 821 finished \tANN training loss 0.061156\n",
      ">> Epoch 822 finished \tANN training loss 0.055652\n",
      ">> Epoch 823 finished \tANN training loss 0.095612\n",
      ">> Epoch 824 finished \tANN training loss 0.056416\n",
      ">> Epoch 825 finished \tANN training loss 0.053383\n",
      ">> Epoch 826 finished \tANN training loss 0.048340\n",
      ">> Epoch 827 finished \tANN training loss 0.048050\n",
      ">> Epoch 828 finished \tANN training loss 0.123763\n",
      ">> Epoch 829 finished \tANN training loss 0.065071\n",
      ">> Epoch 830 finished \tANN training loss 0.053069\n",
      ">> Epoch 831 finished \tANN training loss 0.052646\n",
      ">> Epoch 832 finished \tANN training loss 0.051156\n",
      ">> Epoch 833 finished \tANN training loss 0.054042\n",
      ">> Epoch 834 finished \tANN training loss 0.045606\n",
      ">> Epoch 835 finished \tANN training loss 0.040357\n",
      ">> Epoch 836 finished \tANN training loss 0.045917\n",
      ">> Epoch 837 finished \tANN training loss 0.055586\n",
      ">> Epoch 838 finished \tANN training loss 0.054238\n",
      ">> Epoch 839 finished \tANN training loss 0.059256\n",
      ">> Epoch 840 finished \tANN training loss 0.048341\n",
      ">> Epoch 841 finished \tANN training loss 0.085357\n",
      ">> Epoch 842 finished \tANN training loss 0.074611\n",
      ">> Epoch 843 finished \tANN training loss 0.054321\n",
      ">> Epoch 844 finished \tANN training loss 0.066166\n",
      ">> Epoch 845 finished \tANN training loss 0.062116\n",
      ">> Epoch 846 finished \tANN training loss 0.068719\n",
      ">> Epoch 847 finished \tANN training loss 0.052086\n",
      ">> Epoch 848 finished \tANN training loss 0.091036\n",
      ">> Epoch 849 finished \tANN training loss 0.065270\n",
      ">> Epoch 850 finished \tANN training loss 0.060626\n",
      ">> Epoch 851 finished \tANN training loss 0.063604\n",
      ">> Epoch 852 finished \tANN training loss 0.073140\n",
      ">> Epoch 853 finished \tANN training loss 0.066968\n",
      ">> Epoch 854 finished \tANN training loss 0.061511\n",
      ">> Epoch 855 finished \tANN training loss 0.055904\n",
      ">> Epoch 856 finished \tANN training loss 0.054009\n",
      ">> Epoch 857 finished \tANN training loss 0.055770\n",
      ">> Epoch 858 finished \tANN training loss 0.064402\n",
      ">> Epoch 859 finished \tANN training loss 0.060263\n",
      ">> Epoch 860 finished \tANN training loss 0.053552\n",
      ">> Epoch 861 finished \tANN training loss 0.074225\n",
      ">> Epoch 862 finished \tANN training loss 0.064520\n",
      ">> Epoch 863 finished \tANN training loss 0.073201\n",
      ">> Epoch 864 finished \tANN training loss 0.069274\n",
      ">> Epoch 865 finished \tANN training loss 0.074971\n",
      ">> Epoch 866 finished \tANN training loss 0.062449\n",
      ">> Epoch 867 finished \tANN training loss 0.055880\n",
      ">> Epoch 868 finished \tANN training loss 0.055041\n",
      ">> Epoch 869 finished \tANN training loss 0.055048\n",
      ">> Epoch 870 finished \tANN training loss 0.056032\n",
      ">> Epoch 871 finished \tANN training loss 0.053534\n",
      ">> Epoch 872 finished \tANN training loss 0.049490\n",
      ">> Epoch 873 finished \tANN training loss 0.046519\n",
      ">> Epoch 874 finished \tANN training loss 0.057343\n",
      ">> Epoch 875 finished \tANN training loss 0.053558\n",
      ">> Epoch 876 finished \tANN training loss 0.052914\n",
      ">> Epoch 877 finished \tANN training loss 0.055980\n",
      ">> Epoch 878 finished \tANN training loss 0.049226\n",
      ">> Epoch 879 finished \tANN training loss 0.048692\n",
      ">> Epoch 880 finished \tANN training loss 0.070714\n",
      ">> Epoch 881 finished \tANN training loss 0.048819\n",
      ">> Epoch 882 finished \tANN training loss 0.047076\n",
      ">> Epoch 883 finished \tANN training loss 0.053030\n",
      ">> Epoch 884 finished \tANN training loss 0.076985\n",
      ">> Epoch 885 finished \tANN training loss 0.059510\n",
      ">> Epoch 886 finished \tANN training loss 0.051087\n",
      ">> Epoch 887 finished \tANN training loss 0.046531\n",
      ">> Epoch 888 finished \tANN training loss 0.048633\n",
      ">> Epoch 889 finished \tANN training loss 0.067206\n",
      ">> Epoch 890 finished \tANN training loss 0.056483\n",
      ">> Epoch 891 finished \tANN training loss 0.049186\n",
      ">> Epoch 892 finished \tANN training loss 0.048726\n",
      ">> Epoch 893 finished \tANN training loss 0.053462\n",
      ">> Epoch 894 finished \tANN training loss 0.050414\n",
      ">> Epoch 895 finished \tANN training loss 0.069640\n",
      ">> Epoch 896 finished \tANN training loss 0.065684\n",
      ">> Epoch 897 finished \tANN training loss 0.059344\n",
      ">> Epoch 898 finished \tANN training loss 0.055815\n",
      ">> Epoch 899 finished \tANN training loss 0.052026\n",
      ">> Epoch 900 finished \tANN training loss 0.060141\n",
      ">> Epoch 901 finished \tANN training loss 0.053440\n",
      ">> Epoch 902 finished \tANN training loss 0.047987\n",
      ">> Epoch 903 finished \tANN training loss 0.045646\n",
      ">> Epoch 904 finished \tANN training loss 0.063296\n",
      ">> Epoch 905 finished \tANN training loss 0.065049\n",
      ">> Epoch 906 finished \tANN training loss 0.048058\n",
      ">> Epoch 907 finished \tANN training loss 0.049528\n",
      ">> Epoch 908 finished \tANN training loss 0.072249\n",
      ">> Epoch 909 finished \tANN training loss 0.055895\n",
      ">> Epoch 910 finished \tANN training loss 0.056911\n",
      ">> Epoch 911 finished \tANN training loss 0.060128\n",
      ">> Epoch 912 finished \tANN training loss 0.053423\n",
      ">> Epoch 913 finished \tANN training loss 0.070652\n",
      ">> Epoch 914 finished \tANN training loss 0.080441\n",
      ">> Epoch 915 finished \tANN training loss 0.062397\n",
      ">> Epoch 916 finished \tANN training loss 0.053367\n",
      ">> Epoch 917 finished \tANN training loss 0.053665\n",
      ">> Epoch 918 finished \tANN training loss 0.050525\n",
      ">> Epoch 919 finished \tANN training loss 0.059508\n",
      ">> Epoch 920 finished \tANN training loss 0.052142\n",
      ">> Epoch 921 finished \tANN training loss 0.058556\n",
      ">> Epoch 922 finished \tANN training loss 0.081920\n",
      ">> Epoch 923 finished \tANN training loss 0.058181\n",
      ">> Epoch 924 finished \tANN training loss 0.051597\n",
      ">> Epoch 925 finished \tANN training loss 0.079772\n",
      ">> Epoch 926 finished \tANN training loss 0.050336\n",
      ">> Epoch 927 finished \tANN training loss 0.056081\n",
      ">> Epoch 928 finished \tANN training loss 0.053399\n",
      ">> Epoch 929 finished \tANN training loss 0.054724\n",
      ">> Epoch 930 finished \tANN training loss 0.053846\n",
      ">> Epoch 931 finished \tANN training loss 0.057579\n",
      ">> Epoch 932 finished \tANN training loss 0.058042\n",
      ">> Epoch 933 finished \tANN training loss 0.047849\n",
      ">> Epoch 934 finished \tANN training loss 0.043160\n",
      ">> Epoch 935 finished \tANN training loss 0.045764\n",
      ">> Epoch 936 finished \tANN training loss 0.048808\n",
      ">> Epoch 937 finished \tANN training loss 0.053731\n",
      ">> Epoch 938 finished \tANN training loss 0.053097\n",
      ">> Epoch 939 finished \tANN training loss 0.059150\n",
      ">> Epoch 940 finished \tANN training loss 0.047631\n",
      ">> Epoch 941 finished \tANN training loss 0.042032\n",
      ">> Epoch 942 finished \tANN training loss 0.043778\n",
      ">> Epoch 943 finished \tANN training loss 0.047955\n",
      ">> Epoch 944 finished \tANN training loss 0.049478\n",
      ">> Epoch 945 finished \tANN training loss 0.050035\n",
      ">> Epoch 946 finished \tANN training loss 0.053461\n",
      ">> Epoch 947 finished \tANN training loss 0.063671\n",
      ">> Epoch 948 finished \tANN training loss 0.054585\n",
      ">> Epoch 949 finished \tANN training loss 0.051259\n",
      ">> Epoch 950 finished \tANN training loss 0.044829\n",
      ">> Epoch 951 finished \tANN training loss 0.050890\n",
      ">> Epoch 952 finished \tANN training loss 0.048023\n",
      ">> Epoch 953 finished \tANN training loss 0.046379\n",
      ">> Epoch 954 finished \tANN training loss 0.053008\n",
      ">> Epoch 955 finished \tANN training loss 0.051619\n",
      ">> Epoch 956 finished \tANN training loss 0.051681\n",
      ">> Epoch 957 finished \tANN training loss 0.055035\n",
      ">> Epoch 958 finished \tANN training loss 0.051711\n",
      ">> Epoch 959 finished \tANN training loss 0.050197\n",
      ">> Epoch 960 finished \tANN training loss 0.047006\n",
      ">> Epoch 961 finished \tANN training loss 0.047658\n",
      ">> Epoch 962 finished \tANN training loss 0.048696\n",
      ">> Epoch 963 finished \tANN training loss 0.050098\n",
      ">> Epoch 964 finished \tANN training loss 0.052598\n",
      ">> Epoch 965 finished \tANN training loss 0.056842\n",
      ">> Epoch 966 finished \tANN training loss 0.050962\n",
      ">> Epoch 967 finished \tANN training loss 0.048351\n",
      ">> Epoch 968 finished \tANN training loss 0.046778\n",
      ">> Epoch 969 finished \tANN training loss 0.056930\n",
      ">> Epoch 970 finished \tANN training loss 0.054198\n",
      ">> Epoch 971 finished \tANN training loss 0.049894\n",
      ">> Epoch 972 finished \tANN training loss 0.084158\n",
      ">> Epoch 973 finished \tANN training loss 0.086767\n",
      ">> Epoch 974 finished \tANN training loss 0.054576\n",
      ">> Epoch 975 finished \tANN training loss 0.053991\n",
      ">> Epoch 976 finished \tANN training loss 0.057944\n",
      ">> Epoch 977 finished \tANN training loss 0.054894\n",
      ">> Epoch 978 finished \tANN training loss 0.058338\n",
      ">> Epoch 979 finished \tANN training loss 0.049799\n",
      ">> Epoch 980 finished \tANN training loss 0.049745\n",
      ">> Epoch 981 finished \tANN training loss 0.049158\n",
      ">> Epoch 982 finished \tANN training loss 0.049089\n",
      ">> Epoch 983 finished \tANN training loss 0.055162\n",
      ">> Epoch 984 finished \tANN training loss 0.053991\n",
      ">> Epoch 985 finished \tANN training loss 0.050737\n",
      ">> Epoch 986 finished \tANN training loss 0.051598\n",
      ">> Epoch 987 finished \tANN training loss 0.063154\n",
      ">> Epoch 988 finished \tANN training loss 0.059381\n",
      ">> Epoch 989 finished \tANN training loss 0.088743\n",
      ">> Epoch 990 finished \tANN training loss 0.068043\n",
      ">> Epoch 991 finished \tANN training loss 0.057250\n",
      ">> Epoch 992 finished \tANN training loss 0.056574\n",
      ">> Epoch 993 finished \tANN training loss 0.057765\n",
      ">> Epoch 994 finished \tANN training loss 0.056462\n",
      ">> Epoch 995 finished \tANN training loss 0.060167\n",
      ">> Epoch 996 finished \tANN training loss 0.052578\n",
      ">> Epoch 997 finished \tANN training loss 0.047485\n",
      ">> Epoch 998 finished \tANN training loss 0.048040\n",
      ">> Epoch 999 finished \tANN training loss 0.045626\n",
      "[END] Fine tuning step\n"
     ]
    }
   ],
   "source": [
    "oof_train2, oof_test2 = get_oof(dbn, x_train, y_train, x_test)\n",
    "train2 = Predict(oof_train2)\n",
    "test2 = Predict(oof_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "oof_train3 = np.zeros((x_train.shape[0], _N_CLASS))  \n",
    "oof_test3 = np.empty((x_test.shape[0], _N_CLASS))  \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train,y_train)):\n",
    "    kf_X_train = x_train[train_index]  \n",
    "    kf_X_train = kf_X_train.reshape(kf_X_train.shape[0],1,kf_X_train.shape[1])\n",
    "    kf_y_train = y_train[train_index] \n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = x_train[test_index] \n",
    "    kf_X_test = kf_X_test.reshape(kf_X_test.shape[0],1,kf_X_test.shape[1])\n",
    "\n",
    "    lstm.fit(kf_X_train, kf_y_train,epochs=1000,verbose=0,batch_size=20)  \n",
    "\n",
    "    oof_train3[test_index] = lstm.predict(kf_X_test) \n",
    "    oof_test3 += lstm.predict(x_test_lstm) \n",
    "\n",
    "oof_test3 /= _N_FOLDS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train = []\n",
    "new_train.append(oof_train1)\n",
    "new_train.append(oof_train2)\n",
    "new_train.append(oof_train3)\n",
    "len(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = []\n",
    "new_test.append(oof_test1)\n",
    "new_test.append(oof_test2)\n",
    "new_test.append(oof_test3)\n",
    "len(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = np.concatenate(new_train, axis=1)\n",
    "new_test = np.concatenate(new_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "new_test = np.array(new_test)\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for i in range(162):\n",
    "    for j in range(9):\n",
    "        if new_test[i,j] == np.inf:\n",
    "            count1 += 1\n",
    "            print(i,j)\n",
    "        if new_test[i,j] == np.NaN:\n",
    "            count2 += 1\n",
    "            print(i,j)       \n",
    "print(count1,count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking training ones score: 0.997716894977169\n",
      "stacking testing ones score: 0.9202127659574468\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 5, criterion=\"gini\",random_state=1)\n",
    "# clf = RandomForestRegressor()\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100, 100), random_state=1000, max_iter=1000)\n",
    "clf.fit(new_train, y_train_labels)\n",
    "RFR_pred_train = clf.predict(new_train)\n",
    "RFR_pred_train_label = Predict(RFR_pred_train)\n",
    "stock_train_acc = accuracy_score(y_train, RFR_pred_train_label) \n",
    "new_test1 = np.nan_to_num(new_test.astype(np.float32))\n",
    "RFR1_pred = clf.predict(new_test1)\n",
    "RFR1_pred_label = Predict(RFR1_pred)\n",
    "stock_test_acc = accuracy_score(y_test, RFR1_pred_label) \n",
    "print(\"stacking training ones score: {}\".format(stock_train_acc))\n",
    "print(\"stacking testing ones score: {}\".format(stock_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUSE_stacking = []\n",
    "PUSE_stacking.append(y_train)\n",
    "PUSE_stacking.append(RFR_pred_train_label)\n",
    "PUSE_stacking.append(y_test)\n",
    "PUSE_stacking.append(RFR1_pred_label)\n",
    "test = pd.DataFrame(data=PUSE_stacking,index = ['y_train','ytrain_pred','y_test','ytest_pred'])\n",
    "test.to_csv(r'./dataset/Stackinginging_combine_multi.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff  size=5 face=\"黑体\">方法2：stacking 堆叠</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 0s 24us/sample - loss: 1.2081e-06 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 72us/sample - loss: 1.1939e-06 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.1841e-06 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 46us/sample - loss: 1.1841e-06 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 1.1641e-06 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 46us/sample - loss: 1.1536e-06 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.1427e-06 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 1.1358e-06 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 43us/sample - loss: 1.1274e-06 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 1.1161e-06 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.1063e-06 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.1034e-06 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 1.0849e-06 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 1.0791e-06 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.0722e-06 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.0580e-06 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.0478e-06 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 76us/sample - loss: 1.0427e-06 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 52us/sample - loss: 1.0329e-06 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 1.0220e-06 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 1.0162e-06 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 1.0060e-06 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 43us/sample - loss: 9.9946e-07 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 43us/sample - loss: 9.8565e-07 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 9.7657e-07 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 9.7003e-07 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 9.6094e-07 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 9.5222e-07 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 9.4422e-07 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 43us/sample - loss: 9.3695e-07 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 43us/sample - loss: 9.2932e-07 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 9.1987e-07 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 9.1260e-07 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 9.0497e-07 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 8.9588e-07 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 8.9588e-07 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 8.8316e-07 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 8.7226e-07 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 8.6935e-07 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 8.5845e-07 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 8.4609e-07 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 8.4064e-07 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 8.3737e-07 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 8.2610e-07 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 8.1920e-07 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 8.1411e-07 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 8.0648e-07 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.9812e-07 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.8940e-07 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.8649e-07 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.7486e-07 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.7595e-07 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.5560e-07 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 37us/sample - loss: 7.5850e-07 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.4978e-07 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 40us/sample - loss: 7.4506e-07 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.3779e-07 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.2834e-07 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 7.2543e-07 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.1525e-07 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 7.1416e-07 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 30us/sample - loss: 7.0072e-07 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.9963e-07 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.9308e-07 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.8509e-07 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.8036e-07 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.7491e-07 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.6946e-07 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.5856e-07 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.5420e-07 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.4874e-07 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.4220e-07 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.3602e-07 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.3094e-07 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.2585e-07 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.2185e-07 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 6.1458e-07 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.0840e-07 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 6.0695e-07 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.9750e-07 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.9495e-07 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.8841e-07 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 34us/sample - loss: 5.8042e-07 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.7606e-07 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 5.7169e-07 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 5.6297e-07 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.6406e-07 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 34us/sample - loss: 5.5825e-07 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.5280e-07 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 5.4625e-07 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.4480e-07 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 5.4117e-07 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.3317e-07 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.2917e-07 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 5.2445e-07 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 5.1791e-07 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.1681e-07 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.1354e-07 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 36us/sample - loss: 5.0591e-07 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 33us/sample - loss: 5.0264e-07 - acc: 1.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_5_input to have shape (90,) but got array with shape (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-937247f8174b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0moof_train1_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf_X_test\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 当前验证集进行概率预测， 200 * _N_CLASS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0moof_test1_1\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 对测试集概率预测 oof_test_skf[i, :] ，  500 * _N_CLASS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0moof_test1_1\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0m_N_FOLDS\u001b[0m  \u001b[1;31m# 对每一则交叉验证的结果取平均\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[1;32m--> 699\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m    700\u001b[0m     return predict_loop(\n\u001b[0;32m    701\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2366\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2367\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2368\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2370\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    643\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    646\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_5_input to have shape (90,) but got array with shape (9,)"
     ]
    }
   ],
   "source": [
    "oof_train1_1 = np.zeros((new_train.shape[0], _N_CLASS))  #  _N_CLASS\n",
    "oof_test1_1 = np.empty((new_test1.shape[0], _N_CLASS))  #  _N_CLASS\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(new_train, y_train)):\n",
    "    kf_X_train = x_train[train_index] \n",
    "    kf_y_train = y_train[train_index]  \n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = x_train[test_index]\n",
    "\n",
    "    dnn.fit(kf_X_train, kf_y_train,batch_size=20, epochs=100, shuffle=True, verbose=1)  \n",
    "    \n",
    "    oof_train1_1[test_index] = dnn.predict(kf_X_test) \n",
    "    oof_test1_1 += dnn.predict(new_test1)  \n",
    "\n",
    "oof_test1_1 /= _N_FOLDS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.348823\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.913053\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.551523\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.347839\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.240535\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.176365\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.125811\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.091884\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.078211\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.062592\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.065050\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.055780\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.056126\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.051812\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.056613\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.052684\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.053891\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.059707\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.049383\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.053676\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2.355615\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.893442\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.717718\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1.530050\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 1.264867\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.966598\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.732974\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.553034\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.450036\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.376599\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.329569\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.305830\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.289245\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.263102\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.241177\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.215386\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.194802\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.171335\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.141021\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.109304\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.557310\n",
      ">> Epoch 1 finished \tANN training loss 0.276336\n",
      ">> Epoch 2 finished \tANN training loss 0.146071\n",
      ">> Epoch 3 finished \tANN training loss 0.080394\n",
      ">> Epoch 4 finished \tANN training loss 0.051485\n",
      ">> Epoch 5 finished \tANN training loss 0.034613\n",
      ">> Epoch 6 finished \tANN training loss 0.026751\n",
      ">> Epoch 7 finished \tANN training loss 0.020406\n",
      ">> Epoch 8 finished \tANN training loss 0.016619\n",
      ">> Epoch 9 finished \tANN training loss 0.013836\n",
      ">> Epoch 10 finished \tANN training loss 0.012733\n",
      ">> Epoch 11 finished \tANN training loss 0.010613\n",
      ">> Epoch 12 finished \tANN training loss 0.008704\n",
      ">> Epoch 13 finished \tANN training loss 0.007713\n",
      ">> Epoch 14 finished \tANN training loss 0.007543\n",
      ">> Epoch 15 finished \tANN training loss 0.006928\n",
      ">> Epoch 16 finished \tANN training loss 0.006440\n",
      ">> Epoch 17 finished \tANN training loss 0.006006\n",
      ">> Epoch 18 finished \tANN training loss 0.005731\n",
      ">> Epoch 19 finished \tANN training loss 0.005195\n",
      ">> Epoch 20 finished \tANN training loss 0.004733\n",
      ">> Epoch 21 finished \tANN training loss 0.004617\n",
      ">> Epoch 22 finished \tANN training loss 0.004950\n",
      ">> Epoch 23 finished \tANN training loss 0.004375\n",
      ">> Epoch 24 finished \tANN training loss 0.003915\n",
      ">> Epoch 25 finished \tANN training loss 0.003712\n",
      ">> Epoch 26 finished \tANN training loss 0.003647\n",
      ">> Epoch 27 finished \tANN training loss 0.003662\n",
      ">> Epoch 28 finished \tANN training loss 0.003682\n",
      ">> Epoch 29 finished \tANN training loss 0.003296\n",
      ">> Epoch 30 finished \tANN training loss 0.003229\n",
      ">> Epoch 31 finished \tANN training loss 0.003121\n",
      ">> Epoch 32 finished \tANN training loss 0.002922\n",
      ">> Epoch 33 finished \tANN training loss 0.002858\n",
      ">> Epoch 34 finished \tANN training loss 0.002655\n",
      ">> Epoch 35 finished \tANN training loss 0.002570\n",
      ">> Epoch 36 finished \tANN training loss 0.002412\n",
      ">> Epoch 37 finished \tANN training loss 0.002568\n",
      ">> Epoch 38 finished \tANN training loss 0.002458\n",
      ">> Epoch 39 finished \tANN training loss 0.003055\n",
      ">> Epoch 40 finished \tANN training loss 0.002917\n",
      ">> Epoch 41 finished \tANN training loss 0.002244\n",
      ">> Epoch 42 finished \tANN training loss 0.002019\n",
      ">> Epoch 43 finished \tANN training loss 0.002154\n",
      ">> Epoch 44 finished \tANN training loss 0.002468\n",
      ">> Epoch 45 finished \tANN training loss 0.003081\n",
      ">> Epoch 46 finished \tANN training loss 0.002953\n",
      ">> Epoch 47 finished \tANN training loss 0.002025\n",
      ">> Epoch 48 finished \tANN training loss 0.001962\n",
      ">> Epoch 49 finished \tANN training loss 0.001967\n",
      ">> Epoch 50 finished \tANN training loss 0.001945\n",
      ">> Epoch 51 finished \tANN training loss 0.002095\n",
      ">> Epoch 52 finished \tANN training loss 0.002192\n",
      ">> Epoch 53 finished \tANN training loss 0.002305\n",
      ">> Epoch 54 finished \tANN training loss 0.002308\n",
      ">> Epoch 55 finished \tANN training loss 0.002236\n",
      ">> Epoch 56 finished \tANN training loss 0.002229\n",
      ">> Epoch 57 finished \tANN training loss 0.002420\n",
      ">> Epoch 58 finished \tANN training loss 0.002354\n",
      ">> Epoch 59 finished \tANN training loss 0.001652\n",
      ">> Epoch 60 finished \tANN training loss 0.001649\n",
      ">> Epoch 61 finished \tANN training loss 0.001755\n",
      ">> Epoch 62 finished \tANN training loss 0.001617\n",
      ">> Epoch 63 finished \tANN training loss 0.001481\n",
      ">> Epoch 64 finished \tANN training loss 0.001590\n",
      ">> Epoch 65 finished \tANN training loss 0.001752\n",
      ">> Epoch 66 finished \tANN training loss 0.002110\n",
      ">> Epoch 67 finished \tANN training loss 0.001593\n",
      ">> Epoch 68 finished \tANN training loss 0.001569\n",
      ">> Epoch 69 finished \tANN training loss 0.001670\n",
      ">> Epoch 70 finished \tANN training loss 0.001970\n",
      ">> Epoch 71 finished \tANN training loss 0.001964\n",
      ">> Epoch 72 finished \tANN training loss 0.001970\n",
      ">> Epoch 73 finished \tANN training loss 0.001739\n",
      ">> Epoch 74 finished \tANN training loss 0.001575\n",
      ">> Epoch 75 finished \tANN training loss 0.001511\n",
      ">> Epoch 76 finished \tANN training loss 0.001516\n",
      ">> Epoch 77 finished \tANN training loss 0.002194\n",
      ">> Epoch 78 finished \tANN training loss 0.002023\n",
      ">> Epoch 79 finished \tANN training loss 0.001652\n",
      ">> Epoch 80 finished \tANN training loss 0.001245\n",
      ">> Epoch 81 finished \tANN training loss 0.001402\n",
      ">> Epoch 82 finished \tANN training loss 0.001510\n",
      ">> Epoch 83 finished \tANN training loss 0.001393\n",
      ">> Epoch 84 finished \tANN training loss 0.001560\n",
      ">> Epoch 85 finished \tANN training loss 0.001557\n",
      ">> Epoch 86 finished \tANN training loss 0.001512\n",
      ">> Epoch 87 finished \tANN training loss 0.001589\n",
      ">> Epoch 88 finished \tANN training loss 0.001820\n",
      ">> Epoch 89 finished \tANN training loss 0.001848\n",
      ">> Epoch 90 finished \tANN training loss 0.001216\n",
      ">> Epoch 91 finished \tANN training loss 0.001253\n",
      ">> Epoch 92 finished \tANN training loss 0.001314\n",
      ">> Epoch 93 finished \tANN training loss 0.001281\n",
      ">> Epoch 94 finished \tANN training loss 0.001269\n",
      ">> Epoch 95 finished \tANN training loss 0.001194\n",
      ">> Epoch 96 finished \tANN training loss 0.001119\n",
      ">> Epoch 97 finished \tANN training loss 0.001028\n",
      ">> Epoch 98 finished \tANN training loss 0.001057\n",
      ">> Epoch 99 finished \tANN training loss 0.001019\n",
      ">> Epoch 100 finished \tANN training loss 0.001006\n",
      ">> Epoch 101 finished \tANN training loss 0.001042\n",
      ">> Epoch 102 finished \tANN training loss 0.001321\n",
      ">> Epoch 103 finished \tANN training loss 0.001101\n",
      ">> Epoch 104 finished \tANN training loss 0.001040\n",
      ">> Epoch 105 finished \tANN training loss 0.000991\n",
      ">> Epoch 106 finished \tANN training loss 0.000928\n",
      ">> Epoch 107 finished \tANN training loss 0.000927\n",
      ">> Epoch 108 finished \tANN training loss 0.000851\n",
      ">> Epoch 109 finished \tANN training loss 0.000830\n",
      ">> Epoch 110 finished \tANN training loss 0.000977\n",
      ">> Epoch 111 finished \tANN training loss 0.000852\n",
      ">> Epoch 112 finished \tANN training loss 0.001303\n",
      ">> Epoch 113 finished \tANN training loss 0.001422\n",
      ">> Epoch 114 finished \tANN training loss 0.001263\n",
      ">> Epoch 115 finished \tANN training loss 0.001070\n",
      ">> Epoch 116 finished \tANN training loss 0.001064\n",
      ">> Epoch 117 finished \tANN training loss 0.001318\n",
      ">> Epoch 118 finished \tANN training loss 0.001325\n",
      ">> Epoch 119 finished \tANN training loss 0.001512\n",
      ">> Epoch 120 finished \tANN training loss 0.001062\n",
      ">> Epoch 121 finished \tANN training loss 0.000947\n",
      ">> Epoch 122 finished \tANN training loss 0.000842\n",
      ">> Epoch 123 finished \tANN training loss 0.001049\n",
      ">> Epoch 124 finished \tANN training loss 0.001031\n",
      ">> Epoch 125 finished \tANN training loss 0.001168\n",
      ">> Epoch 126 finished \tANN training loss 0.001094\n",
      ">> Epoch 127 finished \tANN training loss 0.001144\n",
      ">> Epoch 128 finished \tANN training loss 0.000977\n",
      ">> Epoch 129 finished \tANN training loss 0.001391\n",
      ">> Epoch 130 finished \tANN training loss 0.001298\n",
      ">> Epoch 131 finished \tANN training loss 0.000944\n",
      ">> Epoch 132 finished \tANN training loss 0.000972\n",
      ">> Epoch 133 finished \tANN training loss 0.000928\n",
      ">> Epoch 134 finished \tANN training loss 0.000860\n",
      ">> Epoch 135 finished \tANN training loss 0.000781\n",
      ">> Epoch 136 finished \tANN training loss 0.000768\n",
      ">> Epoch 137 finished \tANN training loss 0.000860\n",
      ">> Epoch 138 finished \tANN training loss 0.000784\n",
      ">> Epoch 139 finished \tANN training loss 0.000771\n",
      ">> Epoch 140 finished \tANN training loss 0.000810\n",
      ">> Epoch 141 finished \tANN training loss 0.000985\n",
      ">> Epoch 142 finished \tANN training loss 0.000851\n",
      ">> Epoch 143 finished \tANN training loss 0.000922\n",
      ">> Epoch 144 finished \tANN training loss 0.000846\n",
      ">> Epoch 145 finished \tANN training loss 0.000751\n",
      ">> Epoch 146 finished \tANN training loss 0.000720\n",
      ">> Epoch 147 finished \tANN training loss 0.000700\n",
      ">> Epoch 148 finished \tANN training loss 0.000721\n",
      ">> Epoch 149 finished \tANN training loss 0.000715\n",
      ">> Epoch 150 finished \tANN training loss 0.000761\n",
      ">> Epoch 151 finished \tANN training loss 0.000704\n",
      ">> Epoch 152 finished \tANN training loss 0.000779\n",
      ">> Epoch 153 finished \tANN training loss 0.000838\n",
      ">> Epoch 154 finished \tANN training loss 0.000800\n",
      ">> Epoch 155 finished \tANN training loss 0.000842\n",
      ">> Epoch 156 finished \tANN training loss 0.000789\n",
      ">> Epoch 157 finished \tANN training loss 0.000743\n",
      ">> Epoch 158 finished \tANN training loss 0.000780\n",
      ">> Epoch 159 finished \tANN training loss 0.000759\n",
      ">> Epoch 160 finished \tANN training loss 0.000637\n",
      ">> Epoch 161 finished \tANN training loss 0.000570\n",
      ">> Epoch 162 finished \tANN training loss 0.000615\n",
      ">> Epoch 163 finished \tANN training loss 0.000560\n",
      ">> Epoch 164 finished \tANN training loss 0.000652\n",
      ">> Epoch 165 finished \tANN training loss 0.000598\n",
      ">> Epoch 166 finished \tANN training loss 0.000588\n",
      ">> Epoch 167 finished \tANN training loss 0.000667\n",
      ">> Epoch 168 finished \tANN training loss 0.000741\n",
      ">> Epoch 169 finished \tANN training loss 0.000643\n",
      ">> Epoch 170 finished \tANN training loss 0.000582\n",
      ">> Epoch 171 finished \tANN training loss 0.000642\n",
      ">> Epoch 172 finished \tANN training loss 0.000620\n",
      ">> Epoch 173 finished \tANN training loss 0.000760\n",
      ">> Epoch 174 finished \tANN training loss 0.000643\n",
      ">> Epoch 175 finished \tANN training loss 0.000704\n",
      ">> Epoch 176 finished \tANN training loss 0.000962\n",
      ">> Epoch 177 finished \tANN training loss 0.000813\n",
      ">> Epoch 178 finished \tANN training loss 0.000676\n",
      ">> Epoch 179 finished \tANN training loss 0.000667\n",
      ">> Epoch 180 finished \tANN training loss 0.000870\n",
      ">> Epoch 181 finished \tANN training loss 0.000836\n",
      ">> Epoch 182 finished \tANN training loss 0.000849\n",
      ">> Epoch 183 finished \tANN training loss 0.000706\n",
      ">> Epoch 184 finished \tANN training loss 0.000597\n",
      ">> Epoch 185 finished \tANN training loss 0.000609\n",
      ">> Epoch 186 finished \tANN training loss 0.000607\n",
      ">> Epoch 187 finished \tANN training loss 0.000644\n",
      ">> Epoch 188 finished \tANN training loss 0.000612\n",
      ">> Epoch 189 finished \tANN training loss 0.000515\n",
      ">> Epoch 190 finished \tANN training loss 0.000505\n",
      ">> Epoch 191 finished \tANN training loss 0.000483\n",
      ">> Epoch 192 finished \tANN training loss 0.000460\n",
      ">> Epoch 193 finished \tANN training loss 0.000401\n",
      ">> Epoch 194 finished \tANN training loss 0.000405\n",
      ">> Epoch 195 finished \tANN training loss 0.000455\n",
      ">> Epoch 196 finished \tANN training loss 0.000414\n",
      ">> Epoch 197 finished \tANN training loss 0.000398\n",
      ">> Epoch 198 finished \tANN training loss 0.000382\n",
      ">> Epoch 199 finished \tANN training loss 0.000393\n",
      ">> Epoch 200 finished \tANN training loss 0.000390\n",
      ">> Epoch 201 finished \tANN training loss 0.000579\n",
      ">> Epoch 202 finished \tANN training loss 0.000605\n",
      ">> Epoch 203 finished \tANN training loss 0.000608\n",
      ">> Epoch 204 finished \tANN training loss 0.000581\n",
      ">> Epoch 205 finished \tANN training loss 0.000655\n",
      ">> Epoch 206 finished \tANN training loss 0.000658\n",
      ">> Epoch 207 finished \tANN training loss 0.000549\n",
      ">> Epoch 208 finished \tANN training loss 0.000549\n",
      ">> Epoch 209 finished \tANN training loss 0.000505\n",
      ">> Epoch 210 finished \tANN training loss 0.000509\n",
      ">> Epoch 211 finished \tANN training loss 0.000477\n",
      ">> Epoch 212 finished \tANN training loss 0.000467\n",
      ">> Epoch 213 finished \tANN training loss 0.000480\n",
      ">> Epoch 214 finished \tANN training loss 0.000426\n",
      ">> Epoch 215 finished \tANN training loss 0.000410\n",
      ">> Epoch 216 finished \tANN training loss 0.000415\n",
      ">> Epoch 217 finished \tANN training loss 0.000419\n",
      ">> Epoch 218 finished \tANN training loss 0.000433\n",
      ">> Epoch 219 finished \tANN training loss 0.000523\n",
      ">> Epoch 220 finished \tANN training loss 0.000577\n",
      ">> Epoch 221 finished \tANN training loss 0.000475\n",
      ">> Epoch 222 finished \tANN training loss 0.000484\n",
      ">> Epoch 223 finished \tANN training loss 0.000436\n",
      ">> Epoch 224 finished \tANN training loss 0.000421\n",
      ">> Epoch 225 finished \tANN training loss 0.000436\n",
      ">> Epoch 226 finished \tANN training loss 0.000436\n",
      ">> Epoch 227 finished \tANN training loss 0.000432\n",
      ">> Epoch 228 finished \tANN training loss 0.000442\n",
      ">> Epoch 229 finished \tANN training loss 0.000428\n",
      ">> Epoch 230 finished \tANN training loss 0.000428\n",
      ">> Epoch 231 finished \tANN training loss 0.000442\n",
      ">> Epoch 232 finished \tANN training loss 0.000413\n",
      ">> Epoch 233 finished \tANN training loss 0.000429\n",
      ">> Epoch 234 finished \tANN training loss 0.000505\n",
      ">> Epoch 235 finished \tANN training loss 0.000516\n",
      ">> Epoch 236 finished \tANN training loss 0.000482\n",
      ">> Epoch 237 finished \tANN training loss 0.000423\n",
      ">> Epoch 238 finished \tANN training loss 0.000386\n",
      ">> Epoch 239 finished \tANN training loss 0.000395\n",
      ">> Epoch 240 finished \tANN training loss 0.000416\n",
      ">> Epoch 241 finished \tANN training loss 0.000404\n",
      ">> Epoch 242 finished \tANN training loss 0.000384\n",
      ">> Epoch 243 finished \tANN training loss 0.000386\n",
      ">> Epoch 244 finished \tANN training loss 0.000364\n",
      ">> Epoch 245 finished \tANN training loss 0.000355\n",
      ">> Epoch 246 finished \tANN training loss 0.000338\n",
      ">> Epoch 247 finished \tANN training loss 0.000332\n",
      ">> Epoch 248 finished \tANN training loss 0.000323\n",
      ">> Epoch 249 finished \tANN training loss 0.000386\n",
      ">> Epoch 250 finished \tANN training loss 0.000325\n",
      ">> Epoch 251 finished \tANN training loss 0.000333\n",
      ">> Epoch 252 finished \tANN training loss 0.000338\n",
      ">> Epoch 253 finished \tANN training loss 0.000326\n",
      ">> Epoch 254 finished \tANN training loss 0.000317\n",
      ">> Epoch 255 finished \tANN training loss 0.000310\n",
      ">> Epoch 256 finished \tANN training loss 0.000382\n",
      ">> Epoch 257 finished \tANN training loss 0.000359\n",
      ">> Epoch 258 finished \tANN training loss 0.000342\n",
      ">> Epoch 259 finished \tANN training loss 0.000353\n",
      ">> Epoch 260 finished \tANN training loss 0.000345\n",
      ">> Epoch 261 finished \tANN training loss 0.000375\n",
      ">> Epoch 262 finished \tANN training loss 0.000459\n",
      ">> Epoch 263 finished \tANN training loss 0.000438\n",
      ">> Epoch 264 finished \tANN training loss 0.000553\n",
      ">> Epoch 265 finished \tANN training loss 0.000547\n",
      ">> Epoch 266 finished \tANN training loss 0.000486\n",
      ">> Epoch 267 finished \tANN training loss 0.000426\n",
      ">> Epoch 268 finished \tANN training loss 0.000413\n",
      ">> Epoch 269 finished \tANN training loss 0.000371\n",
      ">> Epoch 270 finished \tANN training loss 0.000396\n",
      ">> Epoch 271 finished \tANN training loss 0.000371\n",
      ">> Epoch 272 finished \tANN training loss 0.000376\n",
      ">> Epoch 273 finished \tANN training loss 0.000379\n",
      ">> Epoch 274 finished \tANN training loss 0.000374\n",
      ">> Epoch 275 finished \tANN training loss 0.000369\n",
      ">> Epoch 276 finished \tANN training loss 0.000358\n",
      ">> Epoch 277 finished \tANN training loss 0.000382\n",
      ">> Epoch 278 finished \tANN training loss 0.000395\n",
      ">> Epoch 279 finished \tANN training loss 0.000463\n",
      ">> Epoch 280 finished \tANN training loss 0.000383\n",
      ">> Epoch 281 finished \tANN training loss 0.000372\n",
      ">> Epoch 282 finished \tANN training loss 0.000349\n",
      ">> Epoch 283 finished \tANN training loss 0.000358\n",
      ">> Epoch 284 finished \tANN training loss 0.000366\n",
      ">> Epoch 285 finished \tANN training loss 0.000366\n",
      ">> Epoch 286 finished \tANN training loss 0.000334\n",
      ">> Epoch 287 finished \tANN training loss 0.000323\n",
      ">> Epoch 288 finished \tANN training loss 0.000328\n",
      ">> Epoch 289 finished \tANN training loss 0.000324\n",
      ">> Epoch 290 finished \tANN training loss 0.000304\n",
      ">> Epoch 291 finished \tANN training loss 0.000311\n",
      ">> Epoch 292 finished \tANN training loss 0.000319\n",
      ">> Epoch 293 finished \tANN training loss 0.000294\n",
      ">> Epoch 294 finished \tANN training loss 0.000289\n",
      ">> Epoch 295 finished \tANN training loss 0.000322\n",
      ">> Epoch 296 finished \tANN training loss 0.000321\n",
      ">> Epoch 297 finished \tANN training loss 0.000336\n",
      ">> Epoch 298 finished \tANN training loss 0.000462\n",
      ">> Epoch 299 finished \tANN training loss 0.000438\n",
      ">> Epoch 300 finished \tANN training loss 0.000360\n",
      ">> Epoch 301 finished \tANN training loss 0.000371\n",
      ">> Epoch 302 finished \tANN training loss 0.000405\n",
      ">> Epoch 303 finished \tANN training loss 0.000377\n",
      ">> Epoch 304 finished \tANN training loss 0.000374\n",
      ">> Epoch 305 finished \tANN training loss 0.000335\n",
      ">> Epoch 306 finished \tANN training loss 0.000342\n",
      ">> Epoch 307 finished \tANN training loss 0.000361\n",
      ">> Epoch 308 finished \tANN training loss 0.000326\n",
      ">> Epoch 309 finished \tANN training loss 0.000344\n",
      ">> Epoch 310 finished \tANN training loss 0.000289\n",
      ">> Epoch 311 finished \tANN training loss 0.000312\n",
      ">> Epoch 312 finished \tANN training loss 0.000278\n",
      ">> Epoch 313 finished \tANN training loss 0.000278\n",
      ">> Epoch 314 finished \tANN training loss 0.000286\n",
      ">> Epoch 315 finished \tANN training loss 0.000275\n",
      ">> Epoch 316 finished \tANN training loss 0.000270\n",
      ">> Epoch 317 finished \tANN training loss 0.000270\n",
      ">> Epoch 318 finished \tANN training loss 0.000300\n",
      ">> Epoch 319 finished \tANN training loss 0.000286\n",
      ">> Epoch 320 finished \tANN training loss 0.000288\n",
      ">> Epoch 321 finished \tANN training loss 0.000270\n",
      ">> Epoch 322 finished \tANN training loss 0.000272\n",
      ">> Epoch 323 finished \tANN training loss 0.000256\n",
      ">> Epoch 324 finished \tANN training loss 0.000272\n",
      ">> Epoch 325 finished \tANN training loss 0.000292\n",
      ">> Epoch 326 finished \tANN training loss 0.000353\n",
      ">> Epoch 327 finished \tANN training loss 0.000276\n",
      ">> Epoch 328 finished \tANN training loss 0.000274\n",
      ">> Epoch 329 finished \tANN training loss 0.000299\n",
      ">> Epoch 330 finished \tANN training loss 0.000267\n",
      ">> Epoch 331 finished \tANN training loss 0.000262\n",
      ">> Epoch 332 finished \tANN training loss 0.000273\n",
      ">> Epoch 333 finished \tANN training loss 0.000288\n",
      ">> Epoch 334 finished \tANN training loss 0.000294\n",
      ">> Epoch 335 finished \tANN training loss 0.000334\n",
      ">> Epoch 336 finished \tANN training loss 0.000333\n",
      ">> Epoch 337 finished \tANN training loss 0.000323\n",
      ">> Epoch 338 finished \tANN training loss 0.000282\n",
      ">> Epoch 339 finished \tANN training loss 0.000315\n",
      ">> Epoch 340 finished \tANN training loss 0.000330\n",
      ">> Epoch 341 finished \tANN training loss 0.000400\n",
      ">> Epoch 342 finished \tANN training loss 0.000406\n",
      ">> Epoch 343 finished \tANN training loss 0.000438\n",
      ">> Epoch 344 finished \tANN training loss 0.000448\n",
      ">> Epoch 345 finished \tANN training loss 0.000326\n",
      ">> Epoch 346 finished \tANN training loss 0.000334\n",
      ">> Epoch 347 finished \tANN training loss 0.000299\n",
      ">> Epoch 348 finished \tANN training loss 0.000297\n",
      ">> Epoch 349 finished \tANN training loss 0.000308\n",
      ">> Epoch 350 finished \tANN training loss 0.000302\n",
      ">> Epoch 351 finished \tANN training loss 0.000303\n",
      ">> Epoch 352 finished \tANN training loss 0.000288\n",
      ">> Epoch 353 finished \tANN training loss 0.000295\n",
      ">> Epoch 354 finished \tANN training loss 0.000299\n",
      ">> Epoch 355 finished \tANN training loss 0.000299\n",
      ">> Epoch 356 finished \tANN training loss 0.000284\n",
      ">> Epoch 357 finished \tANN training loss 0.000287\n",
      ">> Epoch 358 finished \tANN training loss 0.000280\n",
      ">> Epoch 359 finished \tANN training loss 0.000273\n",
      ">> Epoch 360 finished \tANN training loss 0.000278\n",
      ">> Epoch 361 finished \tANN training loss 0.000293\n",
      ">> Epoch 362 finished \tANN training loss 0.000310\n",
      ">> Epoch 363 finished \tANN training loss 0.000340\n",
      ">> Epoch 364 finished \tANN training loss 0.000428\n",
      ">> Epoch 365 finished \tANN training loss 0.000319\n",
      ">> Epoch 366 finished \tANN training loss 0.000305\n",
      ">> Epoch 367 finished \tANN training loss 0.000286\n",
      ">> Epoch 368 finished \tANN training loss 0.000296\n",
      ">> Epoch 369 finished \tANN training loss 0.000325\n",
      ">> Epoch 370 finished \tANN training loss 0.000292\n",
      ">> Epoch 371 finished \tANN training loss 0.000304\n",
      ">> Epoch 372 finished \tANN training loss 0.000365\n",
      ">> Epoch 373 finished \tANN training loss 0.000339\n",
      ">> Epoch 374 finished \tANN training loss 0.000397\n",
      ">> Epoch 375 finished \tANN training loss 0.000403\n",
      ">> Epoch 376 finished \tANN training loss 0.000344\n",
      ">> Epoch 377 finished \tANN training loss 0.000365\n",
      ">> Epoch 378 finished \tANN training loss 0.000325\n",
      ">> Epoch 379 finished \tANN training loss 0.000331\n",
      ">> Epoch 380 finished \tANN training loss 0.000311\n",
      ">> Epoch 381 finished \tANN training loss 0.000299\n",
      ">> Epoch 382 finished \tANN training loss 0.000303\n",
      ">> Epoch 383 finished \tANN training loss 0.000292\n",
      ">> Epoch 384 finished \tANN training loss 0.000329\n",
      ">> Epoch 385 finished \tANN training loss 0.000303\n",
      ">> Epoch 386 finished \tANN training loss 0.000315\n",
      ">> Epoch 387 finished \tANN training loss 0.000325\n",
      ">> Epoch 388 finished \tANN training loss 0.000310\n",
      ">> Epoch 389 finished \tANN training loss 0.000346\n",
      ">> Epoch 390 finished \tANN training loss 0.000333\n",
      ">> Epoch 391 finished \tANN training loss 0.000335\n",
      ">> Epoch 392 finished \tANN training loss 0.000327\n",
      ">> Epoch 393 finished \tANN training loss 0.000338\n",
      ">> Epoch 394 finished \tANN training loss 0.000305\n",
      ">> Epoch 395 finished \tANN training loss 0.000298\n",
      ">> Epoch 396 finished \tANN training loss 0.000287\n",
      ">> Epoch 397 finished \tANN training loss 0.000294\n",
      ">> Epoch 398 finished \tANN training loss 0.000284\n",
      ">> Epoch 399 finished \tANN training loss 0.000280\n",
      ">> Epoch 400 finished \tANN training loss 0.000313\n",
      ">> Epoch 401 finished \tANN training loss 0.000279\n",
      ">> Epoch 402 finished \tANN training loss 0.000299\n",
      ">> Epoch 403 finished \tANN training loss 0.000285\n",
      ">> Epoch 404 finished \tANN training loss 0.000259\n",
      ">> Epoch 405 finished \tANN training loss 0.000268\n",
      ">> Epoch 406 finished \tANN training loss 0.000249\n",
      ">> Epoch 407 finished \tANN training loss 0.000247\n",
      ">> Epoch 408 finished \tANN training loss 0.000241\n",
      ">> Epoch 409 finished \tANN training loss 0.000228\n",
      ">> Epoch 410 finished \tANN training loss 0.000239\n",
      ">> Epoch 411 finished \tANN training loss 0.000254\n",
      ">> Epoch 412 finished \tANN training loss 0.000244\n",
      ">> Epoch 413 finished \tANN training loss 0.000233\n",
      ">> Epoch 414 finished \tANN training loss 0.000226\n",
      ">> Epoch 415 finished \tANN training loss 0.000228\n",
      ">> Epoch 416 finished \tANN training loss 0.000226\n",
      ">> Epoch 417 finished \tANN training loss 0.000215\n",
      ">> Epoch 418 finished \tANN training loss 0.000219\n",
      ">> Epoch 419 finished \tANN training loss 0.000215\n",
      ">> Epoch 420 finished \tANN training loss 0.000197\n",
      ">> Epoch 421 finished \tANN training loss 0.000194\n",
      ">> Epoch 422 finished \tANN training loss 0.000205\n",
      ">> Epoch 423 finished \tANN training loss 0.000207\n",
      ">> Epoch 424 finished \tANN training loss 0.000194\n",
      ">> Epoch 425 finished \tANN training loss 0.000190\n",
      ">> Epoch 426 finished \tANN training loss 0.000186\n",
      ">> Epoch 427 finished \tANN training loss 0.000187\n",
      ">> Epoch 428 finished \tANN training loss 0.000192\n",
      ">> Epoch 429 finished \tANN training loss 0.000195\n",
      ">> Epoch 430 finished \tANN training loss 0.000206\n",
      ">> Epoch 431 finished \tANN training loss 0.000193\n",
      ">> Epoch 432 finished \tANN training loss 0.000202\n",
      ">> Epoch 433 finished \tANN training loss 0.000201\n",
      ">> Epoch 434 finished \tANN training loss 0.000186\n",
      ">> Epoch 435 finished \tANN training loss 0.000181\n",
      ">> Epoch 436 finished \tANN training loss 0.000187\n",
      ">> Epoch 437 finished \tANN training loss 0.000194\n",
      ">> Epoch 438 finished \tANN training loss 0.000201\n",
      ">> Epoch 439 finished \tANN training loss 0.000215\n",
      ">> Epoch 440 finished \tANN training loss 0.000232\n",
      ">> Epoch 441 finished \tANN training loss 0.000264\n",
      ">> Epoch 442 finished \tANN training loss 0.000249\n",
      ">> Epoch 443 finished \tANN training loss 0.000191\n",
      ">> Epoch 444 finished \tANN training loss 0.000206\n",
      ">> Epoch 445 finished \tANN training loss 0.000223\n",
      ">> Epoch 446 finished \tANN training loss 0.000233\n",
      ">> Epoch 447 finished \tANN training loss 0.000218\n",
      ">> Epoch 448 finished \tANN training loss 0.000227\n",
      ">> Epoch 449 finished \tANN training loss 0.000228\n",
      ">> Epoch 450 finished \tANN training loss 0.000211\n",
      ">> Epoch 451 finished \tANN training loss 0.000204\n",
      ">> Epoch 452 finished \tANN training loss 0.000203\n",
      ">> Epoch 453 finished \tANN training loss 0.000200\n",
      ">> Epoch 454 finished \tANN training loss 0.000200\n",
      ">> Epoch 455 finished \tANN training loss 0.000203\n",
      ">> Epoch 456 finished \tANN training loss 0.000247\n",
      ">> Epoch 457 finished \tANN training loss 0.000217\n",
      ">> Epoch 458 finished \tANN training loss 0.000231\n",
      ">> Epoch 459 finished \tANN training loss 0.000223\n",
      ">> Epoch 460 finished \tANN training loss 0.000210\n",
      ">> Epoch 461 finished \tANN training loss 0.000192\n",
      ">> Epoch 462 finished \tANN training loss 0.000189\n",
      ">> Epoch 463 finished \tANN training loss 0.000196\n",
      ">> Epoch 464 finished \tANN training loss 0.000213\n",
      ">> Epoch 465 finished \tANN training loss 0.000269\n",
      ">> Epoch 466 finished \tANN training loss 0.000233\n",
      ">> Epoch 467 finished \tANN training loss 0.000195\n",
      ">> Epoch 468 finished \tANN training loss 0.000200\n",
      ">> Epoch 469 finished \tANN training loss 0.000194\n",
      ">> Epoch 470 finished \tANN training loss 0.000194\n",
      ">> Epoch 471 finished \tANN training loss 0.000186\n",
      ">> Epoch 472 finished \tANN training loss 0.000182\n",
      ">> Epoch 473 finished \tANN training loss 0.000213\n",
      ">> Epoch 474 finished \tANN training loss 0.000228\n",
      ">> Epoch 475 finished \tANN training loss 0.000200\n",
      ">> Epoch 476 finished \tANN training loss 0.000192\n",
      ">> Epoch 477 finished \tANN training loss 0.000184\n",
      ">> Epoch 478 finished \tANN training loss 0.000184\n",
      ">> Epoch 479 finished \tANN training loss 0.000191\n",
      ">> Epoch 480 finished \tANN training loss 0.000189\n",
      ">> Epoch 481 finished \tANN training loss 0.000195\n",
      ">> Epoch 482 finished \tANN training loss 0.000226\n",
      ">> Epoch 483 finished \tANN training loss 0.000206\n",
      ">> Epoch 484 finished \tANN training loss 0.000219\n",
      ">> Epoch 485 finished \tANN training loss 0.000246\n",
      ">> Epoch 486 finished \tANN training loss 0.000224\n",
      ">> Epoch 487 finished \tANN training loss 0.000215\n",
      ">> Epoch 488 finished \tANN training loss 0.000195\n",
      ">> Epoch 489 finished \tANN training loss 0.000191\n",
      ">> Epoch 490 finished \tANN training loss 0.000187\n",
      ">> Epoch 491 finished \tANN training loss 0.000183\n",
      ">> Epoch 492 finished \tANN training loss 0.000183\n",
      ">> Epoch 493 finished \tANN training loss 0.000199\n",
      ">> Epoch 494 finished \tANN training loss 0.000216\n",
      ">> Epoch 495 finished \tANN training loss 0.000210\n",
      ">> Epoch 496 finished \tANN training loss 0.000195\n",
      ">> Epoch 497 finished \tANN training loss 0.000204\n",
      ">> Epoch 498 finished \tANN training loss 0.000194\n",
      ">> Epoch 499 finished \tANN training loss 0.000199\n",
      ">> Epoch 500 finished \tANN training loss 0.000187\n",
      ">> Epoch 501 finished \tANN training loss 0.000203\n",
      ">> Epoch 502 finished \tANN training loss 0.000195\n",
      ">> Epoch 503 finished \tANN training loss 0.000187\n",
      ">> Epoch 504 finished \tANN training loss 0.000183\n",
      ">> Epoch 505 finished \tANN training loss 0.000172\n",
      ">> Epoch 506 finished \tANN training loss 0.000200\n",
      ">> Epoch 507 finished \tANN training loss 0.000216\n",
      ">> Epoch 508 finished \tANN training loss 0.000198\n",
      ">> Epoch 509 finished \tANN training loss 0.000185\n",
      ">> Epoch 510 finished \tANN training loss 0.000198\n",
      ">> Epoch 511 finished \tANN training loss 0.000203\n",
      ">> Epoch 512 finished \tANN training loss 0.000224\n",
      ">> Epoch 513 finished \tANN training loss 0.000211\n",
      ">> Epoch 514 finished \tANN training loss 0.000207\n",
      ">> Epoch 515 finished \tANN training loss 0.000196\n",
      ">> Epoch 516 finished \tANN training loss 0.000240\n",
      ">> Epoch 517 finished \tANN training loss 0.000249\n",
      ">> Epoch 518 finished \tANN training loss 0.000358\n",
      ">> Epoch 519 finished \tANN training loss 0.000256\n",
      ">> Epoch 520 finished \tANN training loss 0.000300\n",
      ">> Epoch 521 finished \tANN training loss 0.000299\n",
      ">> Epoch 522 finished \tANN training loss 0.000287\n",
      ">> Epoch 523 finished \tANN training loss 0.000230\n",
      ">> Epoch 524 finished \tANN training loss 0.000216\n",
      ">> Epoch 525 finished \tANN training loss 0.000217\n",
      ">> Epoch 526 finished \tANN training loss 0.000205\n",
      ">> Epoch 527 finished \tANN training loss 0.000198\n",
      ">> Epoch 528 finished \tANN training loss 0.000198\n",
      ">> Epoch 529 finished \tANN training loss 0.000214\n",
      ">> Epoch 530 finished \tANN training loss 0.000220\n",
      ">> Epoch 531 finished \tANN training loss 0.000222\n",
      ">> Epoch 532 finished \tANN training loss 0.000208\n",
      ">> Epoch 533 finished \tANN training loss 0.000203\n",
      ">> Epoch 534 finished \tANN training loss 0.000222\n",
      ">> Epoch 535 finished \tANN training loss 0.000223\n",
      ">> Epoch 536 finished \tANN training loss 0.000230\n",
      ">> Epoch 537 finished \tANN training loss 0.000231\n",
      ">> Epoch 538 finished \tANN training loss 0.000226\n",
      ">> Epoch 539 finished \tANN training loss 0.000215\n",
      ">> Epoch 540 finished \tANN training loss 0.000211\n",
      ">> Epoch 541 finished \tANN training loss 0.000202\n",
      ">> Epoch 542 finished \tANN training loss 0.000195\n",
      ">> Epoch 543 finished \tANN training loss 0.000198\n",
      ">> Epoch 544 finished \tANN training loss 0.000193\n",
      ">> Epoch 545 finished \tANN training loss 0.000198\n",
      ">> Epoch 546 finished \tANN training loss 0.000206\n",
      ">> Epoch 547 finished \tANN training loss 0.000212\n",
      ">> Epoch 548 finished \tANN training loss 0.000191\n",
      ">> Epoch 549 finished \tANN training loss 0.000198\n",
      ">> Epoch 550 finished \tANN training loss 0.000215\n",
      ">> Epoch 551 finished \tANN training loss 0.000190\n",
      ">> Epoch 552 finished \tANN training loss 0.000198\n",
      ">> Epoch 553 finished \tANN training loss 0.000207\n",
      ">> Epoch 554 finished \tANN training loss 0.000209\n",
      ">> Epoch 555 finished \tANN training loss 0.000198\n",
      ">> Epoch 556 finished \tANN training loss 0.000199\n",
      ">> Epoch 557 finished \tANN training loss 0.000196\n",
      ">> Epoch 558 finished \tANN training loss 0.000217\n",
      ">> Epoch 559 finished \tANN training loss 0.000206\n",
      ">> Epoch 560 finished \tANN training loss 0.000207\n",
      ">> Epoch 561 finished \tANN training loss 0.000215\n",
      ">> Epoch 562 finished \tANN training loss 0.000237\n",
      ">> Epoch 563 finished \tANN training loss 0.000214\n",
      ">> Epoch 564 finished \tANN training loss 0.000210\n",
      ">> Epoch 565 finished \tANN training loss 0.000211\n",
      ">> Epoch 566 finished \tANN training loss 0.000211\n",
      ">> Epoch 567 finished \tANN training loss 0.000200\n",
      ">> Epoch 568 finished \tANN training loss 0.000204\n",
      ">> Epoch 569 finished \tANN training loss 0.000206\n",
      ">> Epoch 570 finished \tANN training loss 0.000201\n",
      ">> Epoch 571 finished \tANN training loss 0.000205\n",
      ">> Epoch 572 finished \tANN training loss 0.000191\n",
      ">> Epoch 573 finished \tANN training loss 0.000195\n",
      ">> Epoch 574 finished \tANN training loss 0.000239\n",
      ">> Epoch 575 finished \tANN training loss 0.000239\n",
      ">> Epoch 576 finished \tANN training loss 0.000237\n",
      ">> Epoch 577 finished \tANN training loss 0.000236\n",
      ">> Epoch 578 finished \tANN training loss 0.000276\n",
      ">> Epoch 579 finished \tANN training loss 0.000257\n",
      ">> Epoch 580 finished \tANN training loss 0.000242\n",
      ">> Epoch 581 finished \tANN training loss 0.000201\n",
      ">> Epoch 582 finished \tANN training loss 0.000239\n",
      ">> Epoch 583 finished \tANN training loss 0.000216\n",
      ">> Epoch 584 finished \tANN training loss 0.000210\n",
      ">> Epoch 585 finished \tANN training loss 0.000198\n",
      ">> Epoch 586 finished \tANN training loss 0.000188\n",
      ">> Epoch 587 finished \tANN training loss 0.000182\n",
      ">> Epoch 588 finished \tANN training loss 0.000171\n",
      ">> Epoch 589 finished \tANN training loss 0.000170\n",
      ">> Epoch 590 finished \tANN training loss 0.000157\n",
      ">> Epoch 591 finished \tANN training loss 0.000163\n",
      ">> Epoch 592 finished \tANN training loss 0.000183\n",
      ">> Epoch 593 finished \tANN training loss 0.000219\n",
      ">> Epoch 594 finished \tANN training loss 0.000217\n",
      ">> Epoch 595 finished \tANN training loss 0.000201\n",
      ">> Epoch 596 finished \tANN training loss 0.000177\n",
      ">> Epoch 597 finished \tANN training loss 0.000185\n",
      ">> Epoch 598 finished \tANN training loss 0.000183\n",
      ">> Epoch 599 finished \tANN training loss 0.000171\n",
      ">> Epoch 600 finished \tANN training loss 0.000172\n",
      ">> Epoch 601 finished \tANN training loss 0.000172\n",
      ">> Epoch 602 finished \tANN training loss 0.000188\n",
      ">> Epoch 603 finished \tANN training loss 0.000177\n",
      ">> Epoch 604 finished \tANN training loss 0.000174\n",
      ">> Epoch 605 finished \tANN training loss 0.000193\n",
      ">> Epoch 606 finished \tANN training loss 0.000210\n",
      ">> Epoch 607 finished \tANN training loss 0.000180\n",
      ">> Epoch 608 finished \tANN training loss 0.000222\n",
      ">> Epoch 609 finished \tANN training loss 0.000224\n",
      ">> Epoch 610 finished \tANN training loss 0.000231\n",
      ">> Epoch 611 finished \tANN training loss 0.000212\n",
      ">> Epoch 612 finished \tANN training loss 0.000213\n",
      ">> Epoch 613 finished \tANN training loss 0.000218\n",
      ">> Epoch 614 finished \tANN training loss 0.000206\n",
      ">> Epoch 615 finished \tANN training loss 0.000203\n",
      ">> Epoch 616 finished \tANN training loss 0.000195\n",
      ">> Epoch 617 finished \tANN training loss 0.000195\n",
      ">> Epoch 618 finished \tANN training loss 0.000199\n",
      ">> Epoch 619 finished \tANN training loss 0.000277\n",
      ">> Epoch 620 finished \tANN training loss 0.000279\n",
      ">> Epoch 621 finished \tANN training loss 0.000238\n",
      ">> Epoch 622 finished \tANN training loss 0.000247\n",
      ">> Epoch 623 finished \tANN training loss 0.000222\n",
      ">> Epoch 624 finished \tANN training loss 0.000254\n",
      ">> Epoch 625 finished \tANN training loss 0.000208\n",
      ">> Epoch 626 finished \tANN training loss 0.000227\n",
      ">> Epoch 627 finished \tANN training loss 0.000475\n",
      ">> Epoch 628 finished \tANN training loss 0.000385\n",
      ">> Epoch 629 finished \tANN training loss 0.000280\n",
      ">> Epoch 630 finished \tANN training loss 0.000253\n",
      ">> Epoch 631 finished \tANN training loss 0.000235\n",
      ">> Epoch 632 finished \tANN training loss 0.000221\n",
      ">> Epoch 633 finished \tANN training loss 0.000195\n",
      ">> Epoch 634 finished \tANN training loss 0.000205\n",
      ">> Epoch 635 finished \tANN training loss 0.000214\n",
      ">> Epoch 636 finished \tANN training loss 0.000201\n",
      ">> Epoch 637 finished \tANN training loss 0.000202\n",
      ">> Epoch 638 finished \tANN training loss 0.000198\n",
      ">> Epoch 639 finished \tANN training loss 0.000189\n",
      ">> Epoch 640 finished \tANN training loss 0.000178\n",
      ">> Epoch 641 finished \tANN training loss 0.000182\n",
      ">> Epoch 642 finished \tANN training loss 0.000177\n",
      ">> Epoch 643 finished \tANN training loss 0.000197\n",
      ">> Epoch 644 finished \tANN training loss 0.000196\n",
      ">> Epoch 645 finished \tANN training loss 0.000186\n",
      ">> Epoch 646 finished \tANN training loss 0.000186\n",
      ">> Epoch 647 finished \tANN training loss 0.000190\n",
      ">> Epoch 648 finished \tANN training loss 0.000205\n",
      ">> Epoch 649 finished \tANN training loss 0.000209\n",
      ">> Epoch 650 finished \tANN training loss 0.000202\n",
      ">> Epoch 651 finished \tANN training loss 0.000201\n",
      ">> Epoch 652 finished \tANN training loss 0.000183\n",
      ">> Epoch 653 finished \tANN training loss 0.000182\n",
      ">> Epoch 654 finished \tANN training loss 0.000187\n",
      ">> Epoch 655 finished \tANN training loss 0.000211\n",
      ">> Epoch 656 finished \tANN training loss 0.000209\n",
      ">> Epoch 657 finished \tANN training loss 0.000196\n",
      ">> Epoch 658 finished \tANN training loss 0.000187\n",
      ">> Epoch 659 finished \tANN training loss 0.000198\n",
      ">> Epoch 660 finished \tANN training loss 0.000191\n",
      ">> Epoch 661 finished \tANN training loss 0.000177\n",
      ">> Epoch 662 finished \tANN training loss 0.000182\n",
      ">> Epoch 663 finished \tANN training loss 0.000179\n",
      ">> Epoch 664 finished \tANN training loss 0.000162\n",
      ">> Epoch 665 finished \tANN training loss 0.000164\n",
      ">> Epoch 666 finished \tANN training loss 0.000184\n",
      ">> Epoch 667 finished \tANN training loss 0.000164\n",
      ">> Epoch 668 finished \tANN training loss 0.000186\n",
      ">> Epoch 669 finished \tANN training loss 0.000174\n",
      ">> Epoch 670 finished \tANN training loss 0.000165\n",
      ">> Epoch 671 finished \tANN training loss 0.000167\n",
      ">> Epoch 672 finished \tANN training loss 0.000169\n",
      ">> Epoch 673 finished \tANN training loss 0.000171\n",
      ">> Epoch 674 finished \tANN training loss 0.000169\n",
      ">> Epoch 675 finished \tANN training loss 0.000166\n",
      ">> Epoch 676 finished \tANN training loss 0.000185\n",
      ">> Epoch 677 finished \tANN training loss 0.000178\n",
      ">> Epoch 678 finished \tANN training loss 0.000172\n",
      ">> Epoch 679 finished \tANN training loss 0.000172\n",
      ">> Epoch 680 finished \tANN training loss 0.000170\n",
      ">> Epoch 681 finished \tANN training loss 0.000183\n",
      ">> Epoch 682 finished \tANN training loss 0.000174\n",
      ">> Epoch 683 finished \tANN training loss 0.000179\n",
      ">> Epoch 684 finished \tANN training loss 0.000189\n",
      ">> Epoch 685 finished \tANN training loss 0.000190\n",
      ">> Epoch 686 finished \tANN training loss 0.000184\n",
      ">> Epoch 687 finished \tANN training loss 0.000184\n",
      ">> Epoch 688 finished \tANN training loss 0.000175\n",
      ">> Epoch 689 finished \tANN training loss 0.000172\n",
      ">> Epoch 690 finished \tANN training loss 0.000173\n",
      ">> Epoch 691 finished \tANN training loss 0.000190\n",
      ">> Epoch 692 finished \tANN training loss 0.000182\n",
      ">> Epoch 693 finished \tANN training loss 0.000192\n",
      ">> Epoch 694 finished \tANN training loss 0.000178\n",
      ">> Epoch 695 finished \tANN training loss 0.000182\n",
      ">> Epoch 696 finished \tANN training loss 0.000180\n",
      ">> Epoch 697 finished \tANN training loss 0.000175\n",
      ">> Epoch 698 finished \tANN training loss 0.000167\n",
      ">> Epoch 699 finished \tANN training loss 0.000161\n",
      ">> Epoch 700 finished \tANN training loss 0.000190\n",
      ">> Epoch 701 finished \tANN training loss 0.000179\n",
      ">> Epoch 702 finished \tANN training loss 0.000316\n",
      ">> Epoch 703 finished \tANN training loss 0.000293\n",
      ">> Epoch 704 finished \tANN training loss 0.000298\n",
      ">> Epoch 705 finished \tANN training loss 0.000290\n",
      ">> Epoch 706 finished \tANN training loss 0.000248\n",
      ">> Epoch 707 finished \tANN training loss 0.000239\n",
      ">> Epoch 708 finished \tANN training loss 0.000224\n",
      ">> Epoch 709 finished \tANN training loss 0.000257\n",
      ">> Epoch 710 finished \tANN training loss 0.000232\n",
      ">> Epoch 711 finished \tANN training loss 0.000211\n",
      ">> Epoch 712 finished \tANN training loss 0.000206\n",
      ">> Epoch 713 finished \tANN training loss 0.000194\n",
      ">> Epoch 714 finished \tANN training loss 0.000188\n",
      ">> Epoch 715 finished \tANN training loss 0.000204\n",
      ">> Epoch 716 finished \tANN training loss 0.000207\n",
      ">> Epoch 717 finished \tANN training loss 0.000210\n",
      ">> Epoch 718 finished \tANN training loss 0.000174\n",
      ">> Epoch 719 finished \tANN training loss 0.000202\n",
      ">> Epoch 720 finished \tANN training loss 0.000217\n",
      ">> Epoch 721 finished \tANN training loss 0.000293\n",
      ">> Epoch 722 finished \tANN training loss 0.000272\n",
      ">> Epoch 723 finished \tANN training loss 0.000245\n",
      ">> Epoch 724 finished \tANN training loss 0.000243\n",
      ">> Epoch 725 finished \tANN training loss 0.000238\n",
      ">> Epoch 726 finished \tANN training loss 0.000207\n",
      ">> Epoch 727 finished \tANN training loss 0.000187\n",
      ">> Epoch 728 finished \tANN training loss 0.000171\n",
      ">> Epoch 729 finished \tANN training loss 0.000170\n",
      ">> Epoch 730 finished \tANN training loss 0.000193\n",
      ">> Epoch 731 finished \tANN training loss 0.000173\n",
      ">> Epoch 732 finished \tANN training loss 0.000266\n",
      ">> Epoch 733 finished \tANN training loss 0.000233\n",
      ">> Epoch 734 finished \tANN training loss 0.000224\n",
      ">> Epoch 735 finished \tANN training loss 0.000196\n",
      ">> Epoch 736 finished \tANN training loss 0.000192\n",
      ">> Epoch 737 finished \tANN training loss 0.000194\n",
      ">> Epoch 738 finished \tANN training loss 0.000214\n",
      ">> Epoch 739 finished \tANN training loss 0.000199\n",
      ">> Epoch 740 finished \tANN training loss 0.000213\n",
      ">> Epoch 741 finished \tANN training loss 0.000191\n",
      ">> Epoch 742 finished \tANN training loss 0.000192\n",
      ">> Epoch 743 finished \tANN training loss 0.000201\n",
      ">> Epoch 744 finished \tANN training loss 0.000193\n",
      ">> Epoch 745 finished \tANN training loss 0.000186\n",
      ">> Epoch 746 finished \tANN training loss 0.000185\n",
      ">> Epoch 747 finished \tANN training loss 0.000183\n",
      ">> Epoch 748 finished \tANN training loss 0.000166\n",
      ">> Epoch 749 finished \tANN training loss 0.000165\n",
      ">> Epoch 750 finished \tANN training loss 0.000168\n",
      ">> Epoch 751 finished \tANN training loss 0.000194\n",
      ">> Epoch 752 finished \tANN training loss 0.000172\n",
      ">> Epoch 753 finished \tANN training loss 0.000160\n",
      ">> Epoch 754 finished \tANN training loss 0.000143\n",
      ">> Epoch 755 finished \tANN training loss 0.000140\n",
      ">> Epoch 756 finished \tANN training loss 0.000144\n",
      ">> Epoch 757 finished \tANN training loss 0.000144\n",
      ">> Epoch 758 finished \tANN training loss 0.000150\n",
      ">> Epoch 759 finished \tANN training loss 0.000135\n",
      ">> Epoch 760 finished \tANN training loss 0.000138\n",
      ">> Epoch 761 finished \tANN training loss 0.000136\n",
      ">> Epoch 762 finished \tANN training loss 0.000135\n",
      ">> Epoch 763 finished \tANN training loss 0.000133\n",
      ">> Epoch 764 finished \tANN training loss 0.000123\n",
      ">> Epoch 765 finished \tANN training loss 0.000123\n",
      ">> Epoch 766 finished \tANN training loss 0.000118\n",
      ">> Epoch 767 finished \tANN training loss 0.000122\n",
      ">> Epoch 768 finished \tANN training loss 0.000118\n",
      ">> Epoch 769 finished \tANN training loss 0.000114\n",
      ">> Epoch 770 finished \tANN training loss 0.000114\n",
      ">> Epoch 771 finished \tANN training loss 0.000124\n",
      ">> Epoch 772 finished \tANN training loss 0.000241\n",
      ">> Epoch 773 finished \tANN training loss 0.000234\n",
      ">> Epoch 774 finished \tANN training loss 0.000186\n",
      ">> Epoch 775 finished \tANN training loss 0.000190\n",
      ">> Epoch 776 finished \tANN training loss 0.000167\n",
      ">> Epoch 777 finished \tANN training loss 0.000213\n",
      ">> Epoch 778 finished \tANN training loss 0.000207\n",
      ">> Epoch 779 finished \tANN training loss 0.000172\n",
      ">> Epoch 780 finished \tANN training loss 0.000164\n",
      ">> Epoch 781 finished \tANN training loss 0.000155\n",
      ">> Epoch 782 finished \tANN training loss 0.000148\n",
      ">> Epoch 783 finished \tANN training loss 0.000149\n",
      ">> Epoch 784 finished \tANN training loss 0.000142\n",
      ">> Epoch 785 finished \tANN training loss 0.000143\n",
      ">> Epoch 786 finished \tANN training loss 0.000145\n",
      ">> Epoch 787 finished \tANN training loss 0.000166\n",
      ">> Epoch 788 finished \tANN training loss 0.000147\n",
      ">> Epoch 789 finished \tANN training loss 0.000162\n",
      ">> Epoch 790 finished \tANN training loss 0.000159\n",
      ">> Epoch 791 finished \tANN training loss 0.000169\n",
      ">> Epoch 792 finished \tANN training loss 0.000228\n",
      ">> Epoch 793 finished \tANN training loss 0.000191\n",
      ">> Epoch 794 finished \tANN training loss 0.000168\n",
      ">> Epoch 795 finished \tANN training loss 0.000185\n",
      ">> Epoch 796 finished \tANN training loss 0.000168\n",
      ">> Epoch 797 finished \tANN training loss 0.000183\n",
      ">> Epoch 798 finished \tANN training loss 0.000172\n",
      ">> Epoch 799 finished \tANN training loss 0.000170\n",
      ">> Epoch 800 finished \tANN training loss 0.000175\n",
      ">> Epoch 801 finished \tANN training loss 0.000154\n",
      ">> Epoch 802 finished \tANN training loss 0.000156\n",
      ">> Epoch 803 finished \tANN training loss 0.000168\n",
      ">> Epoch 804 finished \tANN training loss 0.000173\n",
      ">> Epoch 805 finished \tANN training loss 0.000175\n",
      ">> Epoch 806 finished \tANN training loss 0.000174\n",
      ">> Epoch 807 finished \tANN training loss 0.000172\n",
      ">> Epoch 808 finished \tANN training loss 0.000169\n",
      ">> Epoch 809 finished \tANN training loss 0.000163\n",
      ">> Epoch 810 finished \tANN training loss 0.000167\n",
      ">> Epoch 811 finished \tANN training loss 0.000175\n",
      ">> Epoch 812 finished \tANN training loss 0.000184\n",
      ">> Epoch 813 finished \tANN training loss 0.000184\n",
      ">> Epoch 814 finished \tANN training loss 0.000165\n",
      ">> Epoch 815 finished \tANN training loss 0.000158\n",
      ">> Epoch 816 finished \tANN training loss 0.000144\n",
      ">> Epoch 817 finished \tANN training loss 0.000150\n",
      ">> Epoch 818 finished \tANN training loss 0.000149\n",
      ">> Epoch 819 finished \tANN training loss 0.000142\n",
      ">> Epoch 820 finished \tANN training loss 0.000144\n",
      ">> Epoch 821 finished \tANN training loss 0.000169\n",
      ">> Epoch 822 finished \tANN training loss 0.000164\n",
      ">> Epoch 823 finished \tANN training loss 0.000163\n",
      ">> Epoch 824 finished \tANN training loss 0.000158\n",
      ">> Epoch 825 finished \tANN training loss 0.000167\n",
      ">> Epoch 826 finished \tANN training loss 0.000163\n",
      ">> Epoch 827 finished \tANN training loss 0.000164\n",
      ">> Epoch 828 finished \tANN training loss 0.000276\n",
      ">> Epoch 829 finished \tANN training loss 0.000206\n",
      ">> Epoch 830 finished \tANN training loss 0.000203\n",
      ">> Epoch 831 finished \tANN training loss 0.000216\n",
      ">> Epoch 832 finished \tANN training loss 0.000224\n",
      ">> Epoch 833 finished \tANN training loss 0.000201\n",
      ">> Epoch 834 finished \tANN training loss 0.000200\n",
      ">> Epoch 835 finished \tANN training loss 0.000196\n",
      ">> Epoch 836 finished \tANN training loss 0.000184\n",
      ">> Epoch 837 finished \tANN training loss 0.000176\n",
      ">> Epoch 838 finished \tANN training loss 0.000171\n",
      ">> Epoch 839 finished \tANN training loss 0.000157\n",
      ">> Epoch 840 finished \tANN training loss 0.000164\n",
      ">> Epoch 841 finished \tANN training loss 0.000145\n",
      ">> Epoch 842 finished \tANN training loss 0.000147\n",
      ">> Epoch 843 finished \tANN training loss 0.000144\n",
      ">> Epoch 844 finished \tANN training loss 0.000149\n",
      ">> Epoch 845 finished \tANN training loss 0.000147\n",
      ">> Epoch 846 finished \tANN training loss 0.000143\n",
      ">> Epoch 847 finished \tANN training loss 0.000154\n",
      ">> Epoch 848 finished \tANN training loss 0.000159\n",
      ">> Epoch 849 finished \tANN training loss 0.000154\n",
      ">> Epoch 850 finished \tANN training loss 0.000147\n",
      ">> Epoch 851 finished \tANN training loss 0.000137\n",
      ">> Epoch 852 finished \tANN training loss 0.000136\n",
      ">> Epoch 853 finished \tANN training loss 0.000122\n",
      ">> Epoch 854 finished \tANN training loss 0.000122\n",
      ">> Epoch 855 finished \tANN training loss 0.000130\n",
      ">> Epoch 856 finished \tANN training loss 0.000134\n",
      ">> Epoch 857 finished \tANN training loss 0.000127\n",
      ">> Epoch 858 finished \tANN training loss 0.000124\n",
      ">> Epoch 859 finished \tANN training loss 0.000120\n",
      ">> Epoch 860 finished \tANN training loss 0.000117\n",
      ">> Epoch 861 finished \tANN training loss 0.000131\n",
      ">> Epoch 862 finished \tANN training loss 0.000124\n",
      ">> Epoch 863 finished \tANN training loss 0.000129\n",
      ">> Epoch 864 finished \tANN training loss 0.000120\n",
      ">> Epoch 865 finished \tANN training loss 0.000127\n",
      ">> Epoch 866 finished \tANN training loss 0.000126\n",
      ">> Epoch 867 finished \tANN training loss 0.000114\n",
      ">> Epoch 868 finished \tANN training loss 0.000107\n",
      ">> Epoch 869 finished \tANN training loss 0.000112\n",
      ">> Epoch 870 finished \tANN training loss 0.000107\n",
      ">> Epoch 871 finished \tANN training loss 0.000125\n",
      ">> Epoch 872 finished \tANN training loss 0.000118\n",
      ">> Epoch 873 finished \tANN training loss 0.000116\n",
      ">> Epoch 874 finished \tANN training loss 0.000114\n",
      ">> Epoch 875 finished \tANN training loss 0.000132\n",
      ">> Epoch 876 finished \tANN training loss 0.000131\n",
      ">> Epoch 877 finished \tANN training loss 0.000137\n",
      ">> Epoch 878 finished \tANN training loss 0.000137\n",
      ">> Epoch 879 finished \tANN training loss 0.000135\n",
      ">> Epoch 880 finished \tANN training loss 0.000135\n",
      ">> Epoch 881 finished \tANN training loss 0.000136\n",
      ">> Epoch 882 finished \tANN training loss 0.000134\n",
      ">> Epoch 883 finished \tANN training loss 0.000135\n",
      ">> Epoch 884 finished \tANN training loss 0.000135\n",
      ">> Epoch 885 finished \tANN training loss 0.000129\n",
      ">> Epoch 886 finished \tANN training loss 0.000136\n",
      ">> Epoch 887 finished \tANN training loss 0.000136\n",
      ">> Epoch 888 finished \tANN training loss 0.000135\n",
      ">> Epoch 889 finished \tANN training loss 0.000118\n",
      ">> Epoch 890 finished \tANN training loss 0.000116\n",
      ">> Epoch 891 finished \tANN training loss 0.000116\n",
      ">> Epoch 892 finished \tANN training loss 0.000118\n",
      ">> Epoch 893 finished \tANN training loss 0.000121\n",
      ">> Epoch 894 finished \tANN training loss 0.000118\n",
      ">> Epoch 895 finished \tANN training loss 0.000148\n",
      ">> Epoch 896 finished \tANN training loss 0.000122\n",
      ">> Epoch 897 finished \tANN training loss 0.000126\n",
      ">> Epoch 898 finished \tANN training loss 0.000147\n",
      ">> Epoch 899 finished \tANN training loss 0.000138\n",
      ">> Epoch 900 finished \tANN training loss 0.000134\n",
      ">> Epoch 901 finished \tANN training loss 0.000129\n",
      ">> Epoch 902 finished \tANN training loss 0.000132\n",
      ">> Epoch 903 finished \tANN training loss 0.000119\n",
      ">> Epoch 904 finished \tANN training loss 0.000121\n",
      ">> Epoch 905 finished \tANN training loss 0.000125\n",
      ">> Epoch 906 finished \tANN training loss 0.000135\n",
      ">> Epoch 907 finished \tANN training loss 0.000153\n",
      ">> Epoch 908 finished \tANN training loss 0.000146\n",
      ">> Epoch 909 finished \tANN training loss 0.000139\n",
      ">> Epoch 910 finished \tANN training loss 0.000149\n",
      ">> Epoch 911 finished \tANN training loss 0.000142\n",
      ">> Epoch 912 finished \tANN training loss 0.000140\n",
      ">> Epoch 913 finished \tANN training loss 0.000136\n",
      ">> Epoch 914 finished \tANN training loss 0.000135\n",
      ">> Epoch 915 finished \tANN training loss 0.000139\n",
      ">> Epoch 916 finished \tANN training loss 0.000160\n",
      ">> Epoch 917 finished \tANN training loss 0.000131\n",
      ">> Epoch 918 finished \tANN training loss 0.000124\n",
      ">> Epoch 919 finished \tANN training loss 0.000139\n",
      ">> Epoch 920 finished \tANN training loss 0.000126\n",
      ">> Epoch 921 finished \tANN training loss 0.000129\n",
      ">> Epoch 922 finished \tANN training loss 0.000127\n",
      ">> Epoch 923 finished \tANN training loss 0.000120\n",
      ">> Epoch 924 finished \tANN training loss 0.000112\n",
      ">> Epoch 925 finished \tANN training loss 0.000111\n",
      ">> Epoch 926 finished \tANN training loss 0.000116\n",
      ">> Epoch 927 finished \tANN training loss 0.000120\n",
      ">> Epoch 928 finished \tANN training loss 0.000142\n",
      ">> Epoch 929 finished \tANN training loss 0.000140\n",
      ">> Epoch 930 finished \tANN training loss 0.000119\n",
      ">> Epoch 931 finished \tANN training loss 0.000125\n",
      ">> Epoch 932 finished \tANN training loss 0.000132\n",
      ">> Epoch 933 finished \tANN training loss 0.000133\n",
      ">> Epoch 934 finished \tANN training loss 0.000140\n",
      ">> Epoch 935 finished \tANN training loss 0.000137\n",
      ">> Epoch 936 finished \tANN training loss 0.000134\n",
      ">> Epoch 937 finished \tANN training loss 0.000137\n",
      ">> Epoch 938 finished \tANN training loss 0.000123\n",
      ">> Epoch 939 finished \tANN training loss 0.000120\n",
      ">> Epoch 940 finished \tANN training loss 0.000118\n",
      ">> Epoch 941 finished \tANN training loss 0.000115\n",
      ">> Epoch 942 finished \tANN training loss 0.000114\n",
      ">> Epoch 943 finished \tANN training loss 0.000137\n",
      ">> Epoch 944 finished \tANN training loss 0.000136\n",
      ">> Epoch 945 finished \tANN training loss 0.000117\n",
      ">> Epoch 946 finished \tANN training loss 0.000125\n",
      ">> Epoch 947 finished \tANN training loss 0.000118\n",
      ">> Epoch 948 finished \tANN training loss 0.000134\n",
      ">> Epoch 949 finished \tANN training loss 0.000126\n",
      ">> Epoch 950 finished \tANN training loss 0.000122\n",
      ">> Epoch 951 finished \tANN training loss 0.000117\n",
      ">> Epoch 952 finished \tANN training loss 0.000119\n",
      ">> Epoch 953 finished \tANN training loss 0.000115\n",
      ">> Epoch 954 finished \tANN training loss 0.000114\n",
      ">> Epoch 955 finished \tANN training loss 0.000114\n",
      ">> Epoch 956 finished \tANN training loss 0.000112\n",
      ">> Epoch 957 finished \tANN training loss 0.000113\n",
      ">> Epoch 958 finished \tANN training loss 0.000106\n",
      ">> Epoch 959 finished \tANN training loss 0.000106\n",
      ">> Epoch 960 finished \tANN training loss 0.000111\n",
      ">> Epoch 961 finished \tANN training loss 0.000112\n",
      ">> Epoch 962 finished \tANN training loss 0.000108\n",
      ">> Epoch 963 finished \tANN training loss 0.000112\n",
      ">> Epoch 964 finished \tANN training loss 0.000106\n",
      ">> Epoch 965 finished \tANN training loss 0.000103\n",
      ">> Epoch 966 finished \tANN training loss 0.000107\n",
      ">> Epoch 967 finished \tANN training loss 0.000112\n",
      ">> Epoch 968 finished \tANN training loss 0.000122\n",
      ">> Epoch 969 finished \tANN training loss 0.000125\n",
      ">> Epoch 970 finished \tANN training loss 0.000113\n",
      ">> Epoch 971 finished \tANN training loss 0.000104\n",
      ">> Epoch 972 finished \tANN training loss 0.000099\n",
      ">> Epoch 973 finished \tANN training loss 0.000095\n",
      ">> Epoch 974 finished \tANN training loss 0.000094\n",
      ">> Epoch 975 finished \tANN training loss 0.000082\n",
      ">> Epoch 976 finished \tANN training loss 0.000092\n",
      ">> Epoch 977 finished \tANN training loss 0.000093\n",
      ">> Epoch 978 finished \tANN training loss 0.000099\n",
      ">> Epoch 979 finished \tANN training loss 0.000109\n",
      ">> Epoch 980 finished \tANN training loss 0.000102\n",
      ">> Epoch 981 finished \tANN training loss 0.000099\n",
      ">> Epoch 982 finished \tANN training loss 0.000091\n",
      ">> Epoch 983 finished \tANN training loss 0.000104\n",
      ">> Epoch 984 finished \tANN training loss 0.000101\n",
      ">> Epoch 985 finished \tANN training loss 0.000095\n",
      ">> Epoch 986 finished \tANN training loss 0.000096\n",
      ">> Epoch 987 finished \tANN training loss 0.000097\n",
      ">> Epoch 988 finished \tANN training loss 0.000101\n",
      ">> Epoch 989 finished \tANN training loss 0.000096\n",
      ">> Epoch 990 finished \tANN training loss 0.000091\n",
      ">> Epoch 991 finished \tANN training loss 0.000103\n",
      ">> Epoch 992 finished \tANN training loss 0.000113\n",
      ">> Epoch 993 finished \tANN training loss 0.000113\n",
      ">> Epoch 994 finished \tANN training loss 0.000102\n",
      ">> Epoch 995 finished \tANN training loss 0.000097\n",
      ">> Epoch 996 finished \tANN training loss 0.000097\n",
      ">> Epoch 997 finished \tANN training loss 0.000095\n",
      ">> Epoch 998 finished \tANN training loss 0.000090\n",
      ">> Epoch 999 finished \tANN training loss 0.000135\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.165508\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.572035\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.302496\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.202908\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.146551\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.118368\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.088036\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.080785\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.083155\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.077845\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.077614\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.076987\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.082679\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.079171\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.080383\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.080341\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.089690\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.083351\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.081340\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.081980\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2.270436\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.755896\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.367606\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.846669\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.522638\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.425866\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.368696\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.326839\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.269296\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.222812\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.185026\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.142043\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.109123\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.077194\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.053952\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.035805\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.028774\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.026870\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.025453\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.020979\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.493173\n",
      ">> Epoch 1 finished \tANN training loss 0.268749\n",
      ">> Epoch 2 finished \tANN training loss 0.172080\n",
      ">> Epoch 3 finished \tANN training loss 0.120941\n",
      ">> Epoch 4 finished \tANN training loss 0.091633\n",
      ">> Epoch 5 finished \tANN training loss 0.075707\n",
      ">> Epoch 6 finished \tANN training loss 0.066431\n",
      ">> Epoch 7 finished \tANN training loss 0.061657\n",
      ">> Epoch 8 finished \tANN training loss 0.057612\n",
      ">> Epoch 9 finished \tANN training loss 0.055834\n",
      ">> Epoch 10 finished \tANN training loss 0.053520\n",
      ">> Epoch 11 finished \tANN training loss 0.054604\n",
      ">> Epoch 12 finished \tANN training loss 0.052019\n",
      ">> Epoch 13 finished \tANN training loss 0.050826\n",
      ">> Epoch 14 finished \tANN training loss 0.050487\n",
      ">> Epoch 15 finished \tANN training loss 0.050824\n",
      ">> Epoch 16 finished \tANN training loss 0.051098\n",
      ">> Epoch 17 finished \tANN training loss 0.050560\n",
      ">> Epoch 18 finished \tANN training loss 0.049975\n",
      ">> Epoch 19 finished \tANN training loss 0.049883\n",
      ">> Epoch 20 finished \tANN training loss 0.048746\n",
      ">> Epoch 21 finished \tANN training loss 0.048656\n",
      ">> Epoch 22 finished \tANN training loss 0.048716\n",
      ">> Epoch 23 finished \tANN training loss 0.049120\n",
      ">> Epoch 24 finished \tANN training loss 0.048556\n",
      ">> Epoch 25 finished \tANN training loss 0.049021\n",
      ">> Epoch 26 finished \tANN training loss 0.049634\n",
      ">> Epoch 27 finished \tANN training loss 0.048992\n",
      ">> Epoch 28 finished \tANN training loss 0.048270\n",
      ">> Epoch 29 finished \tANN training loss 0.048347\n",
      ">> Epoch 30 finished \tANN training loss 0.048535\n",
      ">> Epoch 31 finished \tANN training loss 0.048577\n",
      ">> Epoch 32 finished \tANN training loss 0.047473\n",
      ">> Epoch 33 finished \tANN training loss 0.048176\n",
      ">> Epoch 34 finished \tANN training loss 0.049059\n",
      ">> Epoch 35 finished \tANN training loss 0.047814\n",
      ">> Epoch 36 finished \tANN training loss 0.047750\n",
      ">> Epoch 37 finished \tANN training loss 0.046798\n",
      ">> Epoch 38 finished \tANN training loss 0.046779\n",
      ">> Epoch 39 finished \tANN training loss 0.048440\n",
      ">> Epoch 40 finished \tANN training loss 0.046988\n",
      ">> Epoch 41 finished \tANN training loss 0.046450\n",
      ">> Epoch 42 finished \tANN training loss 0.046776\n",
      ">> Epoch 43 finished \tANN training loss 0.046340\n",
      ">> Epoch 44 finished \tANN training loss 0.046207\n",
      ">> Epoch 45 finished \tANN training loss 0.046499\n",
      ">> Epoch 46 finished \tANN training loss 0.046394\n",
      ">> Epoch 47 finished \tANN training loss 0.046200\n",
      ">> Epoch 48 finished \tANN training loss 0.045988\n",
      ">> Epoch 49 finished \tANN training loss 0.045741\n",
      ">> Epoch 50 finished \tANN training loss 0.045700\n",
      ">> Epoch 51 finished \tANN training loss 0.045851\n",
      ">> Epoch 52 finished \tANN training loss 0.045485\n",
      ">> Epoch 53 finished \tANN training loss 0.045238\n",
      ">> Epoch 54 finished \tANN training loss 0.045311\n",
      ">> Epoch 55 finished \tANN training loss 0.045096\n",
      ">> Epoch 56 finished \tANN training loss 0.045270\n",
      ">> Epoch 57 finished \tANN training loss 0.045845\n",
      ">> Epoch 58 finished \tANN training loss 0.045076\n",
      ">> Epoch 59 finished \tANN training loss 0.045486\n",
      ">> Epoch 60 finished \tANN training loss 0.047257\n",
      ">> Epoch 61 finished \tANN training loss 0.044923\n",
      ">> Epoch 62 finished \tANN training loss 0.044826\n",
      ">> Epoch 63 finished \tANN training loss 0.044953\n",
      ">> Epoch 64 finished \tANN training loss 0.045140\n",
      ">> Epoch 65 finished \tANN training loss 0.045255\n",
      ">> Epoch 66 finished \tANN training loss 0.046316\n",
      ">> Epoch 67 finished \tANN training loss 0.046285\n",
      ">> Epoch 68 finished \tANN training loss 0.046087\n",
      ">> Epoch 69 finished \tANN training loss 0.045259\n",
      ">> Epoch 70 finished \tANN training loss 0.045187\n",
      ">> Epoch 71 finished \tANN training loss 0.045142\n",
      ">> Epoch 72 finished \tANN training loss 0.045054\n",
      ">> Epoch 73 finished \tANN training loss 0.045924\n",
      ">> Epoch 74 finished \tANN training loss 0.044533\n",
      ">> Epoch 75 finished \tANN training loss 0.044365\n",
      ">> Epoch 76 finished \tANN training loss 0.044349\n",
      ">> Epoch 77 finished \tANN training loss 0.044642\n",
      ">> Epoch 78 finished \tANN training loss 0.044593\n",
      ">> Epoch 79 finished \tANN training loss 0.044614\n",
      ">> Epoch 80 finished \tANN training loss 0.044742\n",
      ">> Epoch 81 finished \tANN training loss 0.044677\n",
      ">> Epoch 82 finished \tANN training loss 0.044926\n",
      ">> Epoch 83 finished \tANN training loss 0.044774\n",
      ">> Epoch 84 finished \tANN training loss 0.044714\n",
      ">> Epoch 85 finished \tANN training loss 0.044554\n",
      ">> Epoch 86 finished \tANN training loss 0.044522\n",
      ">> Epoch 87 finished \tANN training loss 0.044528\n",
      ">> Epoch 88 finished \tANN training loss 0.045099\n",
      ">> Epoch 89 finished \tANN training loss 0.045120\n",
      ">> Epoch 90 finished \tANN training loss 0.044952\n",
      ">> Epoch 91 finished \tANN training loss 0.044774\n",
      ">> Epoch 92 finished \tANN training loss 0.045036\n",
      ">> Epoch 93 finished \tANN training loss 0.045037\n",
      ">> Epoch 94 finished \tANN training loss 0.045349\n",
      ">> Epoch 95 finished \tANN training loss 0.045052\n",
      ">> Epoch 96 finished \tANN training loss 0.044672\n",
      ">> Epoch 97 finished \tANN training loss 0.044753\n",
      ">> Epoch 98 finished \tANN training loss 0.044539\n",
      ">> Epoch 99 finished \tANN training loss 0.044689\n",
      ">> Epoch 100 finished \tANN training loss 0.044390\n",
      ">> Epoch 101 finished \tANN training loss 0.044492\n",
      ">> Epoch 102 finished \tANN training loss 0.044530\n",
      ">> Epoch 103 finished \tANN training loss 0.044608\n",
      ">> Epoch 104 finished \tANN training loss 0.044606\n",
      ">> Epoch 105 finished \tANN training loss 0.044564\n",
      ">> Epoch 106 finished \tANN training loss 0.044405\n",
      ">> Epoch 107 finished \tANN training loss 0.044209\n",
      ">> Epoch 108 finished \tANN training loss 0.044238\n",
      ">> Epoch 109 finished \tANN training loss 0.044222\n",
      ">> Epoch 110 finished \tANN training loss 0.044172\n",
      ">> Epoch 111 finished \tANN training loss 0.044372\n",
      ">> Epoch 112 finished \tANN training loss 0.044477\n",
      ">> Epoch 113 finished \tANN training loss 0.044480\n",
      ">> Epoch 114 finished \tANN training loss 0.044284\n",
      ">> Epoch 115 finished \tANN training loss 0.044160\n",
      ">> Epoch 116 finished \tANN training loss 0.044458\n",
      ">> Epoch 117 finished \tANN training loss 0.044382\n",
      ">> Epoch 118 finished \tANN training loss 0.044499\n",
      ">> Epoch 119 finished \tANN training loss 0.044462\n",
      ">> Epoch 120 finished \tANN training loss 0.044636\n",
      ">> Epoch 121 finished \tANN training loss 0.046468\n",
      ">> Epoch 122 finished \tANN training loss 0.045209\n",
      ">> Epoch 123 finished \tANN training loss 0.044046\n",
      ">> Epoch 124 finished \tANN training loss 0.045377\n",
      ">> Epoch 125 finished \tANN training loss 0.044283\n",
      ">> Epoch 126 finished \tANN training loss 0.044414\n",
      ">> Epoch 127 finished \tANN training loss 0.044101\n",
      ">> Epoch 128 finished \tANN training loss 0.044292\n",
      ">> Epoch 129 finished \tANN training loss 0.044371\n",
      ">> Epoch 130 finished \tANN training loss 0.045786\n",
      ">> Epoch 131 finished \tANN training loss 0.044521\n",
      ">> Epoch 132 finished \tANN training loss 0.044270\n",
      ">> Epoch 133 finished \tANN training loss 0.044403\n",
      ">> Epoch 134 finished \tANN training loss 0.044335\n",
      ">> Epoch 135 finished \tANN training loss 0.044431\n",
      ">> Epoch 136 finished \tANN training loss 0.044236\n",
      ">> Epoch 137 finished \tANN training loss 0.044149\n",
      ">> Epoch 138 finished \tANN training loss 0.044213\n",
      ">> Epoch 139 finished \tANN training loss 0.044238\n",
      ">> Epoch 140 finished \tANN training loss 0.044190\n",
      ">> Epoch 141 finished \tANN training loss 0.044530\n",
      ">> Epoch 142 finished \tANN training loss 0.044402\n",
      ">> Epoch 143 finished \tANN training loss 0.044760\n",
      ">> Epoch 144 finished \tANN training loss 0.044631\n",
      ">> Epoch 145 finished \tANN training loss 0.044156\n",
      ">> Epoch 146 finished \tANN training loss 0.044341\n",
      ">> Epoch 147 finished \tANN training loss 0.044391\n",
      ">> Epoch 148 finished \tANN training loss 0.044435\n",
      ">> Epoch 149 finished \tANN training loss 0.044265\n",
      ">> Epoch 150 finished \tANN training loss 0.045208\n",
      ">> Epoch 151 finished \tANN training loss 0.044917\n",
      ">> Epoch 152 finished \tANN training loss 0.044113\n",
      ">> Epoch 153 finished \tANN training loss 0.043792\n",
      ">> Epoch 154 finished \tANN training loss 0.043972\n",
      ">> Epoch 155 finished \tANN training loss 0.043760\n",
      ">> Epoch 156 finished \tANN training loss 0.043870\n",
      ">> Epoch 157 finished \tANN training loss 0.043931\n",
      ">> Epoch 158 finished \tANN training loss 0.043661\n",
      ">> Epoch 159 finished \tANN training loss 0.044081\n",
      ">> Epoch 160 finished \tANN training loss 0.044228\n",
      ">> Epoch 161 finished \tANN training loss 0.043827\n",
      ">> Epoch 162 finished \tANN training loss 0.044221\n",
      ">> Epoch 163 finished \tANN training loss 0.043913\n",
      ">> Epoch 164 finished \tANN training loss 0.043784\n",
      ">> Epoch 165 finished \tANN training loss 0.044051\n",
      ">> Epoch 166 finished \tANN training loss 0.044221\n",
      ">> Epoch 167 finished \tANN training loss 0.043714\n",
      ">> Epoch 168 finished \tANN training loss 0.044253\n",
      ">> Epoch 169 finished \tANN training loss 0.044132\n",
      ">> Epoch 170 finished \tANN training loss 0.043749\n",
      ">> Epoch 171 finished \tANN training loss 0.043807\n",
      ">> Epoch 172 finished \tANN training loss 0.043615\n",
      ">> Epoch 173 finished \tANN training loss 0.044299\n",
      ">> Epoch 174 finished \tANN training loss 0.043591\n",
      ">> Epoch 175 finished \tANN training loss 0.043281\n",
      ">> Epoch 176 finished \tANN training loss 0.043445\n",
      ">> Epoch 177 finished \tANN training loss 0.043576\n",
      ">> Epoch 178 finished \tANN training loss 0.043665\n",
      ">> Epoch 179 finished \tANN training loss 0.043193\n",
      ">> Epoch 180 finished \tANN training loss 0.043391\n",
      ">> Epoch 181 finished \tANN training loss 0.045285\n",
      ">> Epoch 182 finished \tANN training loss 0.044240\n",
      ">> Epoch 183 finished \tANN training loss 0.044205\n",
      ">> Epoch 184 finished \tANN training loss 0.044173\n",
      ">> Epoch 185 finished \tANN training loss 0.043888\n",
      ">> Epoch 186 finished \tANN training loss 0.043836\n",
      ">> Epoch 187 finished \tANN training loss 0.043946\n",
      ">> Epoch 188 finished \tANN training loss 0.043940\n",
      ">> Epoch 189 finished \tANN training loss 0.043975\n",
      ">> Epoch 190 finished \tANN training loss 0.044021\n",
      ">> Epoch 191 finished \tANN training loss 0.043845\n",
      ">> Epoch 192 finished \tANN training loss 0.043914\n",
      ">> Epoch 193 finished \tANN training loss 0.043723\n",
      ">> Epoch 194 finished \tANN training loss 0.043732\n",
      ">> Epoch 195 finished \tANN training loss 0.043556\n",
      ">> Epoch 196 finished \tANN training loss 0.043736\n",
      ">> Epoch 197 finished \tANN training loss 0.043809\n",
      ">> Epoch 198 finished \tANN training loss 0.043958\n",
      ">> Epoch 199 finished \tANN training loss 0.044454\n",
      ">> Epoch 200 finished \tANN training loss 0.044402\n",
      ">> Epoch 201 finished \tANN training loss 0.044000\n",
      ">> Epoch 202 finished \tANN training loss 0.043719\n",
      ">> Epoch 203 finished \tANN training loss 0.043630\n",
      ">> Epoch 204 finished \tANN training loss 0.043758\n",
      ">> Epoch 205 finished \tANN training loss 0.043866\n",
      ">> Epoch 206 finished \tANN training loss 0.043749\n",
      ">> Epoch 207 finished \tANN training loss 0.043885\n",
      ">> Epoch 208 finished \tANN training loss 0.043979\n",
      ">> Epoch 209 finished \tANN training loss 0.044245\n",
      ">> Epoch 210 finished \tANN training loss 0.043949\n",
      ">> Epoch 211 finished \tANN training loss 0.043628\n",
      ">> Epoch 212 finished \tANN training loss 0.043742\n",
      ">> Epoch 213 finished \tANN training loss 0.043457\n",
      ">> Epoch 214 finished \tANN training loss 0.043434\n",
      ">> Epoch 215 finished \tANN training loss 0.043543\n",
      ">> Epoch 216 finished \tANN training loss 0.043120\n",
      ">> Epoch 217 finished \tANN training loss 0.043142\n",
      ">> Epoch 218 finished \tANN training loss 0.043095\n",
      ">> Epoch 219 finished \tANN training loss 0.043181\n",
      ">> Epoch 220 finished \tANN training loss 0.043198\n",
      ">> Epoch 221 finished \tANN training loss 0.043792\n",
      ">> Epoch 222 finished \tANN training loss 0.043295\n",
      ">> Epoch 223 finished \tANN training loss 0.043169\n",
      ">> Epoch 224 finished \tANN training loss 0.043059\n",
      ">> Epoch 225 finished \tANN training loss 0.043123\n",
      ">> Epoch 226 finished \tANN training loss 0.042962\n",
      ">> Epoch 227 finished \tANN training loss 0.042908\n",
      ">> Epoch 228 finished \tANN training loss 0.043345\n",
      ">> Epoch 229 finished \tANN training loss 0.043277\n",
      ">> Epoch 230 finished \tANN training loss 0.043354\n",
      ">> Epoch 231 finished \tANN training loss 0.043312\n",
      ">> Epoch 232 finished \tANN training loss 0.043421\n",
      ">> Epoch 233 finished \tANN training loss 0.043202\n",
      ">> Epoch 234 finished \tANN training loss 0.043055\n",
      ">> Epoch 235 finished \tANN training loss 0.043202\n",
      ">> Epoch 236 finished \tANN training loss 0.042629\n",
      ">> Epoch 237 finished \tANN training loss 0.042705\n",
      ">> Epoch 238 finished \tANN training loss 0.042704\n",
      ">> Epoch 239 finished \tANN training loss 0.042768\n",
      ">> Epoch 240 finished \tANN training loss 0.042716\n",
      ">> Epoch 241 finished \tANN training loss 0.042714\n",
      ">> Epoch 242 finished \tANN training loss 0.042797\n",
      ">> Epoch 243 finished \tANN training loss 0.042738\n",
      ">> Epoch 244 finished \tANN training loss 0.042569\n",
      ">> Epoch 245 finished \tANN training loss 0.042504\n",
      ">> Epoch 246 finished \tANN training loss 0.042469\n",
      ">> Epoch 247 finished \tANN training loss 0.042544\n",
      ">> Epoch 248 finished \tANN training loss 0.042714\n",
      ">> Epoch 249 finished \tANN training loss 0.042805\n",
      ">> Epoch 250 finished \tANN training loss 0.042879\n",
      ">> Epoch 251 finished \tANN training loss 0.042697\n",
      ">> Epoch 252 finished \tANN training loss 0.042656\n",
      ">> Epoch 253 finished \tANN training loss 0.042642\n",
      ">> Epoch 254 finished \tANN training loss 0.042715\n",
      ">> Epoch 255 finished \tANN training loss 0.042641\n",
      ">> Epoch 256 finished \tANN training loss 0.042652\n",
      ">> Epoch 257 finished \tANN training loss 0.042839\n",
      ">> Epoch 258 finished \tANN training loss 0.042834\n",
      ">> Epoch 259 finished \tANN training loss 0.042941\n",
      ">> Epoch 260 finished \tANN training loss 0.042924\n",
      ">> Epoch 261 finished \tANN training loss 0.043302\n",
      ">> Epoch 262 finished \tANN training loss 0.042970\n",
      ">> Epoch 263 finished \tANN training loss 0.043063\n",
      ">> Epoch 264 finished \tANN training loss 0.042772\n",
      ">> Epoch 265 finished \tANN training loss 0.042514\n",
      ">> Epoch 266 finished \tANN training loss 0.042550\n",
      ">> Epoch 267 finished \tANN training loss 0.042529\n",
      ">> Epoch 268 finished \tANN training loss 0.042438\n",
      ">> Epoch 269 finished \tANN training loss 0.042415\n",
      ">> Epoch 270 finished \tANN training loss 0.042439\n",
      ">> Epoch 271 finished \tANN training loss 0.042765\n",
      ">> Epoch 272 finished \tANN training loss 0.042906\n",
      ">> Epoch 273 finished \tANN training loss 0.042384\n",
      ">> Epoch 274 finished \tANN training loss 0.042758\n",
      ">> Epoch 275 finished \tANN training loss 0.042455\n",
      ">> Epoch 276 finished \tANN training loss 0.042277\n",
      ">> Epoch 277 finished \tANN training loss 0.042420\n",
      ">> Epoch 278 finished \tANN training loss 0.042397\n",
      ">> Epoch 279 finished \tANN training loss 0.042393\n",
      ">> Epoch 280 finished \tANN training loss 0.042267\n",
      ">> Epoch 281 finished \tANN training loss 0.042418\n",
      ">> Epoch 282 finished \tANN training loss 0.042701\n",
      ">> Epoch 283 finished \tANN training loss 0.042851\n",
      ">> Epoch 284 finished \tANN training loss 0.043035\n",
      ">> Epoch 285 finished \tANN training loss 0.043099\n",
      ">> Epoch 286 finished \tANN training loss 0.043420\n",
      ">> Epoch 287 finished \tANN training loss 0.042909\n",
      ">> Epoch 288 finished \tANN training loss 0.043308\n",
      ">> Epoch 289 finished \tANN training loss 0.043459\n",
      ">> Epoch 290 finished \tANN training loss 0.043123\n",
      ">> Epoch 291 finished \tANN training loss 0.043734\n",
      ">> Epoch 292 finished \tANN training loss 0.043468\n",
      ">> Epoch 293 finished \tANN training loss 0.043001\n",
      ">> Epoch 294 finished \tANN training loss 0.043344\n",
      ">> Epoch 295 finished \tANN training loss 0.042854\n",
      ">> Epoch 296 finished \tANN training loss 0.042529\n",
      ">> Epoch 297 finished \tANN training loss 0.042770\n",
      ">> Epoch 298 finished \tANN training loss 0.042889\n",
      ">> Epoch 299 finished \tANN training loss 0.044447\n",
      ">> Epoch 300 finished \tANN training loss 0.043128\n",
      ">> Epoch 301 finished \tANN training loss 0.042641\n",
      ">> Epoch 302 finished \tANN training loss 0.042649\n",
      ">> Epoch 303 finished \tANN training loss 0.042754\n",
      ">> Epoch 304 finished \tANN training loss 0.042747\n",
      ">> Epoch 305 finished \tANN training loss 0.042836\n",
      ">> Epoch 306 finished \tANN training loss 0.042819\n",
      ">> Epoch 307 finished \tANN training loss 0.042666\n",
      ">> Epoch 308 finished \tANN training loss 0.042529\n",
      ">> Epoch 309 finished \tANN training loss 0.042351\n",
      ">> Epoch 310 finished \tANN training loss 0.042471\n",
      ">> Epoch 311 finished \tANN training loss 0.042340\n",
      ">> Epoch 312 finished \tANN training loss 0.042335\n",
      ">> Epoch 313 finished \tANN training loss 0.042490\n",
      ">> Epoch 314 finished \tANN training loss 0.042620\n",
      ">> Epoch 315 finished \tANN training loss 0.042962\n",
      ">> Epoch 316 finished \tANN training loss 0.042910\n",
      ">> Epoch 317 finished \tANN training loss 0.042559\n",
      ">> Epoch 318 finished \tANN training loss 0.042495\n",
      ">> Epoch 319 finished \tANN training loss 0.042349\n",
      ">> Epoch 320 finished \tANN training loss 0.042391\n",
      ">> Epoch 321 finished \tANN training loss 0.042399\n",
      ">> Epoch 322 finished \tANN training loss 0.042327\n",
      ">> Epoch 323 finished \tANN training loss 0.042186\n",
      ">> Epoch 324 finished \tANN training loss 0.042321\n",
      ">> Epoch 325 finished \tANN training loss 0.042347\n",
      ">> Epoch 326 finished \tANN training loss 0.042421\n",
      ">> Epoch 327 finished \tANN training loss 0.042481\n",
      ">> Epoch 328 finished \tANN training loss 0.042328\n",
      ">> Epoch 329 finished \tANN training loss 0.042344\n",
      ">> Epoch 330 finished \tANN training loss 0.042166\n",
      ">> Epoch 331 finished \tANN training loss 0.042176\n",
      ">> Epoch 332 finished \tANN training loss 0.042089\n",
      ">> Epoch 333 finished \tANN training loss 0.042177\n",
      ">> Epoch 334 finished \tANN training loss 0.042323\n",
      ">> Epoch 335 finished \tANN training loss 0.042595\n",
      ">> Epoch 336 finished \tANN training loss 0.042974\n",
      ">> Epoch 337 finished \tANN training loss 0.042566\n",
      ">> Epoch 338 finished \tANN training loss 0.042588\n",
      ">> Epoch 339 finished \tANN training loss 0.042419\n",
      ">> Epoch 340 finished \tANN training loss 0.042502\n",
      ">> Epoch 341 finished \tANN training loss 0.042452\n",
      ">> Epoch 342 finished \tANN training loss 0.042388\n",
      ">> Epoch 343 finished \tANN training loss 0.042349\n",
      ">> Epoch 344 finished \tANN training loss 0.042422\n",
      ">> Epoch 345 finished \tANN training loss 0.042151\n",
      ">> Epoch 346 finished \tANN training loss 0.042148\n",
      ">> Epoch 347 finished \tANN training loss 0.041918\n",
      ">> Epoch 348 finished \tANN training loss 0.042058\n",
      ">> Epoch 349 finished \tANN training loss 0.042212\n",
      ">> Epoch 350 finished \tANN training loss 0.042294\n",
      ">> Epoch 351 finished \tANN training loss 0.042434\n",
      ">> Epoch 352 finished \tANN training loss 0.042523\n",
      ">> Epoch 353 finished \tANN training loss 0.042342\n",
      ">> Epoch 354 finished \tANN training loss 0.042318\n",
      ">> Epoch 355 finished \tANN training loss 0.042412\n",
      ">> Epoch 356 finished \tANN training loss 0.042444\n",
      ">> Epoch 357 finished \tANN training loss 0.043033\n",
      ">> Epoch 358 finished \tANN training loss 0.042850\n",
      ">> Epoch 359 finished \tANN training loss 0.042591\n",
      ">> Epoch 360 finished \tANN training loss 0.042632\n",
      ">> Epoch 361 finished \tANN training loss 0.042417\n",
      ">> Epoch 362 finished \tANN training loss 0.042458\n",
      ">> Epoch 363 finished \tANN training loss 0.042521\n",
      ">> Epoch 364 finished \tANN training loss 0.042153\n",
      ">> Epoch 365 finished \tANN training loss 0.041977\n",
      ">> Epoch 366 finished \tANN training loss 0.041970\n",
      ">> Epoch 367 finished \tANN training loss 0.042097\n",
      ">> Epoch 368 finished \tANN training loss 0.041942\n",
      ">> Epoch 369 finished \tANN training loss 0.041985\n",
      ">> Epoch 370 finished \tANN training loss 0.042140\n",
      ">> Epoch 371 finished \tANN training loss 0.042319\n",
      ">> Epoch 372 finished \tANN training loss 0.043180\n",
      ">> Epoch 373 finished \tANN training loss 0.042332\n",
      ">> Epoch 374 finished \tANN training loss 0.041880\n",
      ">> Epoch 375 finished \tANN training loss 0.041764\n",
      ">> Epoch 376 finished \tANN training loss 0.041732\n",
      ">> Epoch 377 finished \tANN training loss 0.041764\n",
      ">> Epoch 378 finished \tANN training loss 0.041834\n",
      ">> Epoch 379 finished \tANN training loss 0.041735\n",
      ">> Epoch 380 finished \tANN training loss 0.041867\n",
      ">> Epoch 381 finished \tANN training loss 0.041891\n",
      ">> Epoch 382 finished \tANN training loss 0.042219\n",
      ">> Epoch 383 finished \tANN training loss 0.041937\n",
      ">> Epoch 384 finished \tANN training loss 0.041928\n",
      ">> Epoch 385 finished \tANN training loss 0.042386\n",
      ">> Epoch 386 finished \tANN training loss 0.045004\n",
      ">> Epoch 387 finished \tANN training loss 0.043615\n",
      ">> Epoch 388 finished \tANN training loss 0.042561\n",
      ">> Epoch 389 finished \tANN training loss 0.043196\n",
      ">> Epoch 390 finished \tANN training loss 0.042808\n",
      ">> Epoch 391 finished \tANN training loss 0.042324\n",
      ">> Epoch 392 finished \tANN training loss 0.042027\n",
      ">> Epoch 393 finished \tANN training loss 0.042542\n",
      ">> Epoch 394 finished \tANN training loss 0.041955\n",
      ">> Epoch 395 finished \tANN training loss 0.041852\n",
      ">> Epoch 396 finished \tANN training loss 0.042089\n",
      ">> Epoch 397 finished \tANN training loss 0.042304\n",
      ">> Epoch 398 finished \tANN training loss 0.042414\n",
      ">> Epoch 399 finished \tANN training loss 0.042321\n",
      ">> Epoch 400 finished \tANN training loss 0.042087\n",
      ">> Epoch 401 finished \tANN training loss 0.042164\n",
      ">> Epoch 402 finished \tANN training loss 0.041967\n",
      ">> Epoch 403 finished \tANN training loss 0.042140\n",
      ">> Epoch 404 finished \tANN training loss 0.042057\n",
      ">> Epoch 405 finished \tANN training loss 0.042192\n",
      ">> Epoch 406 finished \tANN training loss 0.042146\n",
      ">> Epoch 407 finished \tANN training loss 0.042006\n",
      ">> Epoch 408 finished \tANN training loss 0.042275\n",
      ">> Epoch 409 finished \tANN training loss 0.042369\n",
      ">> Epoch 410 finished \tANN training loss 0.042342\n",
      ">> Epoch 411 finished \tANN training loss 0.042324\n",
      ">> Epoch 412 finished \tANN training loss 0.042380\n",
      ">> Epoch 413 finished \tANN training loss 0.042585\n",
      ">> Epoch 414 finished \tANN training loss 0.042811\n",
      ">> Epoch 415 finished \tANN training loss 0.042265\n",
      ">> Epoch 416 finished \tANN training loss 0.042041\n",
      ">> Epoch 417 finished \tANN training loss 0.042044\n",
      ">> Epoch 418 finished \tANN training loss 0.042114\n",
      ">> Epoch 419 finished \tANN training loss 0.042432\n",
      ">> Epoch 420 finished \tANN training loss 0.042235\n",
      ">> Epoch 421 finished \tANN training loss 0.042359\n",
      ">> Epoch 422 finished \tANN training loss 0.042499\n",
      ">> Epoch 423 finished \tANN training loss 0.041975\n",
      ">> Epoch 424 finished \tANN training loss 0.041899\n",
      ">> Epoch 425 finished \tANN training loss 0.041898\n",
      ">> Epoch 426 finished \tANN training loss 0.041730\n",
      ">> Epoch 427 finished \tANN training loss 0.041939\n",
      ">> Epoch 428 finished \tANN training loss 0.041844\n",
      ">> Epoch 429 finished \tANN training loss 0.041713\n",
      ">> Epoch 430 finished \tANN training loss 0.041713\n",
      ">> Epoch 431 finished \tANN training loss 0.041845\n",
      ">> Epoch 432 finished \tANN training loss 0.041765\n",
      ">> Epoch 433 finished \tANN training loss 0.041742\n",
      ">> Epoch 434 finished \tANN training loss 0.041757\n",
      ">> Epoch 435 finished \tANN training loss 0.041714\n",
      ">> Epoch 436 finished \tANN training loss 0.041751\n",
      ">> Epoch 437 finished \tANN training loss 0.041929\n",
      ">> Epoch 438 finished \tANN training loss 0.041611\n",
      ">> Epoch 439 finished \tANN training loss 0.041413\n",
      ">> Epoch 440 finished \tANN training loss 0.041537\n",
      ">> Epoch 441 finished \tANN training loss 0.041568\n",
      ">> Epoch 442 finished \tANN training loss 0.041720\n",
      ">> Epoch 443 finished \tANN training loss 0.041836\n",
      ">> Epoch 444 finished \tANN training loss 0.042082\n",
      ">> Epoch 445 finished \tANN training loss 0.042472\n",
      ">> Epoch 446 finished \tANN training loss 0.041954\n",
      ">> Epoch 447 finished \tANN training loss 0.042521\n",
      ">> Epoch 448 finished \tANN training loss 0.042059\n",
      ">> Epoch 449 finished \tANN training loss 0.042103\n",
      ">> Epoch 450 finished \tANN training loss 0.041725\n",
      ">> Epoch 451 finished \tANN training loss 0.041662\n",
      ">> Epoch 452 finished \tANN training loss 0.041700\n",
      ">> Epoch 453 finished \tANN training loss 0.041647\n",
      ">> Epoch 454 finished \tANN training loss 0.042180\n",
      ">> Epoch 455 finished \tANN training loss 0.042240\n",
      ">> Epoch 456 finished \tANN training loss 0.042110\n",
      ">> Epoch 457 finished \tANN training loss 0.041711\n",
      ">> Epoch 458 finished \tANN training loss 0.041571\n",
      ">> Epoch 459 finished \tANN training loss 0.041452\n",
      ">> Epoch 460 finished \tANN training loss 0.041893\n",
      ">> Epoch 461 finished \tANN training loss 0.041489\n",
      ">> Epoch 462 finished \tANN training loss 0.041437\n",
      ">> Epoch 463 finished \tANN training loss 0.041595\n",
      ">> Epoch 464 finished \tANN training loss 0.041380\n",
      ">> Epoch 465 finished \tANN training loss 0.042107\n",
      ">> Epoch 466 finished \tANN training loss 0.041617\n",
      ">> Epoch 467 finished \tANN training loss 0.041551\n",
      ">> Epoch 468 finished \tANN training loss 0.042360\n",
      ">> Epoch 469 finished \tANN training loss 0.041970\n",
      ">> Epoch 470 finished \tANN training loss 0.041667\n",
      ">> Epoch 471 finished \tANN training loss 0.041589\n",
      ">> Epoch 472 finished \tANN training loss 0.041835\n",
      ">> Epoch 473 finished \tANN training loss 0.041758\n",
      ">> Epoch 474 finished \tANN training loss 0.041943\n",
      ">> Epoch 475 finished \tANN training loss 0.041923\n",
      ">> Epoch 476 finished \tANN training loss 0.041838\n",
      ">> Epoch 477 finished \tANN training loss 0.041719\n",
      ">> Epoch 478 finished \tANN training loss 0.041780\n",
      ">> Epoch 479 finished \tANN training loss 0.041649\n",
      ">> Epoch 480 finished \tANN training loss 0.041489\n",
      ">> Epoch 481 finished \tANN training loss 0.041524\n",
      ">> Epoch 482 finished \tANN training loss 0.041365\n",
      ">> Epoch 483 finished \tANN training loss 0.042924\n",
      ">> Epoch 484 finished \tANN training loss 0.041556\n",
      ">> Epoch 485 finished \tANN training loss 0.041588\n",
      ">> Epoch 486 finished \tANN training loss 0.041525\n",
      ">> Epoch 487 finished \tANN training loss 0.041426\n",
      ">> Epoch 488 finished \tANN training loss 0.041321\n",
      ">> Epoch 489 finished \tANN training loss 0.041374\n",
      ">> Epoch 490 finished \tANN training loss 0.041278\n",
      ">> Epoch 491 finished \tANN training loss 0.041132\n",
      ">> Epoch 492 finished \tANN training loss 0.040916\n",
      ">> Epoch 493 finished \tANN training loss 0.040935\n",
      ">> Epoch 494 finished \tANN training loss 0.041038\n",
      ">> Epoch 495 finished \tANN training loss 0.041078\n",
      ">> Epoch 496 finished \tANN training loss 0.041075\n",
      ">> Epoch 497 finished \tANN training loss 0.041308\n",
      ">> Epoch 498 finished \tANN training loss 0.041371\n",
      ">> Epoch 499 finished \tANN training loss 0.041668\n",
      ">> Epoch 500 finished \tANN training loss 0.041792\n",
      ">> Epoch 501 finished \tANN training loss 0.041622\n",
      ">> Epoch 502 finished \tANN training loss 0.041451\n",
      ">> Epoch 503 finished \tANN training loss 0.041294\n",
      ">> Epoch 504 finished \tANN training loss 0.041058\n",
      ">> Epoch 505 finished \tANN training loss 0.040952\n",
      ">> Epoch 506 finished \tANN training loss 0.040883\n",
      ">> Epoch 507 finished \tANN training loss 0.040983\n",
      ">> Epoch 508 finished \tANN training loss 0.040892\n",
      ">> Epoch 509 finished \tANN training loss 0.040815\n",
      ">> Epoch 510 finished \tANN training loss 0.040754\n",
      ">> Epoch 511 finished \tANN training loss 0.040790\n",
      ">> Epoch 512 finished \tANN training loss 0.040841\n",
      ">> Epoch 513 finished \tANN training loss 0.040795\n",
      ">> Epoch 514 finished \tANN training loss 0.040980\n",
      ">> Epoch 515 finished \tANN training loss 0.040793\n",
      ">> Epoch 516 finished \tANN training loss 0.041005\n",
      ">> Epoch 517 finished \tANN training loss 0.041417\n",
      ">> Epoch 518 finished \tANN training loss 0.040975\n",
      ">> Epoch 519 finished \tANN training loss 0.041033\n",
      ">> Epoch 520 finished \tANN training loss 0.041065\n",
      ">> Epoch 521 finished \tANN training loss 0.040982\n",
      ">> Epoch 522 finished \tANN training loss 0.040913\n",
      ">> Epoch 523 finished \tANN training loss 0.040946\n",
      ">> Epoch 524 finished \tANN training loss 0.040963\n",
      ">> Epoch 525 finished \tANN training loss 0.040880\n",
      ">> Epoch 526 finished \tANN training loss 0.041137\n",
      ">> Epoch 527 finished \tANN training loss 0.040971\n",
      ">> Epoch 528 finished \tANN training loss 0.040861\n",
      ">> Epoch 529 finished \tANN training loss 0.040863\n",
      ">> Epoch 530 finished \tANN training loss 0.040991\n",
      ">> Epoch 531 finished \tANN training loss 0.040843\n",
      ">> Epoch 532 finished \tANN training loss 0.040817\n",
      ">> Epoch 533 finished \tANN training loss 0.041002\n",
      ">> Epoch 534 finished \tANN training loss 0.041006\n",
      ">> Epoch 535 finished \tANN training loss 0.041122\n",
      ">> Epoch 536 finished \tANN training loss 0.040830\n",
      ">> Epoch 537 finished \tANN training loss 0.040795\n",
      ">> Epoch 538 finished \tANN training loss 0.040770\n",
      ">> Epoch 539 finished \tANN training loss 0.040680\n",
      ">> Epoch 540 finished \tANN training loss 0.040929\n",
      ">> Epoch 541 finished \tANN training loss 0.040968\n",
      ">> Epoch 542 finished \tANN training loss 0.040781\n",
      ">> Epoch 543 finished \tANN training loss 0.040702\n",
      ">> Epoch 544 finished \tANN training loss 0.040834\n",
      ">> Epoch 545 finished \tANN training loss 0.042334\n",
      ">> Epoch 546 finished \tANN training loss 0.041045\n",
      ">> Epoch 547 finished \tANN training loss 0.041089\n",
      ">> Epoch 548 finished \tANN training loss 0.040724\n",
      ">> Epoch 549 finished \tANN training loss 0.040591\n",
      ">> Epoch 550 finished \tANN training loss 0.040595\n",
      ">> Epoch 551 finished \tANN training loss 0.040481\n",
      ">> Epoch 552 finished \tANN training loss 0.040757\n",
      ">> Epoch 553 finished \tANN training loss 0.040793\n",
      ">> Epoch 554 finished \tANN training loss 0.040497\n",
      ">> Epoch 555 finished \tANN training loss 0.040541\n",
      ">> Epoch 556 finished \tANN training loss 0.040290\n",
      ">> Epoch 557 finished \tANN training loss 0.040332\n",
      ">> Epoch 558 finished \tANN training loss 0.040342\n",
      ">> Epoch 559 finished \tANN training loss 0.040339\n",
      ">> Epoch 560 finished \tANN training loss 0.040161\n",
      ">> Epoch 561 finished \tANN training loss 0.040119\n",
      ">> Epoch 562 finished \tANN training loss 0.040548\n",
      ">> Epoch 563 finished \tANN training loss 0.040386\n",
      ">> Epoch 564 finished \tANN training loss 0.039955\n",
      ">> Epoch 565 finished \tANN training loss 0.039818\n",
      ">> Epoch 566 finished \tANN training loss 0.039942\n",
      ">> Epoch 567 finished \tANN training loss 0.040101\n",
      ">> Epoch 568 finished \tANN training loss 0.039908\n",
      ">> Epoch 569 finished \tANN training loss 0.041039\n",
      ">> Epoch 570 finished \tANN training loss 0.039660\n",
      ">> Epoch 571 finished \tANN training loss 0.039699\n",
      ">> Epoch 572 finished \tANN training loss 0.039629\n",
      ">> Epoch 573 finished \tANN training loss 0.039618\n",
      ">> Epoch 574 finished \tANN training loss 0.039705\n",
      ">> Epoch 575 finished \tANN training loss 0.039810\n",
      ">> Epoch 576 finished \tANN training loss 0.039722\n",
      ">> Epoch 577 finished \tANN training loss 0.039819\n",
      ">> Epoch 578 finished \tANN training loss 0.039903\n",
      ">> Epoch 579 finished \tANN training loss 0.039941\n",
      ">> Epoch 580 finished \tANN training loss 0.040096\n",
      ">> Epoch 581 finished \tANN training loss 0.040027\n",
      ">> Epoch 582 finished \tANN training loss 0.040040\n",
      ">> Epoch 583 finished \tANN training loss 0.039686\n",
      ">> Epoch 584 finished \tANN training loss 0.039717\n",
      ">> Epoch 585 finished \tANN training loss 0.039814\n",
      ">> Epoch 586 finished \tANN training loss 0.039803\n",
      ">> Epoch 587 finished \tANN training loss 0.039730\n",
      ">> Epoch 588 finished \tANN training loss 0.039902\n",
      ">> Epoch 589 finished \tANN training loss 0.039898\n",
      ">> Epoch 590 finished \tANN training loss 0.039828\n",
      ">> Epoch 591 finished \tANN training loss 0.039759\n",
      ">> Epoch 592 finished \tANN training loss 0.039956\n",
      ">> Epoch 593 finished \tANN training loss 0.039747\n",
      ">> Epoch 594 finished \tANN training loss 0.039718\n",
      ">> Epoch 595 finished \tANN training loss 0.039623\n",
      ">> Epoch 596 finished \tANN training loss 0.039682\n",
      ">> Epoch 597 finished \tANN training loss 0.039995\n",
      ">> Epoch 598 finished \tANN training loss 0.039922\n",
      ">> Epoch 599 finished \tANN training loss 0.039733\n",
      ">> Epoch 600 finished \tANN training loss 0.040273\n",
      ">> Epoch 601 finished \tANN training loss 0.040168\n",
      ">> Epoch 602 finished \tANN training loss 0.039961\n",
      ">> Epoch 603 finished \tANN training loss 0.039815\n",
      ">> Epoch 604 finished \tANN training loss 0.040009\n",
      ">> Epoch 605 finished \tANN training loss 0.040397\n",
      ">> Epoch 606 finished \tANN training loss 0.040187\n",
      ">> Epoch 607 finished \tANN training loss 0.040451\n",
      ">> Epoch 608 finished \tANN training loss 0.040281\n",
      ">> Epoch 609 finished \tANN training loss 0.040684\n",
      ">> Epoch 610 finished \tANN training loss 0.039515\n",
      ">> Epoch 611 finished \tANN training loss 0.039655\n",
      ">> Epoch 612 finished \tANN training loss 0.041453\n",
      ">> Epoch 613 finished \tANN training loss 0.040345\n",
      ">> Epoch 614 finished \tANN training loss 0.039702\n",
      ">> Epoch 615 finished \tANN training loss 0.039775\n",
      ">> Epoch 616 finished \tANN training loss 0.039961\n",
      ">> Epoch 617 finished \tANN training loss 0.039737\n",
      ">> Epoch 618 finished \tANN training loss 0.039877\n",
      ">> Epoch 619 finished \tANN training loss 0.039743\n",
      ">> Epoch 620 finished \tANN training loss 0.039713\n",
      ">> Epoch 621 finished \tANN training loss 0.039748\n",
      ">> Epoch 622 finished \tANN training loss 0.039885\n",
      ">> Epoch 623 finished \tANN training loss 0.039968\n",
      ">> Epoch 624 finished \tANN training loss 0.039939\n",
      ">> Epoch 625 finished \tANN training loss 0.039963\n",
      ">> Epoch 626 finished \tANN training loss 0.040164\n",
      ">> Epoch 627 finished \tANN training loss 0.039983\n",
      ">> Epoch 628 finished \tANN training loss 0.039889\n",
      ">> Epoch 629 finished \tANN training loss 0.039652\n",
      ">> Epoch 630 finished \tANN training loss 0.039713\n",
      ">> Epoch 631 finished \tANN training loss 0.039613\n",
      ">> Epoch 632 finished \tANN training loss 0.040028\n",
      ">> Epoch 633 finished \tANN training loss 0.040021\n",
      ">> Epoch 634 finished \tANN training loss 0.040871\n",
      ">> Epoch 635 finished \tANN training loss 0.040865\n",
      ">> Epoch 636 finished \tANN training loss 0.040040\n",
      ">> Epoch 637 finished \tANN training loss 0.039951\n",
      ">> Epoch 638 finished \tANN training loss 0.039870\n",
      ">> Epoch 639 finished \tANN training loss 0.040227\n",
      ">> Epoch 640 finished \tANN training loss 0.039868\n",
      ">> Epoch 641 finished \tANN training loss 0.040089\n",
      ">> Epoch 642 finished \tANN training loss 0.039892\n",
      ">> Epoch 643 finished \tANN training loss 0.039765\n",
      ">> Epoch 644 finished \tANN training loss 0.039810\n",
      ">> Epoch 645 finished \tANN training loss 0.040205\n",
      ">> Epoch 646 finished \tANN training loss 0.040077\n",
      ">> Epoch 647 finished \tANN training loss 0.039768\n",
      ">> Epoch 648 finished \tANN training loss 0.040040\n",
      ">> Epoch 649 finished \tANN training loss 0.039882\n",
      ">> Epoch 650 finished \tANN training loss 0.039858\n",
      ">> Epoch 651 finished \tANN training loss 0.039772\n",
      ">> Epoch 652 finished \tANN training loss 0.039726\n",
      ">> Epoch 653 finished \tANN training loss 0.039778\n",
      ">> Epoch 654 finished \tANN training loss 0.039763\n",
      ">> Epoch 655 finished \tANN training loss 0.039797\n",
      ">> Epoch 656 finished \tANN training loss 0.039735\n",
      ">> Epoch 657 finished \tANN training loss 0.039707\n",
      ">> Epoch 658 finished \tANN training loss 0.040723\n",
      ">> Epoch 659 finished \tANN training loss 0.039858\n",
      ">> Epoch 660 finished \tANN training loss 0.039725\n",
      ">> Epoch 661 finished \tANN training loss 0.039890\n",
      ">> Epoch 662 finished \tANN training loss 0.040128\n",
      ">> Epoch 663 finished \tANN training loss 0.039762\n",
      ">> Epoch 664 finished \tANN training loss 0.039534\n",
      ">> Epoch 665 finished \tANN training loss 0.039870\n",
      ">> Epoch 666 finished \tANN training loss 0.039562\n",
      ">> Epoch 667 finished \tANN training loss 0.039531\n",
      ">> Epoch 668 finished \tANN training loss 0.039509\n",
      ">> Epoch 669 finished \tANN training loss 0.039448\n",
      ">> Epoch 670 finished \tANN training loss 0.039513\n",
      ">> Epoch 671 finished \tANN training loss 0.039471\n",
      ">> Epoch 672 finished \tANN training loss 0.039300\n",
      ">> Epoch 673 finished \tANN training loss 0.039531\n",
      ">> Epoch 674 finished \tANN training loss 0.039331\n",
      ">> Epoch 675 finished \tANN training loss 0.040818\n",
      ">> Epoch 676 finished \tANN training loss 0.039938\n",
      ">> Epoch 677 finished \tANN training loss 0.039536\n",
      ">> Epoch 678 finished \tANN training loss 0.039300\n",
      ">> Epoch 679 finished \tANN training loss 0.039517\n",
      ">> Epoch 680 finished \tANN training loss 0.040763\n",
      ">> Epoch 681 finished \tANN training loss 0.039882\n",
      ">> Epoch 682 finished \tANN training loss 0.039195\n",
      ">> Epoch 683 finished \tANN training loss 0.039170\n",
      ">> Epoch 684 finished \tANN training loss 0.039094\n",
      ">> Epoch 685 finished \tANN training loss 0.039086\n",
      ">> Epoch 686 finished \tANN training loss 0.039271\n",
      ">> Epoch 687 finished \tANN training loss 0.039132\n",
      ">> Epoch 688 finished \tANN training loss 0.040015\n",
      ">> Epoch 689 finished \tANN training loss 0.039193\n",
      ">> Epoch 690 finished \tANN training loss 0.039122\n",
      ">> Epoch 691 finished \tANN training loss 0.039218\n",
      ">> Epoch 692 finished \tANN training loss 0.039347\n",
      ">> Epoch 693 finished \tANN training loss 0.039016\n",
      ">> Epoch 694 finished \tANN training loss 0.039212\n",
      ">> Epoch 695 finished \tANN training loss 0.039236\n",
      ">> Epoch 696 finished \tANN training loss 0.039138\n",
      ">> Epoch 697 finished \tANN training loss 0.039291\n",
      ">> Epoch 698 finished \tANN training loss 0.039462\n",
      ">> Epoch 699 finished \tANN training loss 0.039405\n",
      ">> Epoch 700 finished \tANN training loss 0.039344\n",
      ">> Epoch 701 finished \tANN training loss 0.039214\n",
      ">> Epoch 702 finished \tANN training loss 0.039174\n",
      ">> Epoch 703 finished \tANN training loss 0.039420\n",
      ">> Epoch 704 finished \tANN training loss 0.039099\n",
      ">> Epoch 705 finished \tANN training loss 0.039086\n",
      ">> Epoch 706 finished \tANN training loss 0.039017\n",
      ">> Epoch 707 finished \tANN training loss 0.039127\n",
      ">> Epoch 708 finished \tANN training loss 0.039030\n",
      ">> Epoch 709 finished \tANN training loss 0.039112\n",
      ">> Epoch 710 finished \tANN training loss 0.039077\n",
      ">> Epoch 711 finished \tANN training loss 0.039221\n",
      ">> Epoch 712 finished \tANN training loss 0.039104\n",
      ">> Epoch 713 finished \tANN training loss 0.039151\n",
      ">> Epoch 714 finished \tANN training loss 0.039075\n",
      ">> Epoch 715 finished \tANN training loss 0.039500\n",
      ">> Epoch 716 finished \tANN training loss 0.039437\n",
      ">> Epoch 717 finished \tANN training loss 0.039390\n",
      ">> Epoch 718 finished \tANN training loss 0.039274\n",
      ">> Epoch 719 finished \tANN training loss 0.039338\n",
      ">> Epoch 720 finished \tANN training loss 0.039535\n",
      ">> Epoch 721 finished \tANN training loss 0.039196\n",
      ">> Epoch 722 finished \tANN training loss 0.039272\n",
      ">> Epoch 723 finished \tANN training loss 0.039499\n",
      ">> Epoch 724 finished \tANN training loss 0.040557\n",
      ">> Epoch 725 finished \tANN training loss 0.039364\n",
      ">> Epoch 726 finished \tANN training loss 0.039109\n",
      ">> Epoch 727 finished \tANN training loss 0.039121\n",
      ">> Epoch 728 finished \tANN training loss 0.039022\n",
      ">> Epoch 729 finished \tANN training loss 0.039048\n",
      ">> Epoch 730 finished \tANN training loss 0.039032\n",
      ">> Epoch 731 finished \tANN training loss 0.039059\n",
      ">> Epoch 732 finished \tANN training loss 0.039148\n",
      ">> Epoch 733 finished \tANN training loss 0.039491\n",
      ">> Epoch 734 finished \tANN training loss 0.039056\n",
      ">> Epoch 735 finished \tANN training loss 0.039121\n",
      ">> Epoch 736 finished \tANN training loss 0.039112\n",
      ">> Epoch 737 finished \tANN training loss 0.039054\n",
      ">> Epoch 738 finished \tANN training loss 0.039124\n",
      ">> Epoch 739 finished \tANN training loss 0.039066\n",
      ">> Epoch 740 finished \tANN training loss 0.039069\n",
      ">> Epoch 741 finished \tANN training loss 0.038850\n",
      ">> Epoch 742 finished \tANN training loss 0.038762\n",
      ">> Epoch 743 finished \tANN training loss 0.038930\n",
      ">> Epoch 744 finished \tANN training loss 0.038784\n",
      ">> Epoch 745 finished \tANN training loss 0.038924\n",
      ">> Epoch 746 finished \tANN training loss 0.038782\n",
      ">> Epoch 747 finished \tANN training loss 0.038861\n",
      ">> Epoch 748 finished \tANN training loss 0.039008\n",
      ">> Epoch 749 finished \tANN training loss 0.039454\n",
      ">> Epoch 750 finished \tANN training loss 0.038851\n",
      ">> Epoch 751 finished \tANN training loss 0.038893\n",
      ">> Epoch 752 finished \tANN training loss 0.038852\n",
      ">> Epoch 753 finished \tANN training loss 0.039115\n",
      ">> Epoch 754 finished \tANN training loss 0.038969\n",
      ">> Epoch 755 finished \tANN training loss 0.038870\n",
      ">> Epoch 756 finished \tANN training loss 0.038913\n",
      ">> Epoch 757 finished \tANN training loss 0.038882\n",
      ">> Epoch 758 finished \tANN training loss 0.038775\n",
      ">> Epoch 759 finished \tANN training loss 0.038748\n",
      ">> Epoch 760 finished \tANN training loss 0.038915\n",
      ">> Epoch 761 finished \tANN training loss 0.038875\n",
      ">> Epoch 762 finished \tANN training loss 0.038751\n",
      ">> Epoch 763 finished \tANN training loss 0.038634\n",
      ">> Epoch 764 finished \tANN training loss 0.038681\n",
      ">> Epoch 765 finished \tANN training loss 0.038633\n",
      ">> Epoch 766 finished \tANN training loss 0.038729\n",
      ">> Epoch 767 finished \tANN training loss 0.038969\n",
      ">> Epoch 768 finished \tANN training loss 0.038991\n",
      ">> Epoch 769 finished \tANN training loss 0.038949\n",
      ">> Epoch 770 finished \tANN training loss 0.038842\n",
      ">> Epoch 771 finished \tANN training loss 0.038875\n",
      ">> Epoch 772 finished \tANN training loss 0.038867\n",
      ">> Epoch 773 finished \tANN training loss 0.038980\n",
      ">> Epoch 774 finished \tANN training loss 0.039238\n",
      ">> Epoch 775 finished \tANN training loss 0.039598\n",
      ">> Epoch 776 finished \tANN training loss 0.039361\n",
      ">> Epoch 777 finished \tANN training loss 0.039354\n",
      ">> Epoch 778 finished \tANN training loss 0.039492\n",
      ">> Epoch 779 finished \tANN training loss 0.038925\n",
      ">> Epoch 780 finished \tANN training loss 0.038823\n",
      ">> Epoch 781 finished \tANN training loss 0.038874\n",
      ">> Epoch 782 finished \tANN training loss 0.038817\n",
      ">> Epoch 783 finished \tANN training loss 0.038805\n",
      ">> Epoch 784 finished \tANN training loss 0.041810\n",
      ">> Epoch 785 finished \tANN training loss 0.039803\n",
      ">> Epoch 786 finished \tANN training loss 0.039508\n",
      ">> Epoch 787 finished \tANN training loss 0.041908\n",
      ">> Epoch 788 finished \tANN training loss 0.039497\n",
      ">> Epoch 789 finished \tANN training loss 0.038801\n",
      ">> Epoch 790 finished \tANN training loss 0.038731\n",
      ">> Epoch 791 finished \tANN training loss 0.038664\n",
      ">> Epoch 792 finished \tANN training loss 0.038860\n",
      ">> Epoch 793 finished \tANN training loss 0.038818\n",
      ">> Epoch 794 finished \tANN training loss 0.039280\n",
      ">> Epoch 795 finished \tANN training loss 0.039112\n",
      ">> Epoch 796 finished \tANN training loss 0.039092\n",
      ">> Epoch 797 finished \tANN training loss 0.039119\n",
      ">> Epoch 798 finished \tANN training loss 0.039079\n",
      ">> Epoch 799 finished \tANN training loss 0.038983\n",
      ">> Epoch 800 finished \tANN training loss 0.038833\n",
      ">> Epoch 801 finished \tANN training loss 0.039055\n",
      ">> Epoch 802 finished \tANN training loss 0.040630\n",
      ">> Epoch 803 finished \tANN training loss 0.039216\n",
      ">> Epoch 804 finished \tANN training loss 0.039346\n",
      ">> Epoch 805 finished \tANN training loss 0.039377\n",
      ">> Epoch 806 finished \tANN training loss 0.039029\n",
      ">> Epoch 807 finished \tANN training loss 0.038917\n",
      ">> Epoch 808 finished \tANN training loss 0.039361\n",
      ">> Epoch 809 finished \tANN training loss 0.039214\n",
      ">> Epoch 810 finished \tANN training loss 0.039164\n",
      ">> Epoch 811 finished \tANN training loss 0.039008\n",
      ">> Epoch 812 finished \tANN training loss 0.043110\n",
      ">> Epoch 813 finished \tANN training loss 0.040639\n",
      ">> Epoch 814 finished \tANN training loss 0.039688\n",
      ">> Epoch 815 finished \tANN training loss 0.039510\n",
      ">> Epoch 816 finished \tANN training loss 0.038984\n",
      ">> Epoch 817 finished \tANN training loss 0.039115\n",
      ">> Epoch 818 finished \tANN training loss 0.039450\n",
      ">> Epoch 819 finished \tANN training loss 0.039150\n",
      ">> Epoch 820 finished \tANN training loss 0.039041\n",
      ">> Epoch 821 finished \tANN training loss 0.038882\n",
      ">> Epoch 822 finished \tANN training loss 0.039096\n",
      ">> Epoch 823 finished \tANN training loss 0.039728\n",
      ">> Epoch 824 finished \tANN training loss 0.038935\n",
      ">> Epoch 825 finished \tANN training loss 0.039110\n",
      ">> Epoch 826 finished \tANN training loss 0.040056\n",
      ">> Epoch 827 finished \tANN training loss 0.039001\n",
      ">> Epoch 828 finished \tANN training loss 0.038950\n",
      ">> Epoch 829 finished \tANN training loss 0.038974\n",
      ">> Epoch 830 finished \tANN training loss 0.039175\n",
      ">> Epoch 831 finished \tANN training loss 0.038899\n",
      ">> Epoch 832 finished \tANN training loss 0.038904\n",
      ">> Epoch 833 finished \tANN training loss 0.038803\n",
      ">> Epoch 834 finished \tANN training loss 0.038720\n",
      ">> Epoch 835 finished \tANN training loss 0.038542\n",
      ">> Epoch 836 finished \tANN training loss 0.038532\n",
      ">> Epoch 837 finished \tANN training loss 0.038543\n",
      ">> Epoch 838 finished \tANN training loss 0.038552\n",
      ">> Epoch 839 finished \tANN training loss 0.038635\n",
      ">> Epoch 840 finished \tANN training loss 0.038532\n",
      ">> Epoch 841 finished \tANN training loss 0.038573\n",
      ">> Epoch 842 finished \tANN training loss 0.038483\n",
      ">> Epoch 843 finished \tANN training loss 0.038491\n",
      ">> Epoch 844 finished \tANN training loss 0.038511\n",
      ">> Epoch 845 finished \tANN training loss 0.038699\n",
      ">> Epoch 846 finished \tANN training loss 0.038615\n",
      ">> Epoch 847 finished \tANN training loss 0.038887\n",
      ">> Epoch 848 finished \tANN training loss 0.038606\n",
      ">> Epoch 849 finished \tANN training loss 0.038510\n",
      ">> Epoch 850 finished \tANN training loss 0.038443\n",
      ">> Epoch 851 finished \tANN training loss 0.038537\n",
      ">> Epoch 852 finished \tANN training loss 0.038482\n",
      ">> Epoch 853 finished \tANN training loss 0.038474\n",
      ">> Epoch 854 finished \tANN training loss 0.040194\n",
      ">> Epoch 855 finished \tANN training loss 0.039095\n",
      ">> Epoch 856 finished \tANN training loss 0.039037\n",
      ">> Epoch 857 finished \tANN training loss 0.038907\n",
      ">> Epoch 858 finished \tANN training loss 0.038693\n",
      ">> Epoch 859 finished \tANN training loss 0.038877\n",
      ">> Epoch 860 finished \tANN training loss 0.038658\n",
      ">> Epoch 861 finished \tANN training loss 0.038540\n",
      ">> Epoch 862 finished \tANN training loss 0.038797\n",
      ">> Epoch 863 finished \tANN training loss 0.038648\n",
      ">> Epoch 864 finished \tANN training loss 0.038507\n",
      ">> Epoch 865 finished \tANN training loss 0.038584\n",
      ">> Epoch 866 finished \tANN training loss 0.038498\n",
      ">> Epoch 867 finished \tANN training loss 0.040143\n",
      ">> Epoch 868 finished \tANN training loss 0.038872\n",
      ">> Epoch 869 finished \tANN training loss 0.038905\n",
      ">> Epoch 870 finished \tANN training loss 0.038778\n",
      ">> Epoch 871 finished \tANN training loss 0.038884\n",
      ">> Epoch 872 finished \tANN training loss 0.038488\n",
      ">> Epoch 873 finished \tANN training loss 0.038399\n",
      ">> Epoch 874 finished \tANN training loss 0.038429\n",
      ">> Epoch 875 finished \tANN training loss 0.038439\n",
      ">> Epoch 876 finished \tANN training loss 0.038361\n",
      ">> Epoch 877 finished \tANN training loss 0.039095\n",
      ">> Epoch 878 finished \tANN training loss 0.038557\n",
      ">> Epoch 879 finished \tANN training loss 0.039136\n",
      ">> Epoch 880 finished \tANN training loss 0.038395\n",
      ">> Epoch 881 finished \tANN training loss 0.038492\n",
      ">> Epoch 882 finished \tANN training loss 0.038320\n",
      ">> Epoch 883 finished \tANN training loss 0.038216\n",
      ">> Epoch 884 finished \tANN training loss 0.038209\n",
      ">> Epoch 885 finished \tANN training loss 0.038314\n",
      ">> Epoch 886 finished \tANN training loss 0.040054\n",
      ">> Epoch 887 finished \tANN training loss 0.038300\n",
      ">> Epoch 888 finished \tANN training loss 0.038266\n",
      ">> Epoch 889 finished \tANN training loss 0.038335\n",
      ">> Epoch 890 finished \tANN training loss 0.038278\n",
      ">> Epoch 891 finished \tANN training loss 0.038706\n",
      ">> Epoch 892 finished \tANN training loss 0.038378\n",
      ">> Epoch 893 finished \tANN training loss 0.038270\n",
      ">> Epoch 894 finished \tANN training loss 0.038281\n",
      ">> Epoch 895 finished \tANN training loss 0.038252\n",
      ">> Epoch 896 finished \tANN training loss 0.038280\n",
      ">> Epoch 897 finished \tANN training loss 0.038309\n",
      ">> Epoch 898 finished \tANN training loss 0.038577\n",
      ">> Epoch 899 finished \tANN training loss 0.038440\n",
      ">> Epoch 900 finished \tANN training loss 0.041760\n",
      ">> Epoch 901 finished \tANN training loss 0.039791\n",
      ">> Epoch 902 finished \tANN training loss 0.038534\n",
      ">> Epoch 903 finished \tANN training loss 0.038395\n",
      ">> Epoch 904 finished \tANN training loss 0.038375\n",
      ">> Epoch 905 finished \tANN training loss 0.038305\n",
      ">> Epoch 906 finished \tANN training loss 0.038348\n",
      ">> Epoch 907 finished \tANN training loss 0.038476\n",
      ">> Epoch 908 finished \tANN training loss 0.038422\n",
      ">> Epoch 909 finished \tANN training loss 0.038295\n",
      ">> Epoch 910 finished \tANN training loss 0.038410\n",
      ">> Epoch 911 finished \tANN training loss 0.042652\n",
      ">> Epoch 912 finished \tANN training loss 0.038781\n",
      ">> Epoch 913 finished \tANN training loss 0.038973\n",
      ">> Epoch 914 finished \tANN training loss 0.038614\n",
      ">> Epoch 915 finished \tANN training loss 0.038581\n",
      ">> Epoch 916 finished \tANN training loss 0.038630\n",
      ">> Epoch 917 finished \tANN training loss 0.038561\n",
      ">> Epoch 918 finished \tANN training loss 0.038536\n",
      ">> Epoch 919 finished \tANN training loss 0.039339\n",
      ">> Epoch 920 finished \tANN training loss 0.039085\n",
      ">> Epoch 921 finished \tANN training loss 0.041195\n",
      ">> Epoch 922 finished \tANN training loss 0.039853\n",
      ">> Epoch 923 finished \tANN training loss 0.039017\n",
      ">> Epoch 924 finished \tANN training loss 0.038526\n",
      ">> Epoch 925 finished \tANN training loss 0.038619\n",
      ">> Epoch 926 finished \tANN training loss 0.038310\n",
      ">> Epoch 927 finished \tANN training loss 0.038077\n",
      ">> Epoch 928 finished \tANN training loss 0.038232\n",
      ">> Epoch 929 finished \tANN training loss 0.038156\n",
      ">> Epoch 930 finished \tANN training loss 0.038920\n",
      ">> Epoch 931 finished \tANN training loss 0.038175\n",
      ">> Epoch 932 finished \tANN training loss 0.038311\n",
      ">> Epoch 933 finished \tANN training loss 0.038010\n",
      ">> Epoch 934 finished \tANN training loss 0.037855\n",
      ">> Epoch 935 finished \tANN training loss 0.037892\n",
      ">> Epoch 936 finished \tANN training loss 0.037899\n",
      ">> Epoch 937 finished \tANN training loss 0.037919\n",
      ">> Epoch 938 finished \tANN training loss 0.037731\n",
      ">> Epoch 939 finished \tANN training loss 0.037565\n",
      ">> Epoch 940 finished \tANN training loss 0.037553\n",
      ">> Epoch 941 finished \tANN training loss 0.037800\n",
      ">> Epoch 942 finished \tANN training loss 0.037762\n",
      ">> Epoch 943 finished \tANN training loss 0.037834\n",
      ">> Epoch 944 finished \tANN training loss 0.038949\n",
      ">> Epoch 945 finished \tANN training loss 0.037864\n",
      ">> Epoch 946 finished \tANN training loss 0.037636\n",
      ">> Epoch 947 finished \tANN training loss 0.037546\n",
      ">> Epoch 948 finished \tANN training loss 0.038102\n",
      ">> Epoch 949 finished \tANN training loss 0.037446\n",
      ">> Epoch 950 finished \tANN training loss 0.037595\n",
      ">> Epoch 951 finished \tANN training loss 0.037629\n",
      ">> Epoch 952 finished \tANN training loss 0.037836\n",
      ">> Epoch 953 finished \tANN training loss 0.037814\n",
      ">> Epoch 954 finished \tANN training loss 0.037429\n",
      ">> Epoch 955 finished \tANN training loss 0.037691\n",
      ">> Epoch 956 finished \tANN training loss 0.037581\n",
      ">> Epoch 957 finished \tANN training loss 0.037633\n",
      ">> Epoch 958 finished \tANN training loss 0.037551\n",
      ">> Epoch 959 finished \tANN training loss 0.037546\n",
      ">> Epoch 960 finished \tANN training loss 0.037416\n",
      ">> Epoch 961 finished \tANN training loss 0.037736\n",
      ">> Epoch 962 finished \tANN training loss 0.037581\n",
      ">> Epoch 963 finished \tANN training loss 0.037523\n",
      ">> Epoch 964 finished \tANN training loss 0.037519\n",
      ">> Epoch 965 finished \tANN training loss 0.037460\n",
      ">> Epoch 966 finished \tANN training loss 0.037466\n",
      ">> Epoch 967 finished \tANN training loss 0.037579\n",
      ">> Epoch 968 finished \tANN training loss 0.037540\n",
      ">> Epoch 969 finished \tANN training loss 0.038478\n",
      ">> Epoch 970 finished \tANN training loss 0.037554\n",
      ">> Epoch 971 finished \tANN training loss 0.037430\n",
      ">> Epoch 972 finished \tANN training loss 0.037376\n",
      ">> Epoch 973 finished \tANN training loss 0.037339\n",
      ">> Epoch 974 finished \tANN training loss 0.037563\n",
      ">> Epoch 975 finished \tANN training loss 0.037425\n",
      ">> Epoch 976 finished \tANN training loss 0.037452\n",
      ">> Epoch 977 finished \tANN training loss 0.037526\n",
      ">> Epoch 978 finished \tANN training loss 0.037431\n",
      ">> Epoch 979 finished \tANN training loss 0.038255\n",
      ">> Epoch 980 finished \tANN training loss 0.037337\n",
      ">> Epoch 981 finished \tANN training loss 0.037672\n",
      ">> Epoch 982 finished \tANN training loss 0.037755\n",
      ">> Epoch 983 finished \tANN training loss 0.037604\n",
      ">> Epoch 984 finished \tANN training loss 0.037346\n",
      ">> Epoch 985 finished \tANN training loss 0.037289\n",
      ">> Epoch 986 finished \tANN training loss 0.037396\n",
      ">> Epoch 987 finished \tANN training loss 0.037501\n",
      ">> Epoch 988 finished \tANN training loss 0.037499\n",
      ">> Epoch 989 finished \tANN training loss 0.037377\n",
      ">> Epoch 990 finished \tANN training loss 0.037315\n",
      ">> Epoch 991 finished \tANN training loss 0.037276\n",
      ">> Epoch 992 finished \tANN training loss 0.037853\n",
      ">> Epoch 993 finished \tANN training loss 0.037782\n",
      ">> Epoch 994 finished \tANN training loss 0.037844\n",
      ">> Epoch 995 finished \tANN training loss 0.037807\n",
      ">> Epoch 996 finished \tANN training loss 0.037943\n",
      ">> Epoch 997 finished \tANN training loss 0.037886\n",
      ">> Epoch 998 finished \tANN training loss 0.037712\n",
      ">> Epoch 999 finished \tANN training loss 0.038095\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.475978\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.064126\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.725060\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.512172\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.383712\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.290134\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.205723\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.150040\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.126976\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.099271\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.088106\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.084982\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.084514\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.079365\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.074228\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.077435\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.074887\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.077065\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.073669\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.074900\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2.315615\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.860774\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.688929\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1.540001\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 1.294377\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.983731\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.763848\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.675057\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.609904\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.528146\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.476349\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.409985\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.354492\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.293943\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.262561\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.218902\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.181761\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.141916\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.109867\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.094011\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.616809\n",
      ">> Epoch 1 finished \tANN training loss 0.318327\n",
      ">> Epoch 2 finished \tANN training loss 0.195529\n",
      ">> Epoch 3 finished \tANN training loss 0.129798\n",
      ">> Epoch 4 finished \tANN training loss 0.097345\n",
      ">> Epoch 5 finished \tANN training loss 0.080940\n",
      ">> Epoch 6 finished \tANN training loss 0.070515\n",
      ">> Epoch 7 finished \tANN training loss 0.064052\n",
      ">> Epoch 8 finished \tANN training loss 0.061365\n",
      ">> Epoch 9 finished \tANN training loss 0.060552\n",
      ">> Epoch 10 finished \tANN training loss 0.060783\n",
      ">> Epoch 11 finished \tANN training loss 0.057845\n",
      ">> Epoch 12 finished \tANN training loss 0.056592\n",
      ">> Epoch 13 finished \tANN training loss 0.055708\n",
      ">> Epoch 14 finished \tANN training loss 0.054936\n",
      ">> Epoch 15 finished \tANN training loss 0.054736\n",
      ">> Epoch 16 finished \tANN training loss 0.054443\n",
      ">> Epoch 17 finished \tANN training loss 0.054250\n",
      ">> Epoch 18 finished \tANN training loss 0.054119\n",
      ">> Epoch 19 finished \tANN training loss 0.053698\n",
      ">> Epoch 20 finished \tANN training loss 0.053889\n",
      ">> Epoch 21 finished \tANN training loss 0.053502\n",
      ">> Epoch 22 finished \tANN training loss 0.052811\n",
      ">> Epoch 23 finished \tANN training loss 0.052701\n",
      ">> Epoch 24 finished \tANN training loss 0.052898\n",
      ">> Epoch 25 finished \tANN training loss 0.052986\n",
      ">> Epoch 26 finished \tANN training loss 0.052711\n",
      ">> Epoch 27 finished \tANN training loss 0.052855\n",
      ">> Epoch 28 finished \tANN training loss 0.051419\n",
      ">> Epoch 29 finished \tANN training loss 0.051399\n",
      ">> Epoch 30 finished \tANN training loss 0.051329\n",
      ">> Epoch 31 finished \tANN training loss 0.051823\n",
      ">> Epoch 32 finished \tANN training loss 0.051605\n",
      ">> Epoch 33 finished \tANN training loss 0.051272\n",
      ">> Epoch 34 finished \tANN training loss 0.051796\n",
      ">> Epoch 35 finished \tANN training loss 0.051035\n",
      ">> Epoch 36 finished \tANN training loss 0.050761\n",
      ">> Epoch 37 finished \tANN training loss 0.050765\n",
      ">> Epoch 38 finished \tANN training loss 0.050618\n",
      ">> Epoch 39 finished \tANN training loss 0.050761\n",
      ">> Epoch 40 finished \tANN training loss 0.050778\n",
      ">> Epoch 41 finished \tANN training loss 0.050598\n",
      ">> Epoch 42 finished \tANN training loss 0.050424\n",
      ">> Epoch 43 finished \tANN training loss 0.050373\n",
      ">> Epoch 44 finished \tANN training loss 0.050108\n",
      ">> Epoch 45 finished \tANN training loss 0.050050\n",
      ">> Epoch 46 finished \tANN training loss 0.050706\n",
      ">> Epoch 47 finished \tANN training loss 0.050680\n",
      ">> Epoch 48 finished \tANN training loss 0.050751\n",
      ">> Epoch 49 finished \tANN training loss 0.050018\n",
      ">> Epoch 50 finished \tANN training loss 0.049709\n",
      ">> Epoch 51 finished \tANN training loss 0.050123\n",
      ">> Epoch 52 finished \tANN training loss 0.049761\n",
      ">> Epoch 53 finished \tANN training loss 0.049765\n",
      ">> Epoch 54 finished \tANN training loss 0.049610\n",
      ">> Epoch 55 finished \tANN training loss 0.049466\n",
      ">> Epoch 56 finished \tANN training loss 0.049453\n",
      ">> Epoch 57 finished \tANN training loss 0.049426\n",
      ">> Epoch 58 finished \tANN training loss 0.049334\n",
      ">> Epoch 59 finished \tANN training loss 0.050315\n",
      ">> Epoch 60 finished \tANN training loss 0.049621\n",
      ">> Epoch 61 finished \tANN training loss 0.049575\n",
      ">> Epoch 62 finished \tANN training loss 0.050372\n",
      ">> Epoch 63 finished \tANN training loss 0.049548\n",
      ">> Epoch 64 finished \tANN training loss 0.049352\n",
      ">> Epoch 65 finished \tANN training loss 0.049481\n",
      ">> Epoch 66 finished \tANN training loss 0.049231\n",
      ">> Epoch 67 finished \tANN training loss 0.049282\n",
      ">> Epoch 68 finished \tANN training loss 0.049346\n",
      ">> Epoch 69 finished \tANN training loss 0.049036\n",
      ">> Epoch 70 finished \tANN training loss 0.049163\n",
      ">> Epoch 71 finished \tANN training loss 0.050137\n",
      ">> Epoch 72 finished \tANN training loss 0.050948\n",
      ">> Epoch 73 finished \tANN training loss 0.049728\n",
      ">> Epoch 74 finished \tANN training loss 0.049476\n",
      ">> Epoch 75 finished \tANN training loss 0.049380\n",
      ">> Epoch 76 finished \tANN training loss 0.049035\n",
      ">> Epoch 77 finished \tANN training loss 0.048515\n",
      ">> Epoch 78 finished \tANN training loss 0.049794\n",
      ">> Epoch 79 finished \tANN training loss 0.048960\n",
      ">> Epoch 80 finished \tANN training loss 0.048859\n",
      ">> Epoch 81 finished \tANN training loss 0.048647\n",
      ">> Epoch 82 finished \tANN training loss 0.048369\n",
      ">> Epoch 83 finished \tANN training loss 0.048428\n",
      ">> Epoch 84 finished \tANN training loss 0.048338\n",
      ">> Epoch 85 finished \tANN training loss 0.048528\n",
      ">> Epoch 86 finished \tANN training loss 0.048492\n",
      ">> Epoch 87 finished \tANN training loss 0.048387\n",
      ">> Epoch 88 finished \tANN training loss 0.048136\n",
      ">> Epoch 89 finished \tANN training loss 0.048172\n",
      ">> Epoch 90 finished \tANN training loss 0.048115\n",
      ">> Epoch 91 finished \tANN training loss 0.048376\n",
      ">> Epoch 92 finished \tANN training loss 0.047876\n",
      ">> Epoch 93 finished \tANN training loss 0.047972\n",
      ">> Epoch 94 finished \tANN training loss 0.047966\n",
      ">> Epoch 95 finished \tANN training loss 0.047961\n",
      ">> Epoch 96 finished \tANN training loss 0.048186\n",
      ">> Epoch 97 finished \tANN training loss 0.048101\n",
      ">> Epoch 98 finished \tANN training loss 0.047929\n",
      ">> Epoch 99 finished \tANN training loss 0.048144\n",
      ">> Epoch 100 finished \tANN training loss 0.047868\n",
      ">> Epoch 101 finished \tANN training loss 0.048042\n",
      ">> Epoch 102 finished \tANN training loss 0.047901\n",
      ">> Epoch 103 finished \tANN training loss 0.048217\n",
      ">> Epoch 104 finished \tANN training loss 0.047836\n",
      ">> Epoch 105 finished \tANN training loss 0.049197\n",
      ">> Epoch 106 finished \tANN training loss 0.048652\n",
      ">> Epoch 107 finished \tANN training loss 0.048235\n",
      ">> Epoch 108 finished \tANN training loss 0.048046\n",
      ">> Epoch 109 finished \tANN training loss 0.048306\n",
      ">> Epoch 110 finished \tANN training loss 0.048352\n",
      ">> Epoch 111 finished \tANN training loss 0.048811\n",
      ">> Epoch 112 finished \tANN training loss 0.048073\n",
      ">> Epoch 113 finished \tANN training loss 0.047795\n",
      ">> Epoch 114 finished \tANN training loss 0.047809\n",
      ">> Epoch 115 finished \tANN training loss 0.047622\n",
      ">> Epoch 116 finished \tANN training loss 0.048047\n",
      ">> Epoch 117 finished \tANN training loss 0.048022\n",
      ">> Epoch 118 finished \tANN training loss 0.047910\n",
      ">> Epoch 119 finished \tANN training loss 0.047936\n",
      ">> Epoch 120 finished \tANN training loss 0.047575\n",
      ">> Epoch 121 finished \tANN training loss 0.047739\n",
      ">> Epoch 122 finished \tANN training loss 0.047979\n",
      ">> Epoch 123 finished \tANN training loss 0.047936\n",
      ">> Epoch 124 finished \tANN training loss 0.047870\n",
      ">> Epoch 125 finished \tANN training loss 0.047782\n",
      ">> Epoch 126 finished \tANN training loss 0.047095\n",
      ">> Epoch 127 finished \tANN training loss 0.047034\n",
      ">> Epoch 128 finished \tANN training loss 0.047192\n",
      ">> Epoch 129 finished \tANN training loss 0.047265\n",
      ">> Epoch 130 finished \tANN training loss 0.047679\n",
      ">> Epoch 131 finished \tANN training loss 0.047491\n",
      ">> Epoch 132 finished \tANN training loss 0.047534\n",
      ">> Epoch 133 finished \tANN training loss 0.047605\n",
      ">> Epoch 134 finished \tANN training loss 0.047863\n",
      ">> Epoch 135 finished \tANN training loss 0.048118\n",
      ">> Epoch 136 finished \tANN training loss 0.047871\n",
      ">> Epoch 137 finished \tANN training loss 0.047460\n",
      ">> Epoch 138 finished \tANN training loss 0.047604\n",
      ">> Epoch 139 finished \tANN training loss 0.047630\n",
      ">> Epoch 140 finished \tANN training loss 0.047860\n",
      ">> Epoch 141 finished \tANN training loss 0.047389\n",
      ">> Epoch 142 finished \tANN training loss 0.047164\n",
      ">> Epoch 143 finished \tANN training loss 0.047166\n",
      ">> Epoch 144 finished \tANN training loss 0.047394\n",
      ">> Epoch 145 finished \tANN training loss 0.047316\n",
      ">> Epoch 146 finished \tANN training loss 0.046920\n",
      ">> Epoch 147 finished \tANN training loss 0.046959\n",
      ">> Epoch 148 finished \tANN training loss 0.047071\n",
      ">> Epoch 149 finished \tANN training loss 0.047225\n",
      ">> Epoch 150 finished \tANN training loss 0.047679\n",
      ">> Epoch 151 finished \tANN training loss 0.047372\n",
      ">> Epoch 152 finished \tANN training loss 0.047307\n",
      ">> Epoch 153 finished \tANN training loss 0.047212\n",
      ">> Epoch 154 finished \tANN training loss 0.046907\n",
      ">> Epoch 155 finished \tANN training loss 0.046904\n",
      ">> Epoch 156 finished \tANN training loss 0.047486\n",
      ">> Epoch 157 finished \tANN training loss 0.047447\n",
      ">> Epoch 158 finished \tANN training loss 0.046929\n",
      ">> Epoch 159 finished \tANN training loss 0.046865\n",
      ">> Epoch 160 finished \tANN training loss 0.046988\n",
      ">> Epoch 161 finished \tANN training loss 0.046974\n",
      ">> Epoch 162 finished \tANN training loss 0.046708\n",
      ">> Epoch 163 finished \tANN training loss 0.046820\n",
      ">> Epoch 164 finished \tANN training loss 0.046777\n",
      ">> Epoch 165 finished \tANN training loss 0.047187\n",
      ">> Epoch 166 finished \tANN training loss 0.046987\n",
      ">> Epoch 167 finished \tANN training loss 0.046781\n",
      ">> Epoch 168 finished \tANN training loss 0.046654\n",
      ">> Epoch 169 finished \tANN training loss 0.047014\n",
      ">> Epoch 170 finished \tANN training loss 0.046595\n",
      ">> Epoch 171 finished \tANN training loss 0.046607\n",
      ">> Epoch 172 finished \tANN training loss 0.046840\n",
      ">> Epoch 173 finished \tANN training loss 0.046481\n",
      ">> Epoch 174 finished \tANN training loss 0.046365\n",
      ">> Epoch 175 finished \tANN training loss 0.046538\n",
      ">> Epoch 176 finished \tANN training loss 0.046453\n",
      ">> Epoch 177 finished \tANN training loss 0.046673\n",
      ">> Epoch 178 finished \tANN training loss 0.046601\n",
      ">> Epoch 179 finished \tANN training loss 0.046864\n",
      ">> Epoch 180 finished \tANN training loss 0.046992\n",
      ">> Epoch 181 finished \tANN training loss 0.047279\n",
      ">> Epoch 182 finished \tANN training loss 0.046748\n",
      ">> Epoch 183 finished \tANN training loss 0.046606\n",
      ">> Epoch 184 finished \tANN training loss 0.046453\n",
      ">> Epoch 185 finished \tANN training loss 0.046098\n",
      ">> Epoch 186 finished \tANN training loss 0.046249\n",
      ">> Epoch 187 finished \tANN training loss 0.046284\n",
      ">> Epoch 188 finished \tANN training loss 0.046258\n",
      ">> Epoch 189 finished \tANN training loss 0.046395\n",
      ">> Epoch 190 finished \tANN training loss 0.046356\n",
      ">> Epoch 191 finished \tANN training loss 0.046854\n",
      ">> Epoch 192 finished \tANN training loss 0.046424\n",
      ">> Epoch 193 finished \tANN training loss 0.046257\n",
      ">> Epoch 194 finished \tANN training loss 0.046095\n",
      ">> Epoch 195 finished \tANN training loss 0.046109\n",
      ">> Epoch 196 finished \tANN training loss 0.046262\n",
      ">> Epoch 197 finished \tANN training loss 0.046204\n",
      ">> Epoch 198 finished \tANN training loss 0.046298\n",
      ">> Epoch 199 finished \tANN training loss 0.046661\n",
      ">> Epoch 200 finished \tANN training loss 0.046384\n",
      ">> Epoch 201 finished \tANN training loss 0.045848\n",
      ">> Epoch 202 finished \tANN training loss 0.045712\n",
      ">> Epoch 203 finished \tANN training loss 0.045816\n",
      ">> Epoch 204 finished \tANN training loss 0.045950\n",
      ">> Epoch 205 finished \tANN training loss 0.045986\n",
      ">> Epoch 206 finished \tANN training loss 0.046343\n",
      ">> Epoch 207 finished \tANN training loss 0.046264\n",
      ">> Epoch 208 finished \tANN training loss 0.046238\n",
      ">> Epoch 209 finished \tANN training loss 0.046406\n",
      ">> Epoch 210 finished \tANN training loss 0.045921\n",
      ">> Epoch 211 finished \tANN training loss 0.045971\n",
      ">> Epoch 212 finished \tANN training loss 0.046196\n",
      ">> Epoch 213 finished \tANN training loss 0.045724\n",
      ">> Epoch 214 finished \tANN training loss 0.046099\n",
      ">> Epoch 215 finished \tANN training loss 0.046569\n",
      ">> Epoch 216 finished \tANN training loss 0.045998\n",
      ">> Epoch 217 finished \tANN training loss 0.045702\n",
      ">> Epoch 218 finished \tANN training loss 0.045625\n",
      ">> Epoch 219 finished \tANN training loss 0.046051\n",
      ">> Epoch 220 finished \tANN training loss 0.045852\n",
      ">> Epoch 221 finished \tANN training loss 0.045846\n",
      ">> Epoch 222 finished \tANN training loss 0.045715\n",
      ">> Epoch 223 finished \tANN training loss 0.046374\n",
      ">> Epoch 224 finished \tANN training loss 0.045748\n",
      ">> Epoch 225 finished \tANN training loss 0.045638\n",
      ">> Epoch 226 finished \tANN training loss 0.045657\n",
      ">> Epoch 227 finished \tANN training loss 0.047953\n",
      ">> Epoch 228 finished \tANN training loss 0.047021\n",
      ">> Epoch 229 finished \tANN training loss 0.046987\n",
      ">> Epoch 230 finished \tANN training loss 0.046180\n",
      ">> Epoch 231 finished \tANN training loss 0.045979\n",
      ">> Epoch 232 finished \tANN training loss 0.045892\n",
      ">> Epoch 233 finished \tANN training loss 0.045734\n",
      ">> Epoch 234 finished \tANN training loss 0.045524\n",
      ">> Epoch 235 finished \tANN training loss 0.045647\n",
      ">> Epoch 236 finished \tANN training loss 0.045784\n",
      ">> Epoch 237 finished \tANN training loss 0.045687\n",
      ">> Epoch 238 finished \tANN training loss 0.045637\n",
      ">> Epoch 239 finished \tANN training loss 0.045705\n",
      ">> Epoch 240 finished \tANN training loss 0.045518\n",
      ">> Epoch 241 finished \tANN training loss 0.045599\n",
      ">> Epoch 242 finished \tANN training loss 0.045835\n",
      ">> Epoch 243 finished \tANN training loss 0.046076\n",
      ">> Epoch 244 finished \tANN training loss 0.046305\n",
      ">> Epoch 245 finished \tANN training loss 0.046288\n",
      ">> Epoch 246 finished \tANN training loss 0.045389\n",
      ">> Epoch 247 finished \tANN training loss 0.045492\n",
      ">> Epoch 248 finished \tANN training loss 0.045455\n",
      ">> Epoch 249 finished \tANN training loss 0.045371\n",
      ">> Epoch 250 finished \tANN training loss 0.045503\n",
      ">> Epoch 251 finished \tANN training loss 0.045533\n",
      ">> Epoch 252 finished \tANN training loss 0.045599\n",
      ">> Epoch 253 finished \tANN training loss 0.045399\n",
      ">> Epoch 254 finished \tANN training loss 0.045271\n",
      ">> Epoch 255 finished \tANN training loss 0.045415\n",
      ">> Epoch 256 finished \tANN training loss 0.045322\n",
      ">> Epoch 257 finished \tANN training loss 0.045334\n",
      ">> Epoch 258 finished \tANN training loss 0.045561\n",
      ">> Epoch 259 finished \tANN training loss 0.045555\n",
      ">> Epoch 260 finished \tANN training loss 0.045336\n",
      ">> Epoch 261 finished \tANN training loss 0.045332\n",
      ">> Epoch 262 finished \tANN training loss 0.045918\n",
      ">> Epoch 263 finished \tANN training loss 0.045384\n",
      ">> Epoch 264 finished \tANN training loss 0.045316\n",
      ">> Epoch 265 finished \tANN training loss 0.046350\n",
      ">> Epoch 266 finished \tANN training loss 0.044868\n",
      ">> Epoch 267 finished \tANN training loss 0.045655\n",
      ">> Epoch 268 finished \tANN training loss 0.044583\n",
      ">> Epoch 269 finished \tANN training loss 0.044895\n",
      ">> Epoch 270 finished \tANN training loss 0.044682\n",
      ">> Epoch 271 finished \tANN training loss 0.045733\n",
      ">> Epoch 272 finished \tANN training loss 0.047176\n",
      ">> Epoch 273 finished \tANN training loss 0.045952\n",
      ">> Epoch 274 finished \tANN training loss 0.045299\n",
      ">> Epoch 275 finished \tANN training loss 0.044766\n",
      ">> Epoch 276 finished \tANN training loss 0.044651\n",
      ">> Epoch 277 finished \tANN training loss 0.044402\n",
      ">> Epoch 278 finished \tANN training loss 0.044405\n",
      ">> Epoch 279 finished \tANN training loss 0.045371\n",
      ">> Epoch 280 finished \tANN training loss 0.045264\n",
      ">> Epoch 281 finished \tANN training loss 0.044715\n",
      ">> Epoch 282 finished \tANN training loss 0.044817\n",
      ">> Epoch 283 finished \tANN training loss 0.044545\n",
      ">> Epoch 284 finished \tANN training loss 0.044415\n",
      ">> Epoch 285 finished \tANN training loss 0.044160\n",
      ">> Epoch 286 finished \tANN training loss 0.044267\n",
      ">> Epoch 287 finished \tANN training loss 0.043990\n",
      ">> Epoch 288 finished \tANN training loss 0.043781\n",
      ">> Epoch 289 finished \tANN training loss 0.043888\n",
      ">> Epoch 290 finished \tANN training loss 0.043931\n",
      ">> Epoch 291 finished \tANN training loss 0.044433\n",
      ">> Epoch 292 finished \tANN training loss 0.044083\n",
      ">> Epoch 293 finished \tANN training loss 0.043896\n",
      ">> Epoch 294 finished \tANN training loss 0.043770\n",
      ">> Epoch 295 finished \tANN training loss 0.043745\n",
      ">> Epoch 296 finished \tANN training loss 0.043806\n",
      ">> Epoch 297 finished \tANN training loss 0.044052\n",
      ">> Epoch 298 finished \tANN training loss 0.043913\n",
      ">> Epoch 299 finished \tANN training loss 0.043954\n",
      ">> Epoch 300 finished \tANN training loss 0.043859\n",
      ">> Epoch 301 finished \tANN training loss 0.043800\n",
      ">> Epoch 302 finished \tANN training loss 0.043938\n",
      ">> Epoch 303 finished \tANN training loss 0.044038\n",
      ">> Epoch 304 finished \tANN training loss 0.044216\n",
      ">> Epoch 305 finished \tANN training loss 0.044577\n",
      ">> Epoch 306 finished \tANN training loss 0.043878\n",
      ">> Epoch 307 finished \tANN training loss 0.044008\n",
      ">> Epoch 308 finished \tANN training loss 0.043573\n",
      ">> Epoch 309 finished \tANN training loss 0.044157\n",
      ">> Epoch 310 finished \tANN training loss 0.044059\n",
      ">> Epoch 311 finished \tANN training loss 0.044336\n",
      ">> Epoch 312 finished \tANN training loss 0.043946\n",
      ">> Epoch 313 finished \tANN training loss 0.043659\n",
      ">> Epoch 314 finished \tANN training loss 0.043583\n",
      ">> Epoch 315 finished \tANN training loss 0.043950\n",
      ">> Epoch 316 finished \tANN training loss 0.043854\n",
      ">> Epoch 317 finished \tANN training loss 0.043662\n",
      ">> Epoch 318 finished \tANN training loss 0.043472\n",
      ">> Epoch 319 finished \tANN training loss 0.043376\n",
      ">> Epoch 320 finished \tANN training loss 0.043719\n",
      ">> Epoch 321 finished \tANN training loss 0.043660\n",
      ">> Epoch 322 finished \tANN training loss 0.043564\n",
      ">> Epoch 323 finished \tANN training loss 0.043567\n",
      ">> Epoch 324 finished \tANN training loss 0.043648\n",
      ">> Epoch 325 finished \tANN training loss 0.043599\n",
      ">> Epoch 326 finished \tANN training loss 0.043378\n",
      ">> Epoch 327 finished \tANN training loss 0.043534\n",
      ">> Epoch 328 finished \tANN training loss 0.043460\n",
      ">> Epoch 329 finished \tANN training loss 0.043722\n",
      ">> Epoch 330 finished \tANN training loss 0.043667\n",
      ">> Epoch 331 finished \tANN training loss 0.043412\n",
      ">> Epoch 332 finished \tANN training loss 0.043539\n",
      ">> Epoch 333 finished \tANN training loss 0.043835\n",
      ">> Epoch 334 finished \tANN training loss 0.043567\n",
      ">> Epoch 335 finished \tANN training loss 0.043584\n",
      ">> Epoch 336 finished \tANN training loss 0.043156\n",
      ">> Epoch 337 finished \tANN training loss 0.043385\n",
      ">> Epoch 338 finished \tANN training loss 0.043300\n",
      ">> Epoch 339 finished \tANN training loss 0.043364\n",
      ">> Epoch 340 finished \tANN training loss 0.043421\n",
      ">> Epoch 341 finished \tANN training loss 0.043366\n",
      ">> Epoch 342 finished \tANN training loss 0.043712\n",
      ">> Epoch 343 finished \tANN training loss 0.044136\n",
      ">> Epoch 344 finished \tANN training loss 0.043858\n",
      ">> Epoch 345 finished \tANN training loss 0.043526\n",
      ">> Epoch 346 finished \tANN training loss 0.044115\n",
      ">> Epoch 347 finished \tANN training loss 0.043168\n",
      ">> Epoch 348 finished \tANN training loss 0.044203\n",
      ">> Epoch 349 finished \tANN training loss 0.045231\n",
      ">> Epoch 350 finished \tANN training loss 0.044746\n",
      ">> Epoch 351 finished \tANN training loss 0.044260\n",
      ">> Epoch 352 finished \tANN training loss 0.044219\n",
      ">> Epoch 353 finished \tANN training loss 0.043556\n",
      ">> Epoch 354 finished \tANN training loss 0.043776\n",
      ">> Epoch 355 finished \tANN training loss 0.043506\n",
      ">> Epoch 356 finished \tANN training loss 0.043746\n",
      ">> Epoch 357 finished \tANN training loss 0.043070\n",
      ">> Epoch 358 finished \tANN training loss 0.043328\n",
      ">> Epoch 359 finished \tANN training loss 0.043417\n",
      ">> Epoch 360 finished \tANN training loss 0.043260\n",
      ">> Epoch 361 finished \tANN training loss 0.043987\n",
      ">> Epoch 362 finished \tANN training loss 0.043814\n",
      ">> Epoch 363 finished \tANN training loss 0.043291\n",
      ">> Epoch 364 finished \tANN training loss 0.043328\n",
      ">> Epoch 365 finished \tANN training loss 0.045189\n",
      ">> Epoch 366 finished \tANN training loss 0.047322\n",
      ">> Epoch 367 finished \tANN training loss 0.045017\n",
      ">> Epoch 368 finished \tANN training loss 0.043715\n",
      ">> Epoch 369 finished \tANN training loss 0.042924\n",
      ">> Epoch 370 finished \tANN training loss 0.043001\n",
      ">> Epoch 371 finished \tANN training loss 0.042953\n",
      ">> Epoch 372 finished \tANN training loss 0.043073\n",
      ">> Epoch 373 finished \tANN training loss 0.043215\n",
      ">> Epoch 374 finished \tANN training loss 0.042741\n",
      ">> Epoch 375 finished \tANN training loss 0.043034\n",
      ">> Epoch 376 finished \tANN training loss 0.043170\n",
      ">> Epoch 377 finished \tANN training loss 0.042946\n",
      ">> Epoch 378 finished \tANN training loss 0.042414\n",
      ">> Epoch 379 finished \tANN training loss 0.042428\n",
      ">> Epoch 380 finished \tANN training loss 0.042594\n",
      ">> Epoch 381 finished \tANN training loss 0.042585\n",
      ">> Epoch 382 finished \tANN training loss 0.042793\n",
      ">> Epoch 383 finished \tANN training loss 0.042505\n",
      ">> Epoch 384 finished \tANN training loss 0.042554\n",
      ">> Epoch 385 finished \tANN training loss 0.042550\n",
      ">> Epoch 386 finished \tANN training loss 0.042741\n",
      ">> Epoch 387 finished \tANN training loss 0.042423\n",
      ">> Epoch 388 finished \tANN training loss 0.042382\n",
      ">> Epoch 389 finished \tANN training loss 0.042328\n",
      ">> Epoch 390 finished \tANN training loss 0.042881\n",
      ">> Epoch 391 finished \tANN training loss 0.042643\n",
      ">> Epoch 392 finished \tANN training loss 0.042415\n",
      ">> Epoch 393 finished \tANN training loss 0.043171\n",
      ">> Epoch 394 finished \tANN training loss 0.042380\n",
      ">> Epoch 395 finished \tANN training loss 0.042727\n",
      ">> Epoch 396 finished \tANN training loss 0.042546\n",
      ">> Epoch 397 finished \tANN training loss 0.042523\n",
      ">> Epoch 398 finished \tANN training loss 0.042454\n",
      ">> Epoch 399 finished \tANN training loss 0.042350\n",
      ">> Epoch 400 finished \tANN training loss 0.042404\n",
      ">> Epoch 401 finished \tANN training loss 0.042575\n",
      ">> Epoch 402 finished \tANN training loss 0.042506\n",
      ">> Epoch 403 finished \tANN training loss 0.042860\n",
      ">> Epoch 404 finished \tANN training loss 0.042647\n",
      ">> Epoch 405 finished \tANN training loss 0.042774\n",
      ">> Epoch 406 finished \tANN training loss 0.042854\n",
      ">> Epoch 407 finished \tANN training loss 0.042739\n",
      ">> Epoch 408 finished \tANN training loss 0.043186\n",
      ">> Epoch 409 finished \tANN training loss 0.043085\n",
      ">> Epoch 410 finished \tANN training loss 0.042537\n",
      ">> Epoch 411 finished \tANN training loss 0.043549\n",
      ">> Epoch 412 finished \tANN training loss 0.043053\n",
      ">> Epoch 413 finished \tANN training loss 0.042822\n",
      ">> Epoch 414 finished \tANN training loss 0.042460\n",
      ">> Epoch 415 finished \tANN training loss 0.042589\n",
      ">> Epoch 416 finished \tANN training loss 0.042411\n",
      ">> Epoch 417 finished \tANN training loss 0.042383\n",
      ">> Epoch 418 finished \tANN training loss 0.042289\n",
      ">> Epoch 419 finished \tANN training loss 0.042336\n",
      ">> Epoch 420 finished \tANN training loss 0.042387\n",
      ">> Epoch 421 finished \tANN training loss 0.042005\n",
      ">> Epoch 422 finished \tANN training loss 0.042334\n",
      ">> Epoch 423 finished \tANN training loss 0.042113\n",
      ">> Epoch 424 finished \tANN training loss 0.042101\n",
      ">> Epoch 425 finished \tANN training loss 0.042524\n",
      ">> Epoch 426 finished \tANN training loss 0.042120\n",
      ">> Epoch 427 finished \tANN training loss 0.042217\n",
      ">> Epoch 428 finished \tANN training loss 0.042003\n",
      ">> Epoch 429 finished \tANN training loss 0.042057\n",
      ">> Epoch 430 finished \tANN training loss 0.042474\n",
      ">> Epoch 431 finished \tANN training loss 0.042179\n",
      ">> Epoch 432 finished \tANN training loss 0.042162\n",
      ">> Epoch 433 finished \tANN training loss 0.042206\n",
      ">> Epoch 434 finished \tANN training loss 0.043127\n",
      ">> Epoch 435 finished \tANN training loss 0.042388\n",
      ">> Epoch 436 finished \tANN training loss 0.043127\n",
      ">> Epoch 437 finished \tANN training loss 0.043492\n",
      ">> Epoch 438 finished \tANN training loss 0.042348\n",
      ">> Epoch 439 finished \tANN training loss 0.042782\n",
      ">> Epoch 440 finished \tANN training loss 0.042309\n",
      ">> Epoch 441 finished \tANN training loss 0.042301\n",
      ">> Epoch 442 finished \tANN training loss 0.042444\n",
      ">> Epoch 443 finished \tANN training loss 0.042244\n",
      ">> Epoch 444 finished \tANN training loss 0.042417\n",
      ">> Epoch 445 finished \tANN training loss 0.042510\n",
      ">> Epoch 446 finished \tANN training loss 0.042437\n",
      ">> Epoch 447 finished \tANN training loss 0.042133\n",
      ">> Epoch 448 finished \tANN training loss 0.042520\n",
      ">> Epoch 449 finished \tANN training loss 0.042249\n",
      ">> Epoch 450 finished \tANN training loss 0.042321\n",
      ">> Epoch 451 finished \tANN training loss 0.042257\n",
      ">> Epoch 452 finished \tANN training loss 0.042229\n",
      ">> Epoch 453 finished \tANN training loss 0.042337\n",
      ">> Epoch 454 finished \tANN training loss 0.042424\n",
      ">> Epoch 455 finished \tANN training loss 0.042235\n",
      ">> Epoch 456 finished \tANN training loss 0.042612\n",
      ">> Epoch 457 finished \tANN training loss 0.042419\n",
      ">> Epoch 458 finished \tANN training loss 0.042323\n",
      ">> Epoch 459 finished \tANN training loss 0.042289\n",
      ">> Epoch 460 finished \tANN training loss 0.042609\n",
      ">> Epoch 461 finished \tANN training loss 0.042513\n",
      ">> Epoch 462 finished \tANN training loss 0.042552\n",
      ">> Epoch 463 finished \tANN training loss 0.042840\n",
      ">> Epoch 464 finished \tANN training loss 0.043090\n",
      ">> Epoch 465 finished \tANN training loss 0.042429\n",
      ">> Epoch 466 finished \tANN training loss 0.042405\n",
      ">> Epoch 467 finished \tANN training loss 0.042257\n",
      ">> Epoch 468 finished \tANN training loss 0.042256\n",
      ">> Epoch 469 finished \tANN training loss 0.042345\n",
      ">> Epoch 470 finished \tANN training loss 0.042581\n",
      ">> Epoch 471 finished \tANN training loss 0.042331\n",
      ">> Epoch 472 finished \tANN training loss 0.042467\n",
      ">> Epoch 473 finished \tANN training loss 0.042302\n",
      ">> Epoch 474 finished \tANN training loss 0.042206\n",
      ">> Epoch 475 finished \tANN training loss 0.042281\n",
      ">> Epoch 476 finished \tANN training loss 0.042374\n",
      ">> Epoch 477 finished \tANN training loss 0.042383\n",
      ">> Epoch 478 finished \tANN training loss 0.042047\n",
      ">> Epoch 479 finished \tANN training loss 0.042147\n",
      ">> Epoch 480 finished \tANN training loss 0.042379\n",
      ">> Epoch 481 finished \tANN training loss 0.042430\n",
      ">> Epoch 482 finished \tANN training loss 0.042345\n",
      ">> Epoch 483 finished \tANN training loss 0.042851\n",
      ">> Epoch 484 finished \tANN training loss 0.042489\n",
      ">> Epoch 485 finished \tANN training loss 0.043253\n",
      ">> Epoch 486 finished \tANN training loss 0.043140\n",
      ">> Epoch 487 finished \tANN training loss 0.042807\n",
      ">> Epoch 488 finished \tANN training loss 0.042482\n",
      ">> Epoch 489 finished \tANN training loss 0.042657\n",
      ">> Epoch 490 finished \tANN training loss 0.042327\n",
      ">> Epoch 491 finished \tANN training loss 0.042288\n",
      ">> Epoch 492 finished \tANN training loss 0.042264\n",
      ">> Epoch 493 finished \tANN training loss 0.042091\n",
      ">> Epoch 494 finished \tANN training loss 0.042186\n",
      ">> Epoch 495 finished \tANN training loss 0.041952\n",
      ">> Epoch 496 finished \tANN training loss 0.042308\n",
      ">> Epoch 497 finished \tANN training loss 0.042296\n",
      ">> Epoch 498 finished \tANN training loss 0.041817\n",
      ">> Epoch 499 finished \tANN training loss 0.041934\n",
      ">> Epoch 500 finished \tANN training loss 0.041627\n",
      ">> Epoch 501 finished \tANN training loss 0.041810\n",
      ">> Epoch 502 finished \tANN training loss 0.041723\n",
      ">> Epoch 503 finished \tANN training loss 0.041686\n",
      ">> Epoch 504 finished \tANN training loss 0.041636\n",
      ">> Epoch 505 finished \tANN training loss 0.041841\n",
      ">> Epoch 506 finished \tANN training loss 0.041716\n",
      ">> Epoch 507 finished \tANN training loss 0.041667\n",
      ">> Epoch 508 finished \tANN training loss 0.042443\n",
      ">> Epoch 509 finished \tANN training loss 0.042149\n",
      ">> Epoch 510 finished \tANN training loss 0.042031\n",
      ">> Epoch 511 finished \tANN training loss 0.041688\n",
      ">> Epoch 512 finished \tANN training loss 0.041847\n",
      ">> Epoch 513 finished \tANN training loss 0.041749\n",
      ">> Epoch 514 finished \tANN training loss 0.042792\n",
      ">> Epoch 515 finished \tANN training loss 0.041698\n",
      ">> Epoch 516 finished \tANN training loss 0.041569\n",
      ">> Epoch 517 finished \tANN training loss 0.042013\n",
      ">> Epoch 518 finished \tANN training loss 0.041585\n",
      ">> Epoch 519 finished \tANN training loss 0.041361\n",
      ">> Epoch 520 finished \tANN training loss 0.043901\n",
      ">> Epoch 521 finished \tANN training loss 0.042862\n",
      ">> Epoch 522 finished \tANN training loss 0.042880\n",
      ">> Epoch 523 finished \tANN training loss 0.041772\n",
      ">> Epoch 524 finished \tANN training loss 0.041124\n",
      ">> Epoch 525 finished \tANN training loss 0.041186\n",
      ">> Epoch 526 finished \tANN training loss 0.041200\n",
      ">> Epoch 527 finished \tANN training loss 0.041208\n",
      ">> Epoch 528 finished \tANN training loss 0.040918\n",
      ">> Epoch 529 finished \tANN training loss 0.041124\n",
      ">> Epoch 530 finished \tANN training loss 0.041253\n",
      ">> Epoch 531 finished \tANN training loss 0.041377\n",
      ">> Epoch 532 finished \tANN training loss 0.040720\n",
      ">> Epoch 533 finished \tANN training loss 0.040580\n",
      ">> Epoch 534 finished \tANN training loss 0.040601\n",
      ">> Epoch 535 finished \tANN training loss 0.040739\n",
      ">> Epoch 536 finished \tANN training loss 0.040564\n",
      ">> Epoch 537 finished \tANN training loss 0.041094\n",
      ">> Epoch 538 finished \tANN training loss 0.040779\n",
      ">> Epoch 539 finished \tANN training loss 0.040889\n",
      ">> Epoch 540 finished \tANN training loss 0.040801\n",
      ">> Epoch 541 finished \tANN training loss 0.040921\n",
      ">> Epoch 542 finished \tANN training loss 0.041015\n",
      ">> Epoch 543 finished \tANN training loss 0.040774\n",
      ">> Epoch 544 finished \tANN training loss 0.040791\n",
      ">> Epoch 545 finished \tANN training loss 0.040531\n",
      ">> Epoch 546 finished \tANN training loss 0.042647\n",
      ">> Epoch 547 finished \tANN training loss 0.041016\n",
      ">> Epoch 548 finished \tANN training loss 0.041024\n",
      ">> Epoch 549 finished \tANN training loss 0.041930\n",
      ">> Epoch 550 finished \tANN training loss 0.040939\n",
      ">> Epoch 551 finished \tANN training loss 0.041257\n",
      ">> Epoch 552 finished \tANN training loss 0.041743\n",
      ">> Epoch 553 finished \tANN training loss 0.040858\n",
      ">> Epoch 554 finished \tANN training loss 0.040848\n",
      ">> Epoch 555 finished \tANN training loss 0.040793\n",
      ">> Epoch 556 finished \tANN training loss 0.040906\n",
      ">> Epoch 557 finished \tANN training loss 0.040893\n",
      ">> Epoch 558 finished \tANN training loss 0.041166\n",
      ">> Epoch 559 finished \tANN training loss 0.040766\n",
      ">> Epoch 560 finished \tANN training loss 0.040777\n",
      ">> Epoch 561 finished \tANN training loss 0.040875\n",
      ">> Epoch 562 finished \tANN training loss 0.040890\n",
      ">> Epoch 563 finished \tANN training loss 0.040846\n",
      ">> Epoch 564 finished \tANN training loss 0.043507\n",
      ">> Epoch 565 finished \tANN training loss 0.040519\n",
      ">> Epoch 566 finished \tANN training loss 0.040724\n",
      ">> Epoch 567 finished \tANN training loss 0.040566\n",
      ">> Epoch 568 finished \tANN training loss 0.042001\n",
      ">> Epoch 569 finished \tANN training loss 0.040754\n",
      ">> Epoch 570 finished \tANN training loss 0.040332\n",
      ">> Epoch 571 finished \tANN training loss 0.040717\n",
      ">> Epoch 572 finished \tANN training loss 0.040640\n",
      ">> Epoch 573 finished \tANN training loss 0.040467\n",
      ">> Epoch 574 finished \tANN training loss 0.040222\n",
      ">> Epoch 575 finished \tANN training loss 0.040554\n",
      ">> Epoch 576 finished \tANN training loss 0.040435\n",
      ">> Epoch 577 finished \tANN training loss 0.040737\n",
      ">> Epoch 578 finished \tANN training loss 0.040540\n",
      ">> Epoch 579 finished \tANN training loss 0.040774\n",
      ">> Epoch 580 finished \tANN training loss 0.040557\n",
      ">> Epoch 581 finished \tANN training loss 0.040470\n",
      ">> Epoch 582 finished \tANN training loss 0.040396\n",
      ">> Epoch 583 finished \tANN training loss 0.040352\n",
      ">> Epoch 584 finished \tANN training loss 0.040491\n",
      ">> Epoch 585 finished \tANN training loss 0.040814\n",
      ">> Epoch 586 finished \tANN training loss 0.040553\n",
      ">> Epoch 587 finished \tANN training loss 0.040558\n",
      ">> Epoch 588 finished \tANN training loss 0.040860\n",
      ">> Epoch 589 finished \tANN training loss 0.040604\n",
      ">> Epoch 590 finished \tANN training loss 0.040638\n",
      ">> Epoch 591 finished \tANN training loss 0.040556\n",
      ">> Epoch 592 finished \tANN training loss 0.040808\n",
      ">> Epoch 593 finished \tANN training loss 0.041101\n",
      ">> Epoch 594 finished \tANN training loss 0.040472\n",
      ">> Epoch 595 finished \tANN training loss 0.040514\n",
      ">> Epoch 596 finished \tANN training loss 0.040377\n",
      ">> Epoch 597 finished \tANN training loss 0.040663\n",
      ">> Epoch 598 finished \tANN training loss 0.040494\n",
      ">> Epoch 599 finished \tANN training loss 0.040490\n",
      ">> Epoch 600 finished \tANN training loss 0.040920\n",
      ">> Epoch 601 finished \tANN training loss 0.040793\n",
      ">> Epoch 602 finished \tANN training loss 0.040304\n",
      ">> Epoch 603 finished \tANN training loss 0.040234\n",
      ">> Epoch 604 finished \tANN training loss 0.040238\n",
      ">> Epoch 605 finished \tANN training loss 0.040384\n",
      ">> Epoch 606 finished \tANN training loss 0.040740\n",
      ">> Epoch 607 finished \tANN training loss 0.041259\n",
      ">> Epoch 608 finished \tANN training loss 0.040852\n",
      ">> Epoch 609 finished \tANN training loss 0.040366\n",
      ">> Epoch 610 finished \tANN training loss 0.040868\n",
      ">> Epoch 611 finished \tANN training loss 0.040450\n",
      ">> Epoch 612 finished \tANN training loss 0.040483\n",
      ">> Epoch 613 finished \tANN training loss 0.040570\n",
      ">> Epoch 614 finished \tANN training loss 0.040562\n",
      ">> Epoch 615 finished \tANN training loss 0.040453\n",
      ">> Epoch 616 finished \tANN training loss 0.040873\n",
      ">> Epoch 617 finished \tANN training loss 0.040663\n",
      ">> Epoch 618 finished \tANN training loss 0.041190\n",
      ">> Epoch 619 finished \tANN training loss 0.040650\n",
      ">> Epoch 620 finished \tANN training loss 0.040577\n",
      ">> Epoch 621 finished \tANN training loss 0.040384\n",
      ">> Epoch 622 finished \tANN training loss 0.040425\n",
      ">> Epoch 623 finished \tANN training loss 0.040587\n",
      ">> Epoch 624 finished \tANN training loss 0.040248\n",
      ">> Epoch 625 finished \tANN training loss 0.040333\n",
      ">> Epoch 626 finished \tANN training loss 0.040130\n",
      ">> Epoch 627 finished \tANN training loss 0.040228\n",
      ">> Epoch 628 finished \tANN training loss 0.040437\n",
      ">> Epoch 629 finished \tANN training loss 0.040319\n",
      ">> Epoch 630 finished \tANN training loss 0.040274\n",
      ">> Epoch 631 finished \tANN training loss 0.040313\n",
      ">> Epoch 632 finished \tANN training loss 0.040767\n",
      ">> Epoch 633 finished \tANN training loss 0.040205\n",
      ">> Epoch 634 finished \tANN training loss 0.040212\n",
      ">> Epoch 635 finished \tANN training loss 0.040277\n",
      ">> Epoch 636 finished \tANN training loss 0.040148\n",
      ">> Epoch 637 finished \tANN training loss 0.040295\n",
      ">> Epoch 638 finished \tANN training loss 0.040351\n",
      ">> Epoch 639 finished \tANN training loss 0.040258\n",
      ">> Epoch 640 finished \tANN training loss 0.040354\n",
      ">> Epoch 641 finished \tANN training loss 0.040264\n",
      ">> Epoch 642 finished \tANN training loss 0.040310\n",
      ">> Epoch 643 finished \tANN training loss 0.040465\n",
      ">> Epoch 644 finished \tANN training loss 0.040317\n",
      ">> Epoch 645 finished \tANN training loss 0.040255\n",
      ">> Epoch 646 finished \tANN training loss 0.040116\n",
      ">> Epoch 647 finished \tANN training loss 0.040391\n",
      ">> Epoch 648 finished \tANN training loss 0.040361\n",
      ">> Epoch 649 finished \tANN training loss 0.040314\n",
      ">> Epoch 650 finished \tANN training loss 0.040183\n",
      ">> Epoch 651 finished \tANN training loss 0.040125\n",
      ">> Epoch 652 finished \tANN training loss 0.040501\n",
      ">> Epoch 653 finished \tANN training loss 0.040405\n",
      ">> Epoch 654 finished \tANN training loss 0.039993\n",
      ">> Epoch 655 finished \tANN training loss 0.040190\n",
      ">> Epoch 656 finished \tANN training loss 0.039946\n",
      ">> Epoch 657 finished \tANN training loss 0.039900\n",
      ">> Epoch 658 finished \tANN training loss 0.041930\n",
      ">> Epoch 659 finished \tANN training loss 0.041312\n",
      ">> Epoch 660 finished \tANN training loss 0.041140\n",
      ">> Epoch 661 finished \tANN training loss 0.040860\n",
      ">> Epoch 662 finished \tANN training loss 0.041285\n",
      ">> Epoch 663 finished \tANN training loss 0.041056\n",
      ">> Epoch 664 finished \tANN training loss 0.043962\n",
      ">> Epoch 665 finished \tANN training loss 0.040727\n",
      ">> Epoch 666 finished \tANN training loss 0.040289\n",
      ">> Epoch 667 finished \tANN training loss 0.043317\n",
      ">> Epoch 668 finished \tANN training loss 0.048926\n",
      ">> Epoch 669 finished \tANN training loss 0.043874\n",
      ">> Epoch 670 finished \tANN training loss 0.042583\n",
      ">> Epoch 671 finished \tANN training loss 0.041090\n",
      ">> Epoch 672 finished \tANN training loss 0.040190\n",
      ">> Epoch 673 finished \tANN training loss 0.040116\n",
      ">> Epoch 674 finished \tANN training loss 0.040027\n",
      ">> Epoch 675 finished \tANN training loss 0.040069\n",
      ">> Epoch 676 finished \tANN training loss 0.044918\n",
      ">> Epoch 677 finished \tANN training loss 0.040603\n",
      ">> Epoch 678 finished \tANN training loss 0.040316\n",
      ">> Epoch 679 finished \tANN training loss 0.040241\n",
      ">> Epoch 680 finished \tANN training loss 0.040096\n",
      ">> Epoch 681 finished \tANN training loss 0.040113\n",
      ">> Epoch 682 finished \tANN training loss 0.039886\n",
      ">> Epoch 683 finished \tANN training loss 0.042343\n",
      ">> Epoch 684 finished \tANN training loss 0.039897\n",
      ">> Epoch 685 finished \tANN training loss 0.039763\n",
      ">> Epoch 686 finished \tANN training loss 0.039924\n",
      ">> Epoch 687 finished \tANN training loss 0.039846\n",
      ">> Epoch 688 finished \tANN training loss 0.039817\n",
      ">> Epoch 689 finished \tANN training loss 0.039729\n",
      ">> Epoch 690 finished \tANN training loss 0.040159\n",
      ">> Epoch 691 finished \tANN training loss 0.039708\n",
      ">> Epoch 692 finished \tANN training loss 0.040107\n",
      ">> Epoch 693 finished \tANN training loss 0.039984\n",
      ">> Epoch 694 finished \tANN training loss 0.039912\n",
      ">> Epoch 695 finished \tANN training loss 0.042475\n",
      ">> Epoch 696 finished \tANN training loss 0.040450\n",
      ">> Epoch 697 finished \tANN training loss 0.040536\n",
      ">> Epoch 698 finished \tANN training loss 0.041383\n",
      ">> Epoch 699 finished \tANN training loss 0.040988\n",
      ">> Epoch 700 finished \tANN training loss 0.040533\n",
      ">> Epoch 701 finished \tANN training loss 0.040377\n",
      ">> Epoch 702 finished \tANN training loss 0.040502\n",
      ">> Epoch 703 finished \tANN training loss 0.040117\n",
      ">> Epoch 704 finished \tANN training loss 0.040040\n",
      ">> Epoch 705 finished \tANN training loss 0.040149\n",
      ">> Epoch 706 finished \tANN training loss 0.040140\n",
      ">> Epoch 707 finished \tANN training loss 0.040101\n",
      ">> Epoch 708 finished \tANN training loss 0.040294\n",
      ">> Epoch 709 finished \tANN training loss 0.042317\n",
      ">> Epoch 710 finished \tANN training loss 0.039958\n",
      ">> Epoch 711 finished \tANN training loss 0.039815\n",
      ">> Epoch 712 finished \tANN training loss 0.039307\n",
      ">> Epoch 713 finished \tANN training loss 0.040303\n",
      ">> Epoch 714 finished \tANN training loss 0.039268\n",
      ">> Epoch 715 finished \tANN training loss 0.039483\n",
      ">> Epoch 716 finished \tANN training loss 0.039747\n",
      ">> Epoch 717 finished \tANN training loss 0.039324\n",
      ">> Epoch 718 finished \tANN training loss 0.039336\n",
      ">> Epoch 719 finished \tANN training loss 0.039474\n",
      ">> Epoch 720 finished \tANN training loss 0.039644\n",
      ">> Epoch 721 finished \tANN training loss 0.039301\n",
      ">> Epoch 722 finished \tANN training loss 0.039081\n",
      ">> Epoch 723 finished \tANN training loss 0.038947\n",
      ">> Epoch 724 finished \tANN training loss 0.038905\n",
      ">> Epoch 725 finished \tANN training loss 0.039053\n",
      ">> Epoch 726 finished \tANN training loss 0.039134\n",
      ">> Epoch 727 finished \tANN training loss 0.039079\n",
      ">> Epoch 728 finished \tANN training loss 0.038951\n",
      ">> Epoch 729 finished \tANN training loss 0.039161\n",
      ">> Epoch 730 finished \tANN training loss 0.039102\n",
      ">> Epoch 731 finished \tANN training loss 0.039175\n",
      ">> Epoch 732 finished \tANN training loss 0.039012\n",
      ">> Epoch 733 finished \tANN training loss 0.038947\n",
      ">> Epoch 734 finished \tANN training loss 0.039126\n",
      ">> Epoch 735 finished \tANN training loss 0.038977\n",
      ">> Epoch 736 finished \tANN training loss 0.039064\n",
      ">> Epoch 737 finished \tANN training loss 0.039872\n",
      ">> Epoch 738 finished \tANN training loss 0.039254\n",
      ">> Epoch 739 finished \tANN training loss 0.040273\n",
      ">> Epoch 740 finished \tANN training loss 0.039317\n",
      ">> Epoch 741 finished \tANN training loss 0.039303\n",
      ">> Epoch 742 finished \tANN training loss 0.039087\n",
      ">> Epoch 743 finished \tANN training loss 0.039024\n",
      ">> Epoch 744 finished \tANN training loss 0.038798\n",
      ">> Epoch 745 finished \tANN training loss 0.039289\n",
      ">> Epoch 746 finished \tANN training loss 0.039107\n",
      ">> Epoch 747 finished \tANN training loss 0.039380\n",
      ">> Epoch 748 finished \tANN training loss 0.039096\n",
      ">> Epoch 749 finished \tANN training loss 0.040107\n",
      ">> Epoch 750 finished \tANN training loss 0.039208\n",
      ">> Epoch 751 finished \tANN training loss 0.039197\n",
      ">> Epoch 752 finished \tANN training loss 0.039283\n",
      ">> Epoch 753 finished \tANN training loss 0.038994\n",
      ">> Epoch 754 finished \tANN training loss 0.038803\n",
      ">> Epoch 755 finished \tANN training loss 0.038732\n",
      ">> Epoch 756 finished \tANN training loss 0.038810\n",
      ">> Epoch 757 finished \tANN training loss 0.038428\n",
      ">> Epoch 758 finished \tANN training loss 0.038592\n",
      ">> Epoch 759 finished \tANN training loss 0.039020\n",
      ">> Epoch 760 finished \tANN training loss 0.039026\n",
      ">> Epoch 761 finished \tANN training loss 0.038977\n",
      ">> Epoch 762 finished \tANN training loss 0.038826\n",
      ">> Epoch 763 finished \tANN training loss 0.039000\n",
      ">> Epoch 764 finished \tANN training loss 0.038815\n",
      ">> Epoch 765 finished \tANN training loss 0.038936\n",
      ">> Epoch 766 finished \tANN training loss 0.038834\n",
      ">> Epoch 767 finished \tANN training loss 0.038697\n",
      ">> Epoch 768 finished \tANN training loss 0.038697\n",
      ">> Epoch 769 finished \tANN training loss 0.038662\n",
      ">> Epoch 770 finished \tANN training loss 0.038798\n",
      ">> Epoch 771 finished \tANN training loss 0.038698\n",
      ">> Epoch 772 finished \tANN training loss 0.038711\n",
      ">> Epoch 773 finished \tANN training loss 0.038614\n",
      ">> Epoch 774 finished \tANN training loss 0.038602\n",
      ">> Epoch 775 finished \tANN training loss 0.038397\n",
      ">> Epoch 776 finished \tANN training loss 0.038371\n",
      ">> Epoch 777 finished \tANN training loss 0.038413\n",
      ">> Epoch 778 finished \tANN training loss 0.038708\n",
      ">> Epoch 779 finished \tANN training loss 0.038759\n",
      ">> Epoch 780 finished \tANN training loss 0.038530\n",
      ">> Epoch 781 finished \tANN training loss 0.038570\n",
      ">> Epoch 782 finished \tANN training loss 0.042597\n",
      ">> Epoch 783 finished \tANN training loss 0.039320\n",
      ">> Epoch 784 finished \tANN training loss 0.038864\n",
      ">> Epoch 785 finished \tANN training loss 0.038609\n",
      ">> Epoch 786 finished \tANN training loss 0.038792\n",
      ">> Epoch 787 finished \tANN training loss 0.042313\n",
      ">> Epoch 788 finished \tANN training loss 0.039403\n",
      ">> Epoch 789 finished \tANN training loss 0.039987\n",
      ">> Epoch 790 finished \tANN training loss 0.041608\n",
      ">> Epoch 791 finished \tANN training loss 0.040469\n",
      ">> Epoch 792 finished \tANN training loss 0.039574\n",
      ">> Epoch 793 finished \tANN training loss 0.042047\n",
      ">> Epoch 794 finished \tANN training loss 0.040046\n",
      ">> Epoch 795 finished \tANN training loss 0.040236\n",
      ">> Epoch 796 finished \tANN training loss 0.039472\n",
      ">> Epoch 797 finished \tANN training loss 0.039574\n",
      ">> Epoch 798 finished \tANN training loss 0.039354\n",
      ">> Epoch 799 finished \tANN training loss 0.039652\n",
      ">> Epoch 800 finished \tANN training loss 0.038888\n",
      ">> Epoch 801 finished \tANN training loss 0.038837\n",
      ">> Epoch 802 finished \tANN training loss 0.038816\n",
      ">> Epoch 803 finished \tANN training loss 0.042928\n",
      ">> Epoch 804 finished \tANN training loss 0.040793\n",
      ">> Epoch 805 finished \tANN training loss 0.040617\n",
      ">> Epoch 806 finished \tANN training loss 0.040260\n",
      ">> Epoch 807 finished \tANN training loss 0.039818\n",
      ">> Epoch 808 finished \tANN training loss 0.039471\n",
      ">> Epoch 809 finished \tANN training loss 0.039238\n",
      ">> Epoch 810 finished \tANN training loss 0.039174\n",
      ">> Epoch 811 finished \tANN training loss 0.039489\n",
      ">> Epoch 812 finished \tANN training loss 0.038897\n",
      ">> Epoch 813 finished \tANN training loss 0.038279\n",
      ">> Epoch 814 finished \tANN training loss 0.038260\n",
      ">> Epoch 815 finished \tANN training loss 0.038211\n",
      ">> Epoch 816 finished \tANN training loss 0.038193\n",
      ">> Epoch 817 finished \tANN training loss 0.038237\n",
      ">> Epoch 818 finished \tANN training loss 0.038176\n",
      ">> Epoch 819 finished \tANN training loss 0.038074\n",
      ">> Epoch 820 finished \tANN training loss 0.038178\n",
      ">> Epoch 821 finished \tANN training loss 0.038124\n",
      ">> Epoch 822 finished \tANN training loss 0.037989\n",
      ">> Epoch 823 finished \tANN training loss 0.038068\n",
      ">> Epoch 824 finished \tANN training loss 0.038752\n",
      ">> Epoch 825 finished \tANN training loss 0.038797\n",
      ">> Epoch 826 finished \tANN training loss 0.038279\n",
      ">> Epoch 827 finished \tANN training loss 0.037896\n",
      ">> Epoch 828 finished \tANN training loss 0.037735\n",
      ">> Epoch 829 finished \tANN training loss 0.037765\n",
      ">> Epoch 830 finished \tANN training loss 0.037712\n",
      ">> Epoch 831 finished \tANN training loss 0.038401\n",
      ">> Epoch 832 finished \tANN training loss 0.038064\n",
      ">> Epoch 833 finished \tANN training loss 0.037922\n",
      ">> Epoch 834 finished \tANN training loss 0.038509\n",
      ">> Epoch 835 finished \tANN training loss 0.038070\n",
      ">> Epoch 836 finished \tANN training loss 0.038135\n",
      ">> Epoch 837 finished \tANN training loss 0.037953\n",
      ">> Epoch 838 finished \tANN training loss 0.038040\n",
      ">> Epoch 839 finished \tANN training loss 0.038192\n",
      ">> Epoch 840 finished \tANN training loss 0.038167\n",
      ">> Epoch 841 finished \tANN training loss 0.038485\n",
      ">> Epoch 842 finished \tANN training loss 0.038224\n",
      ">> Epoch 843 finished \tANN training loss 0.037987\n",
      ">> Epoch 844 finished \tANN training loss 0.037931\n",
      ">> Epoch 845 finished \tANN training loss 0.037930\n",
      ">> Epoch 846 finished \tANN training loss 0.038423\n",
      ">> Epoch 847 finished \tANN training loss 0.037858\n",
      ">> Epoch 848 finished \tANN training loss 0.037558\n",
      ">> Epoch 849 finished \tANN training loss 0.037603\n",
      ">> Epoch 850 finished \tANN training loss 0.037755\n",
      ">> Epoch 851 finished \tANN training loss 0.037701\n",
      ">> Epoch 852 finished \tANN training loss 0.039514\n",
      ">> Epoch 853 finished \tANN training loss 0.038691\n",
      ">> Epoch 854 finished \tANN training loss 0.037965\n",
      ">> Epoch 855 finished \tANN training loss 0.038888\n",
      ">> Epoch 856 finished \tANN training loss 0.038010\n",
      ">> Epoch 857 finished \tANN training loss 0.037966\n",
      ">> Epoch 858 finished \tANN training loss 0.037795\n",
      ">> Epoch 859 finished \tANN training loss 0.037797\n",
      ">> Epoch 860 finished \tANN training loss 0.038231\n",
      ">> Epoch 861 finished \tANN training loss 0.038069\n",
      ">> Epoch 862 finished \tANN training loss 0.038235\n",
      ">> Epoch 863 finished \tANN training loss 0.040308\n",
      ">> Epoch 864 finished \tANN training loss 0.039494\n",
      ">> Epoch 865 finished \tANN training loss 0.039022\n",
      ">> Epoch 866 finished \tANN training loss 0.038664\n",
      ">> Epoch 867 finished \tANN training loss 0.038044\n",
      ">> Epoch 868 finished \tANN training loss 0.037869\n",
      ">> Epoch 869 finished \tANN training loss 0.040497\n",
      ">> Epoch 870 finished \tANN training loss 0.038716\n",
      ">> Epoch 871 finished \tANN training loss 0.038063\n",
      ">> Epoch 872 finished \tANN training loss 0.038650\n",
      ">> Epoch 873 finished \tANN training loss 0.038067\n",
      ">> Epoch 874 finished \tANN training loss 0.038225\n",
      ">> Epoch 875 finished \tANN training loss 0.039591\n",
      ">> Epoch 876 finished \tANN training loss 0.038904\n",
      ">> Epoch 877 finished \tANN training loss 0.040063\n",
      ">> Epoch 878 finished \tANN training loss 0.038445\n",
      ">> Epoch 879 finished \tANN training loss 0.037977\n",
      ">> Epoch 880 finished \tANN training loss 0.037840\n",
      ">> Epoch 881 finished \tANN training loss 0.037857\n",
      ">> Epoch 882 finished \tANN training loss 0.037883\n",
      ">> Epoch 883 finished \tANN training loss 0.037737\n",
      ">> Epoch 884 finished \tANN training loss 0.038407\n",
      ">> Epoch 885 finished \tANN training loss 0.038784\n",
      ">> Epoch 886 finished \tANN training loss 0.038007\n",
      ">> Epoch 887 finished \tANN training loss 0.038310\n",
      ">> Epoch 888 finished \tANN training loss 0.038535\n",
      ">> Epoch 889 finished \tANN training loss 0.037792\n",
      ">> Epoch 890 finished \tANN training loss 0.037696\n",
      ">> Epoch 891 finished \tANN training loss 0.038940\n",
      ">> Epoch 892 finished \tANN training loss 0.037841\n",
      ">> Epoch 893 finished \tANN training loss 0.038044\n",
      ">> Epoch 894 finished \tANN training loss 0.038278\n",
      ">> Epoch 895 finished \tANN training loss 0.038166\n",
      ">> Epoch 896 finished \tANN training loss 0.038123\n",
      ">> Epoch 897 finished \tANN training loss 0.037994\n",
      ">> Epoch 898 finished \tANN training loss 0.039413\n",
      ">> Epoch 899 finished \tANN training loss 0.038186\n",
      ">> Epoch 900 finished \tANN training loss 0.038317\n",
      ">> Epoch 901 finished \tANN training loss 0.040482\n",
      ">> Epoch 902 finished \tANN training loss 0.039296\n",
      ">> Epoch 903 finished \tANN training loss 0.038941\n",
      ">> Epoch 904 finished \tANN training loss 0.040031\n",
      ">> Epoch 905 finished \tANN training loss 0.039271\n",
      ">> Epoch 906 finished \tANN training loss 0.038674\n",
      ">> Epoch 907 finished \tANN training loss 0.038125\n",
      ">> Epoch 908 finished \tANN training loss 0.037847\n",
      ">> Epoch 909 finished \tANN training loss 0.038669\n",
      ">> Epoch 910 finished \tANN training loss 0.038771\n",
      ">> Epoch 911 finished \tANN training loss 0.039118\n",
      ">> Epoch 912 finished \tANN training loss 0.038696\n",
      ">> Epoch 913 finished \tANN training loss 0.039528\n",
      ">> Epoch 914 finished \tANN training loss 0.039175\n",
      ">> Epoch 915 finished \tANN training loss 0.038989\n",
      ">> Epoch 916 finished \tANN training loss 0.038531\n",
      ">> Epoch 917 finished \tANN training loss 0.038847\n",
      ">> Epoch 918 finished \tANN training loss 0.038931\n",
      ">> Epoch 919 finished \tANN training loss 0.038899\n",
      ">> Epoch 920 finished \tANN training loss 0.038608\n",
      ">> Epoch 921 finished \tANN training loss 0.038304\n",
      ">> Epoch 922 finished \tANN training loss 0.038230\n",
      ">> Epoch 923 finished \tANN training loss 0.038543\n",
      ">> Epoch 924 finished \tANN training loss 0.038268\n",
      ">> Epoch 925 finished \tANN training loss 0.038571\n",
      ">> Epoch 926 finished \tANN training loss 0.038279\n",
      ">> Epoch 927 finished \tANN training loss 0.038192\n",
      ">> Epoch 928 finished \tANN training loss 0.038068\n",
      ">> Epoch 929 finished \tANN training loss 0.038154\n",
      ">> Epoch 930 finished \tANN training loss 0.038010\n",
      ">> Epoch 931 finished \tANN training loss 0.038181\n",
      ">> Epoch 932 finished \tANN training loss 0.038294\n",
      ">> Epoch 933 finished \tANN training loss 0.038242\n",
      ">> Epoch 934 finished \tANN training loss 0.038056\n",
      ">> Epoch 935 finished \tANN training loss 0.039235\n",
      ">> Epoch 936 finished \tANN training loss 0.037873\n",
      ">> Epoch 937 finished \tANN training loss 0.039029\n",
      ">> Epoch 938 finished \tANN training loss 0.037711\n",
      ">> Epoch 939 finished \tANN training loss 0.037359\n",
      ">> Epoch 940 finished \tANN training loss 0.037347\n",
      ">> Epoch 941 finished \tANN training loss 0.037227\n",
      ">> Epoch 942 finished \tANN training loss 0.037673\n",
      ">> Epoch 943 finished \tANN training loss 0.037595\n",
      ">> Epoch 944 finished \tANN training loss 0.037227\n",
      ">> Epoch 945 finished \tANN training loss 0.037415\n",
      ">> Epoch 946 finished \tANN training loss 0.037487\n",
      ">> Epoch 947 finished \tANN training loss 0.037718\n",
      ">> Epoch 948 finished \tANN training loss 0.037926\n",
      ">> Epoch 949 finished \tANN training loss 0.037245\n",
      ">> Epoch 950 finished \tANN training loss 0.038583\n",
      ">> Epoch 951 finished \tANN training loss 0.037446\n",
      ">> Epoch 952 finished \tANN training loss 0.037259\n",
      ">> Epoch 953 finished \tANN training loss 0.037232\n",
      ">> Epoch 954 finished \tANN training loss 0.037304\n",
      ">> Epoch 955 finished \tANN training loss 0.037134\n",
      ">> Epoch 956 finished \tANN training loss 0.037260\n",
      ">> Epoch 957 finished \tANN training loss 0.037571\n",
      ">> Epoch 958 finished \tANN training loss 0.037551\n",
      ">> Epoch 959 finished \tANN training loss 0.037702\n",
      ">> Epoch 960 finished \tANN training loss 0.037419\n",
      ">> Epoch 961 finished \tANN training loss 0.037110\n",
      ">> Epoch 962 finished \tANN training loss 0.036937\n",
      ">> Epoch 963 finished \tANN training loss 0.036914\n",
      ">> Epoch 964 finished \tANN training loss 0.036944\n",
      ">> Epoch 965 finished \tANN training loss 0.036836\n",
      ">> Epoch 966 finished \tANN training loss 0.037049\n",
      ">> Epoch 967 finished \tANN training loss 0.036915\n",
      ">> Epoch 968 finished \tANN training loss 0.036728\n",
      ">> Epoch 969 finished \tANN training loss 0.038752\n",
      ">> Epoch 970 finished \tANN training loss 0.036801\n",
      ">> Epoch 971 finished \tANN training loss 0.036947\n",
      ">> Epoch 972 finished \tANN training loss 0.036562\n",
      ">> Epoch 973 finished \tANN training loss 0.036740\n",
      ">> Epoch 974 finished \tANN training loss 0.036922\n",
      ">> Epoch 975 finished \tANN training loss 0.036702\n",
      ">> Epoch 976 finished \tANN training loss 0.042565\n",
      ">> Epoch 977 finished \tANN training loss 0.038872\n",
      ">> Epoch 978 finished \tANN training loss 0.037388\n",
      ">> Epoch 979 finished \tANN training loss 0.037188\n",
      ">> Epoch 980 finished \tANN training loss 0.038086\n",
      ">> Epoch 981 finished \tANN training loss 0.037244\n",
      ">> Epoch 982 finished \tANN training loss 0.037167\n",
      ">> Epoch 983 finished \tANN training loss 0.037242\n",
      ">> Epoch 984 finished \tANN training loss 0.036729\n",
      ">> Epoch 985 finished \tANN training loss 0.036776\n",
      ">> Epoch 986 finished \tANN training loss 0.036706\n",
      ">> Epoch 987 finished \tANN training loss 0.036698\n",
      ">> Epoch 988 finished \tANN training loss 0.036710\n",
      ">> Epoch 989 finished \tANN training loss 0.039179\n",
      ">> Epoch 990 finished \tANN training loss 0.041427\n",
      ">> Epoch 991 finished \tANN training loss 0.040822\n",
      ">> Epoch 992 finished \tANN training loss 0.037472\n",
      ">> Epoch 993 finished \tANN training loss 0.037834\n",
      ">> Epoch 994 finished \tANN training loss 0.037701\n",
      ">> Epoch 995 finished \tANN training loss 0.037059\n",
      ">> Epoch 996 finished \tANN training loss 0.036878\n",
      ">> Epoch 997 finished \tANN training loss 0.036650\n",
      ">> Epoch 998 finished \tANN training loss 0.038060\n",
      ">> Epoch 999 finished \tANN training loss 0.036717\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.387220\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.959233\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.639322\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.424894\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.308084\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.226460\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.169925\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.139868\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.107119\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.091502\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.082561\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.087096\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.077415\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.075213\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.074028\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.077928\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.074678\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.072348\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.077812\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.072085\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2.423332\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.868955\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.654321\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1.430612\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 1.126911\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.837195\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.686087\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.610055\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.537097\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.475494\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.416592\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.353832\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.303859\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.255113\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.218143\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.196864\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.144244\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.117565\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.092451\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.085354\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.596954\n",
      ">> Epoch 1 finished \tANN training loss 0.312210\n",
      ">> Epoch 2 finished \tANN training loss 0.180148\n",
      ">> Epoch 3 finished \tANN training loss 0.118946\n",
      ">> Epoch 4 finished \tANN training loss 0.090587\n",
      ">> Epoch 5 finished \tANN training loss 0.075356\n",
      ">> Epoch 6 finished \tANN training loss 0.068180\n",
      ">> Epoch 7 finished \tANN training loss 0.063851\n",
      ">> Epoch 8 finished \tANN training loss 0.060908\n",
      ">> Epoch 9 finished \tANN training loss 0.059322\n",
      ">> Epoch 10 finished \tANN training loss 0.058271\n",
      ">> Epoch 11 finished \tANN training loss 0.057616\n",
      ">> Epoch 12 finished \tANN training loss 0.056887\n",
      ">> Epoch 13 finished \tANN training loss 0.056440\n",
      ">> Epoch 14 finished \tANN training loss 0.055738\n",
      ">> Epoch 15 finished \tANN training loss 0.055490\n",
      ">> Epoch 16 finished \tANN training loss 0.054993\n",
      ">> Epoch 17 finished \tANN training loss 0.054211\n",
      ">> Epoch 18 finished \tANN training loss 0.053990\n",
      ">> Epoch 19 finished \tANN training loss 0.054196\n",
      ">> Epoch 20 finished \tANN training loss 0.053533\n",
      ">> Epoch 21 finished \tANN training loss 0.053773\n",
      ">> Epoch 22 finished \tANN training loss 0.053299\n",
      ">> Epoch 23 finished \tANN training loss 0.053108\n",
      ">> Epoch 24 finished \tANN training loss 0.052885\n",
      ">> Epoch 25 finished \tANN training loss 0.052400\n",
      ">> Epoch 26 finished \tANN training loss 0.052457\n",
      ">> Epoch 27 finished \tANN training loss 0.051831\n",
      ">> Epoch 28 finished \tANN training loss 0.051774\n",
      ">> Epoch 29 finished \tANN training loss 0.052127\n",
      ">> Epoch 30 finished \tANN training loss 0.052434\n",
      ">> Epoch 31 finished \tANN training loss 0.051774\n",
      ">> Epoch 32 finished \tANN training loss 0.051486\n",
      ">> Epoch 33 finished \tANN training loss 0.051084\n",
      ">> Epoch 34 finished \tANN training loss 0.052394\n",
      ">> Epoch 35 finished \tANN training loss 0.051225\n",
      ">> Epoch 36 finished \tANN training loss 0.051206\n",
      ">> Epoch 37 finished \tANN training loss 0.051309\n",
      ">> Epoch 38 finished \tANN training loss 0.051563\n",
      ">> Epoch 39 finished \tANN training loss 0.051392\n",
      ">> Epoch 40 finished \tANN training loss 0.051134\n",
      ">> Epoch 41 finished \tANN training loss 0.050906\n",
      ">> Epoch 42 finished \tANN training loss 0.050474\n",
      ">> Epoch 43 finished \tANN training loss 0.050748\n",
      ">> Epoch 44 finished \tANN training loss 0.050750\n",
      ">> Epoch 45 finished \tANN training loss 0.050768\n",
      ">> Epoch 46 finished \tANN training loss 0.050568\n",
      ">> Epoch 47 finished \tANN training loss 0.050758\n",
      ">> Epoch 48 finished \tANN training loss 0.051667\n",
      ">> Epoch 49 finished \tANN training loss 0.049956\n",
      ">> Epoch 50 finished \tANN training loss 0.049867\n",
      ">> Epoch 51 finished \tANN training loss 0.050445\n",
      ">> Epoch 52 finished \tANN training loss 0.049850\n",
      ">> Epoch 53 finished \tANN training loss 0.049814\n",
      ">> Epoch 54 finished \tANN training loss 0.049589\n",
      ">> Epoch 55 finished \tANN training loss 0.049382\n",
      ">> Epoch 56 finished \tANN training loss 0.049589\n",
      ">> Epoch 57 finished \tANN training loss 0.049642\n",
      ">> Epoch 58 finished \tANN training loss 0.049849\n",
      ">> Epoch 59 finished \tANN training loss 0.049774\n",
      ">> Epoch 60 finished \tANN training loss 0.049976\n",
      ">> Epoch 61 finished \tANN training loss 0.049418\n",
      ">> Epoch 62 finished \tANN training loss 0.049185\n",
      ">> Epoch 63 finished \tANN training loss 0.049056\n",
      ">> Epoch 64 finished \tANN training loss 0.049348\n",
      ">> Epoch 65 finished \tANN training loss 0.049467\n",
      ">> Epoch 66 finished \tANN training loss 0.049359\n",
      ">> Epoch 67 finished \tANN training loss 0.049522\n",
      ">> Epoch 68 finished \tANN training loss 0.049651\n",
      ">> Epoch 69 finished \tANN training loss 0.049449\n",
      ">> Epoch 70 finished \tANN training loss 0.049355\n",
      ">> Epoch 71 finished \tANN training loss 0.049435\n",
      ">> Epoch 72 finished \tANN training loss 0.049745\n",
      ">> Epoch 73 finished \tANN training loss 0.049304\n",
      ">> Epoch 74 finished \tANN training loss 0.049363\n",
      ">> Epoch 75 finished \tANN training loss 0.049100\n",
      ">> Epoch 76 finished \tANN training loss 0.049055\n",
      ">> Epoch 77 finished \tANN training loss 0.049229\n",
      ">> Epoch 78 finished \tANN training loss 0.049120\n",
      ">> Epoch 79 finished \tANN training loss 0.048981\n",
      ">> Epoch 80 finished \tANN training loss 0.048806\n",
      ">> Epoch 81 finished \tANN training loss 0.048961\n",
      ">> Epoch 82 finished \tANN training loss 0.049182\n",
      ">> Epoch 83 finished \tANN training loss 0.048948\n",
      ">> Epoch 84 finished \tANN training loss 0.048628\n",
      ">> Epoch 85 finished \tANN training loss 0.048599\n",
      ">> Epoch 86 finished \tANN training loss 0.048586\n",
      ">> Epoch 87 finished \tANN training loss 0.048354\n",
      ">> Epoch 88 finished \tANN training loss 0.048392\n",
      ">> Epoch 89 finished \tANN training loss 0.049159\n",
      ">> Epoch 90 finished \tANN training loss 0.048728\n",
      ">> Epoch 91 finished \tANN training loss 0.048918\n",
      ">> Epoch 92 finished \tANN training loss 0.048513\n",
      ">> Epoch 93 finished \tANN training loss 0.048612\n",
      ">> Epoch 94 finished \tANN training loss 0.048907\n",
      ">> Epoch 95 finished \tANN training loss 0.049137\n",
      ">> Epoch 96 finished \tANN training loss 0.049399\n",
      ">> Epoch 97 finished \tANN training loss 0.050189\n",
      ">> Epoch 98 finished \tANN training loss 0.049300\n",
      ">> Epoch 99 finished \tANN training loss 0.048863\n",
      ">> Epoch 100 finished \tANN training loss 0.049063\n",
      ">> Epoch 101 finished \tANN training loss 0.048679\n",
      ">> Epoch 102 finished \tANN training loss 0.049000\n",
      ">> Epoch 103 finished \tANN training loss 0.048748\n",
      ">> Epoch 104 finished \tANN training loss 0.048736\n",
      ">> Epoch 105 finished \tANN training loss 0.049083\n",
      ">> Epoch 106 finished \tANN training loss 0.048051\n",
      ">> Epoch 107 finished \tANN training loss 0.048093\n",
      ">> Epoch 108 finished \tANN training loss 0.048027\n",
      ">> Epoch 109 finished \tANN training loss 0.047869\n",
      ">> Epoch 110 finished \tANN training loss 0.047709\n",
      ">> Epoch 111 finished \tANN training loss 0.047776\n",
      ">> Epoch 112 finished \tANN training loss 0.047775\n",
      ">> Epoch 113 finished \tANN training loss 0.047756\n",
      ">> Epoch 114 finished \tANN training loss 0.049379\n",
      ">> Epoch 115 finished \tANN training loss 0.048725\n",
      ">> Epoch 116 finished \tANN training loss 0.047720\n",
      ">> Epoch 117 finished \tANN training loss 0.048533\n",
      ">> Epoch 118 finished \tANN training loss 0.047715\n",
      ">> Epoch 119 finished \tANN training loss 0.047568\n",
      ">> Epoch 120 finished \tANN training loss 0.047523\n",
      ">> Epoch 121 finished \tANN training loss 0.047422\n",
      ">> Epoch 122 finished \tANN training loss 0.047576\n",
      ">> Epoch 123 finished \tANN training loss 0.047381\n",
      ">> Epoch 124 finished \tANN training loss 0.047645\n",
      ">> Epoch 125 finished \tANN training loss 0.047257\n",
      ">> Epoch 126 finished \tANN training loss 0.047205\n",
      ">> Epoch 127 finished \tANN training loss 0.047122\n",
      ">> Epoch 128 finished \tANN training loss 0.047580\n",
      ">> Epoch 129 finished \tANN training loss 0.047445\n",
      ">> Epoch 130 finished \tANN training loss 0.047335\n",
      ">> Epoch 131 finished \tANN training loss 0.047482\n",
      ">> Epoch 132 finished \tANN training loss 0.047809\n",
      ">> Epoch 133 finished \tANN training loss 0.047875\n",
      ">> Epoch 134 finished \tANN training loss 0.048130\n",
      ">> Epoch 135 finished \tANN training loss 0.049162\n",
      ">> Epoch 136 finished \tANN training loss 0.047523\n",
      ">> Epoch 137 finished \tANN training loss 0.047521\n",
      ">> Epoch 138 finished \tANN training loss 0.047683\n",
      ">> Epoch 139 finished \tANN training loss 0.047371\n",
      ">> Epoch 140 finished \tANN training loss 0.047637\n",
      ">> Epoch 141 finished \tANN training loss 0.047199\n",
      ">> Epoch 142 finished \tANN training loss 0.047484\n",
      ">> Epoch 143 finished \tANN training loss 0.047285\n",
      ">> Epoch 144 finished \tANN training loss 0.047345\n",
      ">> Epoch 145 finished \tANN training loss 0.047385\n",
      ">> Epoch 146 finished \tANN training loss 0.047453\n",
      ">> Epoch 147 finished \tANN training loss 0.047640\n",
      ">> Epoch 148 finished \tANN training loss 0.048864\n",
      ">> Epoch 149 finished \tANN training loss 0.047063\n",
      ">> Epoch 150 finished \tANN training loss 0.046999\n",
      ">> Epoch 151 finished \tANN training loss 0.046964\n",
      ">> Epoch 152 finished \tANN training loss 0.047001\n",
      ">> Epoch 153 finished \tANN training loss 0.047301\n",
      ">> Epoch 154 finished \tANN training loss 0.047409\n",
      ">> Epoch 155 finished \tANN training loss 0.046997\n",
      ">> Epoch 156 finished \tANN training loss 0.046990\n",
      ">> Epoch 157 finished \tANN training loss 0.047526\n",
      ">> Epoch 158 finished \tANN training loss 0.046846\n",
      ">> Epoch 159 finished \tANN training loss 0.048093\n",
      ">> Epoch 160 finished \tANN training loss 0.047026\n",
      ">> Epoch 161 finished \tANN training loss 0.047043\n",
      ">> Epoch 162 finished \tANN training loss 0.047348\n",
      ">> Epoch 163 finished \tANN training loss 0.047014\n",
      ">> Epoch 164 finished \tANN training loss 0.046774\n",
      ">> Epoch 165 finished \tANN training loss 0.046857\n",
      ">> Epoch 166 finished \tANN training loss 0.046855\n",
      ">> Epoch 167 finished \tANN training loss 0.046707\n",
      ">> Epoch 168 finished \tANN training loss 0.046790\n",
      ">> Epoch 169 finished \tANN training loss 0.047118\n",
      ">> Epoch 170 finished \tANN training loss 0.047372\n",
      ">> Epoch 171 finished \tANN training loss 0.046959\n",
      ">> Epoch 172 finished \tANN training loss 0.046741\n",
      ">> Epoch 173 finished \tANN training loss 0.048175\n",
      ">> Epoch 174 finished \tANN training loss 0.046984\n",
      ">> Epoch 175 finished \tANN training loss 0.047477\n",
      ">> Epoch 176 finished \tANN training loss 0.046730\n",
      ">> Epoch 177 finished \tANN training loss 0.046800\n",
      ">> Epoch 178 finished \tANN training loss 0.046830\n",
      ">> Epoch 179 finished \tANN training loss 0.046972\n",
      ">> Epoch 180 finished \tANN training loss 0.046958\n",
      ">> Epoch 181 finished \tANN training loss 0.047129\n",
      ">> Epoch 182 finished \tANN training loss 0.046947\n",
      ">> Epoch 183 finished \tANN training loss 0.047102\n",
      ">> Epoch 184 finished \tANN training loss 0.048300\n",
      ">> Epoch 185 finished \tANN training loss 0.046864\n",
      ">> Epoch 186 finished \tANN training loss 0.046698\n",
      ">> Epoch 187 finished \tANN training loss 0.046624\n",
      ">> Epoch 188 finished \tANN training loss 0.046916\n",
      ">> Epoch 189 finished \tANN training loss 0.047078\n",
      ">> Epoch 190 finished \tANN training loss 0.046710\n",
      ">> Epoch 191 finished \tANN training loss 0.047190\n",
      ">> Epoch 192 finished \tANN training loss 0.046788\n",
      ">> Epoch 193 finished \tANN training loss 0.046768\n",
      ">> Epoch 194 finished \tANN training loss 0.047129\n",
      ">> Epoch 195 finished \tANN training loss 0.047040\n",
      ">> Epoch 196 finished \tANN training loss 0.046874\n",
      ">> Epoch 197 finished \tANN training loss 0.046514\n",
      ">> Epoch 198 finished \tANN training loss 0.046583\n",
      ">> Epoch 199 finished \tANN training loss 0.046632\n",
      ">> Epoch 200 finished \tANN training loss 0.046371\n",
      ">> Epoch 201 finished \tANN training loss 0.046483\n",
      ">> Epoch 202 finished \tANN training loss 0.046272\n",
      ">> Epoch 203 finished \tANN training loss 0.046415\n",
      ">> Epoch 204 finished \tANN training loss 0.046098\n",
      ">> Epoch 205 finished \tANN training loss 0.046092\n",
      ">> Epoch 206 finished \tANN training loss 0.046584\n",
      ">> Epoch 207 finished \tANN training loss 0.046383\n",
      ">> Epoch 208 finished \tANN training loss 0.046391\n",
      ">> Epoch 209 finished \tANN training loss 0.046670\n",
      ">> Epoch 210 finished \tANN training loss 0.046380\n",
      ">> Epoch 211 finished \tANN training loss 0.046430\n",
      ">> Epoch 212 finished \tANN training loss 0.046301\n",
      ">> Epoch 213 finished \tANN training loss 0.047589\n",
      ">> Epoch 214 finished \tANN training loss 0.046667\n",
      ">> Epoch 215 finished \tANN training loss 0.046368\n",
      ">> Epoch 216 finished \tANN training loss 0.046337\n",
      ">> Epoch 217 finished \tANN training loss 0.046238\n",
      ">> Epoch 218 finished \tANN training loss 0.046567\n",
      ">> Epoch 219 finished \tANN training loss 0.046338\n",
      ">> Epoch 220 finished \tANN training loss 0.046292\n",
      ">> Epoch 221 finished \tANN training loss 0.046200\n",
      ">> Epoch 222 finished \tANN training loss 0.046071\n",
      ">> Epoch 223 finished \tANN training loss 0.046499\n",
      ">> Epoch 224 finished \tANN training loss 0.045964\n",
      ">> Epoch 225 finished \tANN training loss 0.046177\n",
      ">> Epoch 226 finished \tANN training loss 0.045857\n",
      ">> Epoch 227 finished \tANN training loss 0.045813\n",
      ">> Epoch 228 finished \tANN training loss 0.046542\n",
      ">> Epoch 229 finished \tANN training loss 0.046849\n",
      ">> Epoch 230 finished \tANN training loss 0.046474\n",
      ">> Epoch 231 finished \tANN training loss 0.046040\n",
      ">> Epoch 232 finished \tANN training loss 0.046258\n",
      ">> Epoch 233 finished \tANN training loss 0.046036\n",
      ">> Epoch 234 finished \tANN training loss 0.045902\n",
      ">> Epoch 235 finished \tANN training loss 0.045818\n",
      ">> Epoch 236 finished \tANN training loss 0.045874\n",
      ">> Epoch 237 finished \tANN training loss 0.046327\n",
      ">> Epoch 238 finished \tANN training loss 0.046418\n",
      ">> Epoch 239 finished \tANN training loss 0.045906\n",
      ">> Epoch 240 finished \tANN training loss 0.045596\n",
      ">> Epoch 241 finished \tANN training loss 0.045846\n",
      ">> Epoch 242 finished \tANN training loss 0.046006\n",
      ">> Epoch 243 finished \tANN training loss 0.045765\n",
      ">> Epoch 244 finished \tANN training loss 0.045858\n",
      ">> Epoch 245 finished \tANN training loss 0.045932\n",
      ">> Epoch 246 finished \tANN training loss 0.046528\n",
      ">> Epoch 247 finished \tANN training loss 0.046268\n",
      ">> Epoch 248 finished \tANN training loss 0.045958\n",
      ">> Epoch 249 finished \tANN training loss 0.045969\n",
      ">> Epoch 250 finished \tANN training loss 0.045599\n",
      ">> Epoch 251 finished \tANN training loss 0.045452\n",
      ">> Epoch 252 finished \tANN training loss 0.045438\n",
      ">> Epoch 253 finished \tANN training loss 0.045742\n",
      ">> Epoch 254 finished \tANN training loss 0.045524\n",
      ">> Epoch 255 finished \tANN training loss 0.046122\n",
      ">> Epoch 256 finished \tANN training loss 0.045836\n",
      ">> Epoch 257 finished \tANN training loss 0.045630\n",
      ">> Epoch 258 finished \tANN training loss 0.045230\n",
      ">> Epoch 259 finished \tANN training loss 0.045191\n",
      ">> Epoch 260 finished \tANN training loss 0.045375\n",
      ">> Epoch 261 finished \tANN training loss 0.045369\n",
      ">> Epoch 262 finished \tANN training loss 0.045891\n",
      ">> Epoch 263 finished \tANN training loss 0.045050\n",
      ">> Epoch 264 finished \tANN training loss 0.045267\n",
      ">> Epoch 265 finished \tANN training loss 0.045981\n",
      ">> Epoch 266 finished \tANN training loss 0.045281\n",
      ">> Epoch 267 finished \tANN training loss 0.044955\n",
      ">> Epoch 268 finished \tANN training loss 0.044652\n",
      ">> Epoch 269 finished \tANN training loss 0.044731\n",
      ">> Epoch 270 finished \tANN training loss 0.045876\n",
      ">> Epoch 271 finished \tANN training loss 0.044991\n",
      ">> Epoch 272 finished \tANN training loss 0.045280\n",
      ">> Epoch 273 finished \tANN training loss 0.044592\n",
      ">> Epoch 274 finished \tANN training loss 0.045070\n",
      ">> Epoch 275 finished \tANN training loss 0.044706\n",
      ">> Epoch 276 finished \tANN training loss 0.044485\n",
      ">> Epoch 277 finished \tANN training loss 0.045195\n",
      ">> Epoch 278 finished \tANN training loss 0.044834\n",
      ">> Epoch 279 finished \tANN training loss 0.044571\n",
      ">> Epoch 280 finished \tANN training loss 0.044913\n",
      ">> Epoch 281 finished \tANN training loss 0.044484\n",
      ">> Epoch 282 finished \tANN training loss 0.044373\n",
      ">> Epoch 283 finished \tANN training loss 0.044804\n",
      ">> Epoch 284 finished \tANN training loss 0.044428\n",
      ">> Epoch 285 finished \tANN training loss 0.044450\n",
      ">> Epoch 286 finished \tANN training loss 0.044347\n",
      ">> Epoch 287 finished \tANN training loss 0.044203\n",
      ">> Epoch 288 finished \tANN training loss 0.043996\n",
      ">> Epoch 289 finished \tANN training loss 0.044655\n",
      ">> Epoch 290 finished \tANN training loss 0.044360\n",
      ">> Epoch 291 finished \tANN training loss 0.043849\n",
      ">> Epoch 292 finished \tANN training loss 0.043775\n",
      ">> Epoch 293 finished \tANN training loss 0.043973\n",
      ">> Epoch 294 finished \tANN training loss 0.044131\n",
      ">> Epoch 295 finished \tANN training loss 0.044218\n",
      ">> Epoch 296 finished \tANN training loss 0.044083\n",
      ">> Epoch 297 finished \tANN training loss 0.044379\n",
      ">> Epoch 298 finished \tANN training loss 0.044116\n",
      ">> Epoch 299 finished \tANN training loss 0.044151\n",
      ">> Epoch 300 finished \tANN training loss 0.044217\n",
      ">> Epoch 301 finished \tANN training loss 0.044358\n",
      ">> Epoch 302 finished \tANN training loss 0.044208\n",
      ">> Epoch 303 finished \tANN training loss 0.044099\n",
      ">> Epoch 304 finished \tANN training loss 0.045987\n",
      ">> Epoch 305 finished \tANN training loss 0.045548\n",
      ">> Epoch 306 finished \tANN training loss 0.045067\n",
      ">> Epoch 307 finished \tANN training loss 0.044510\n",
      ">> Epoch 308 finished \tANN training loss 0.044254\n",
      ">> Epoch 309 finished \tANN training loss 0.044587\n",
      ">> Epoch 310 finished \tANN training loss 0.044705\n",
      ">> Epoch 311 finished \tANN training loss 0.047044\n",
      ">> Epoch 312 finished \tANN training loss 0.047980\n",
      ">> Epoch 313 finished \tANN training loss 0.045137\n",
      ">> Epoch 314 finished \tANN training loss 0.047147\n",
      ">> Epoch 315 finished \tANN training loss 0.044469\n",
      ">> Epoch 316 finished \tANN training loss 0.044155\n",
      ">> Epoch 317 finished \tANN training loss 0.045729\n",
      ">> Epoch 318 finished \tANN training loss 0.044135\n",
      ">> Epoch 319 finished \tANN training loss 0.043854\n",
      ">> Epoch 320 finished \tANN training loss 0.043763\n",
      ">> Epoch 321 finished \tANN training loss 0.045231\n",
      ">> Epoch 322 finished \tANN training loss 0.044289\n",
      ">> Epoch 323 finished \tANN training loss 0.044797\n",
      ">> Epoch 324 finished \tANN training loss 0.043992\n",
      ">> Epoch 325 finished \tANN training loss 0.044083\n",
      ">> Epoch 326 finished \tANN training loss 0.044170\n",
      ">> Epoch 327 finished \tANN training loss 0.044091\n",
      ">> Epoch 328 finished \tANN training loss 0.044516\n",
      ">> Epoch 329 finished \tANN training loss 0.044318\n",
      ">> Epoch 330 finished \tANN training loss 0.044200\n",
      ">> Epoch 331 finished \tANN training loss 0.044196\n",
      ">> Epoch 332 finished \tANN training loss 0.044632\n",
      ">> Epoch 333 finished \tANN training loss 0.044254\n",
      ">> Epoch 334 finished \tANN training loss 0.044424\n",
      ">> Epoch 335 finished \tANN training loss 0.044401\n",
      ">> Epoch 336 finished \tANN training loss 0.044288\n",
      ">> Epoch 337 finished \tANN training loss 0.044571\n",
      ">> Epoch 338 finished \tANN training loss 0.044300\n",
      ">> Epoch 339 finished \tANN training loss 0.045192\n",
      ">> Epoch 340 finished \tANN training loss 0.044199\n",
      ">> Epoch 341 finished \tANN training loss 0.044188\n",
      ">> Epoch 342 finished \tANN training loss 0.044139\n",
      ">> Epoch 343 finished \tANN training loss 0.043848\n",
      ">> Epoch 344 finished \tANN training loss 0.043828\n",
      ">> Epoch 345 finished \tANN training loss 0.043794\n",
      ">> Epoch 346 finished \tANN training loss 0.043876\n",
      ">> Epoch 347 finished \tANN training loss 0.043689\n",
      ">> Epoch 348 finished \tANN training loss 0.043659\n",
      ">> Epoch 349 finished \tANN training loss 0.044211\n",
      ">> Epoch 350 finished \tANN training loss 0.043875\n",
      ">> Epoch 351 finished \tANN training loss 0.043512\n",
      ">> Epoch 352 finished \tANN training loss 0.043800\n",
      ">> Epoch 353 finished \tANN training loss 0.043778\n",
      ">> Epoch 354 finished \tANN training loss 0.043979\n",
      ">> Epoch 355 finished \tANN training loss 0.044016\n",
      ">> Epoch 356 finished \tANN training loss 0.043949\n",
      ">> Epoch 357 finished \tANN training loss 0.044117\n",
      ">> Epoch 358 finished \tANN training loss 0.043916\n",
      ">> Epoch 359 finished \tANN training loss 0.044766\n",
      ">> Epoch 360 finished \tANN training loss 0.044890\n",
      ">> Epoch 361 finished \tANN training loss 0.044715\n",
      ">> Epoch 362 finished \tANN training loss 0.045758\n",
      ">> Epoch 363 finished \tANN training loss 0.044363\n",
      ">> Epoch 364 finished \tANN training loss 0.044730\n",
      ">> Epoch 365 finished \tANN training loss 0.044140\n",
      ">> Epoch 366 finished \tANN training loss 0.043658\n",
      ">> Epoch 367 finished \tANN training loss 0.043675\n",
      ">> Epoch 368 finished \tANN training loss 0.043643\n",
      ">> Epoch 369 finished \tANN training loss 0.043965\n",
      ">> Epoch 370 finished \tANN training loss 0.043755\n",
      ">> Epoch 371 finished \tANN training loss 0.043725\n",
      ">> Epoch 372 finished \tANN training loss 0.043754\n",
      ">> Epoch 373 finished \tANN training loss 0.043346\n",
      ">> Epoch 374 finished \tANN training loss 0.043270\n",
      ">> Epoch 375 finished \tANN training loss 0.043217\n",
      ">> Epoch 376 finished \tANN training loss 0.043294\n",
      ">> Epoch 377 finished \tANN training loss 0.043323\n",
      ">> Epoch 378 finished \tANN training loss 0.043145\n",
      ">> Epoch 379 finished \tANN training loss 0.043489\n",
      ">> Epoch 380 finished \tANN training loss 0.043193\n",
      ">> Epoch 381 finished \tANN training loss 0.042973\n",
      ">> Epoch 382 finished \tANN training loss 0.042747\n",
      ">> Epoch 383 finished \tANN training loss 0.042671\n",
      ">> Epoch 384 finished \tANN training loss 0.042819\n",
      ">> Epoch 385 finished \tANN training loss 0.042933\n",
      ">> Epoch 386 finished \tANN training loss 0.042801\n",
      ">> Epoch 387 finished \tANN training loss 0.042686\n",
      ">> Epoch 388 finished \tANN training loss 0.043283\n",
      ">> Epoch 389 finished \tANN training loss 0.042686\n",
      ">> Epoch 390 finished \tANN training loss 0.042737\n",
      ">> Epoch 391 finished \tANN training loss 0.042858\n",
      ">> Epoch 392 finished \tANN training loss 0.043054\n",
      ">> Epoch 393 finished \tANN training loss 0.043181\n",
      ">> Epoch 394 finished \tANN training loss 0.042919\n",
      ">> Epoch 395 finished \tANN training loss 0.043008\n",
      ">> Epoch 396 finished \tANN training loss 0.044510\n",
      ">> Epoch 397 finished \tANN training loss 0.045132\n",
      ">> Epoch 398 finished \tANN training loss 0.043265\n",
      ">> Epoch 399 finished \tANN training loss 0.042882\n",
      ">> Epoch 400 finished \tANN training loss 0.042939\n",
      ">> Epoch 401 finished \tANN training loss 0.043013\n",
      ">> Epoch 402 finished \tANN training loss 0.042745\n",
      ">> Epoch 403 finished \tANN training loss 0.042732\n",
      ">> Epoch 404 finished \tANN training loss 0.042791\n",
      ">> Epoch 405 finished \tANN training loss 0.042830\n",
      ">> Epoch 406 finished \tANN training loss 0.043076\n",
      ">> Epoch 407 finished \tANN training loss 0.042934\n",
      ">> Epoch 408 finished \tANN training loss 0.042762\n",
      ">> Epoch 409 finished \tANN training loss 0.042820\n",
      ">> Epoch 410 finished \tANN training loss 0.042606\n",
      ">> Epoch 411 finished \tANN training loss 0.042564\n",
      ">> Epoch 412 finished \tANN training loss 0.043214\n",
      ">> Epoch 413 finished \tANN training loss 0.042509\n",
      ">> Epoch 414 finished \tANN training loss 0.042611\n",
      ">> Epoch 415 finished \tANN training loss 0.042666\n",
      ">> Epoch 416 finished \tANN training loss 0.042550\n",
      ">> Epoch 417 finished \tANN training loss 0.042946\n",
      ">> Epoch 418 finished \tANN training loss 0.042603\n",
      ">> Epoch 419 finished \tANN training loss 0.042635\n",
      ">> Epoch 420 finished \tANN training loss 0.042346\n",
      ">> Epoch 421 finished \tANN training loss 0.042302\n",
      ">> Epoch 422 finished \tANN training loss 0.042315\n",
      ">> Epoch 423 finished \tANN training loss 0.042420\n",
      ">> Epoch 424 finished \tANN training loss 0.042438\n",
      ">> Epoch 425 finished \tANN training loss 0.042668\n",
      ">> Epoch 426 finished \tANN training loss 0.042716\n",
      ">> Epoch 427 finished \tANN training loss 0.043157\n",
      ">> Epoch 428 finished \tANN training loss 0.044042\n",
      ">> Epoch 429 finished \tANN training loss 0.043098\n",
      ">> Epoch 430 finished \tANN training loss 0.042903\n",
      ">> Epoch 431 finished \tANN training loss 0.042871\n",
      ">> Epoch 432 finished \tANN training loss 0.042739\n",
      ">> Epoch 433 finished \tANN training loss 0.042786\n",
      ">> Epoch 434 finished \tANN training loss 0.042649\n",
      ">> Epoch 435 finished \tANN training loss 0.042748\n",
      ">> Epoch 436 finished \tANN training loss 0.042618\n",
      ">> Epoch 437 finished \tANN training loss 0.042967\n",
      ">> Epoch 438 finished \tANN training loss 0.042726\n",
      ">> Epoch 439 finished \tANN training loss 0.042909\n",
      ">> Epoch 440 finished \tANN training loss 0.043117\n",
      ">> Epoch 441 finished \tANN training loss 0.043726\n",
      ">> Epoch 442 finished \tANN training loss 0.042891\n",
      ">> Epoch 443 finished \tANN training loss 0.042722\n",
      ">> Epoch 444 finished \tANN training loss 0.042664\n",
      ">> Epoch 445 finished \tANN training loss 0.042619\n",
      ">> Epoch 446 finished \tANN training loss 0.042717\n",
      ">> Epoch 447 finished \tANN training loss 0.042546\n",
      ">> Epoch 448 finished \tANN training loss 0.042425\n",
      ">> Epoch 449 finished \tANN training loss 0.042445\n",
      ">> Epoch 450 finished \tANN training loss 0.042477\n",
      ">> Epoch 451 finished \tANN training loss 0.042840\n",
      ">> Epoch 452 finished \tANN training loss 0.042775\n",
      ">> Epoch 453 finished \tANN training loss 0.042748\n",
      ">> Epoch 454 finished \tANN training loss 0.043139\n",
      ">> Epoch 455 finished \tANN training loss 0.044068\n",
      ">> Epoch 456 finished \tANN training loss 0.043448\n",
      ">> Epoch 457 finished \tANN training loss 0.042993\n",
      ">> Epoch 458 finished \tANN training loss 0.043014\n",
      ">> Epoch 459 finished \tANN training loss 0.042567\n",
      ">> Epoch 460 finished \tANN training loss 0.042620\n",
      ">> Epoch 461 finished \tANN training loss 0.042621\n",
      ">> Epoch 462 finished \tANN training loss 0.042716\n",
      ">> Epoch 463 finished \tANN training loss 0.042872\n",
      ">> Epoch 464 finished \tANN training loss 0.042370\n",
      ">> Epoch 465 finished \tANN training loss 0.042220\n",
      ">> Epoch 466 finished \tANN training loss 0.042451\n",
      ">> Epoch 467 finished \tANN training loss 0.042356\n",
      ">> Epoch 468 finished \tANN training loss 0.042311\n",
      ">> Epoch 469 finished \tANN training loss 0.042636\n",
      ">> Epoch 470 finished \tANN training loss 0.042766\n",
      ">> Epoch 471 finished \tANN training loss 0.042575\n",
      ">> Epoch 472 finished \tANN training loss 0.042434\n",
      ">> Epoch 473 finished \tANN training loss 0.042682\n",
      ">> Epoch 474 finished \tANN training loss 0.042526\n",
      ">> Epoch 475 finished \tANN training loss 0.042255\n",
      ">> Epoch 476 finished \tANN training loss 0.042250\n",
      ">> Epoch 477 finished \tANN training loss 0.042202\n",
      ">> Epoch 478 finished \tANN training loss 0.042309\n",
      ">> Epoch 479 finished \tANN training loss 0.042187\n",
      ">> Epoch 480 finished \tANN training loss 0.042194\n",
      ">> Epoch 481 finished \tANN training loss 0.042680\n",
      ">> Epoch 482 finished \tANN training loss 0.043150\n",
      ">> Epoch 483 finished \tANN training loss 0.042542\n",
      ">> Epoch 484 finished \tANN training loss 0.042224\n",
      ">> Epoch 485 finished \tANN training loss 0.042904\n",
      ">> Epoch 486 finished \tANN training loss 0.042379\n",
      ">> Epoch 487 finished \tANN training loss 0.042579\n",
      ">> Epoch 488 finished \tANN training loss 0.042033\n",
      ">> Epoch 489 finished \tANN training loss 0.042219\n",
      ">> Epoch 490 finished \tANN training loss 0.042026\n",
      ">> Epoch 491 finished \tANN training loss 0.041789\n",
      ">> Epoch 492 finished \tANN training loss 0.041876\n",
      ">> Epoch 493 finished \tANN training loss 0.041934\n",
      ">> Epoch 494 finished \tANN training loss 0.041959\n",
      ">> Epoch 495 finished \tANN training loss 0.041678\n",
      ">> Epoch 496 finished \tANN training loss 0.041591\n",
      ">> Epoch 497 finished \tANN training loss 0.041649\n",
      ">> Epoch 498 finished \tANN training loss 0.041613\n",
      ">> Epoch 499 finished \tANN training loss 0.041552\n",
      ">> Epoch 500 finished \tANN training loss 0.041505\n",
      ">> Epoch 501 finished \tANN training loss 0.041466\n",
      ">> Epoch 502 finished \tANN training loss 0.041445\n",
      ">> Epoch 503 finished \tANN training loss 0.041563\n",
      ">> Epoch 504 finished \tANN training loss 0.041526\n",
      ">> Epoch 505 finished \tANN training loss 0.041343\n",
      ">> Epoch 506 finished \tANN training loss 0.041513\n",
      ">> Epoch 507 finished \tANN training loss 0.041366\n",
      ">> Epoch 508 finished \tANN training loss 0.041346\n",
      ">> Epoch 509 finished \tANN training loss 0.041279\n",
      ">> Epoch 510 finished \tANN training loss 0.041257\n",
      ">> Epoch 511 finished \tANN training loss 0.041498\n",
      ">> Epoch 512 finished \tANN training loss 0.041494\n",
      ">> Epoch 513 finished \tANN training loss 0.041374\n",
      ">> Epoch 514 finished \tANN training loss 0.041663\n",
      ">> Epoch 515 finished \tANN training loss 0.041416\n",
      ">> Epoch 516 finished \tANN training loss 0.041357\n",
      ">> Epoch 517 finished \tANN training loss 0.041441\n",
      ">> Epoch 518 finished \tANN training loss 0.041362\n",
      ">> Epoch 519 finished \tANN training loss 0.041576\n",
      ">> Epoch 520 finished \tANN training loss 0.041983\n",
      ">> Epoch 521 finished \tANN training loss 0.041803\n",
      ">> Epoch 522 finished \tANN training loss 0.041825\n",
      ">> Epoch 523 finished \tANN training loss 0.042083\n",
      ">> Epoch 524 finished \tANN training loss 0.041978\n",
      ">> Epoch 525 finished \tANN training loss 0.041814\n",
      ">> Epoch 526 finished \tANN training loss 0.041882\n",
      ">> Epoch 527 finished \tANN training loss 0.041683\n",
      ">> Epoch 528 finished \tANN training loss 0.041614\n",
      ">> Epoch 529 finished \tANN training loss 0.041981\n",
      ">> Epoch 530 finished \tANN training loss 0.041854\n",
      ">> Epoch 531 finished \tANN training loss 0.041773\n",
      ">> Epoch 532 finished \tANN training loss 0.042358\n",
      ">> Epoch 533 finished \tANN training loss 0.041835\n",
      ">> Epoch 534 finished \tANN training loss 0.041461\n",
      ">> Epoch 535 finished \tANN training loss 0.041746\n",
      ">> Epoch 536 finished \tANN training loss 0.041448\n",
      ">> Epoch 537 finished \tANN training loss 0.044989\n",
      ">> Epoch 538 finished \tANN training loss 0.042124\n",
      ">> Epoch 539 finished \tANN training loss 0.041878\n",
      ">> Epoch 540 finished \tANN training loss 0.041424\n",
      ">> Epoch 541 finished \tANN training loss 0.041342\n",
      ">> Epoch 542 finished \tANN training loss 0.041262\n",
      ">> Epoch 543 finished \tANN training loss 0.041230\n",
      ">> Epoch 544 finished \tANN training loss 0.041602\n",
      ">> Epoch 545 finished \tANN training loss 0.041288\n",
      ">> Epoch 546 finished \tANN training loss 0.041196\n",
      ">> Epoch 547 finished \tANN training loss 0.041179\n",
      ">> Epoch 548 finished \tANN training loss 0.041270\n",
      ">> Epoch 549 finished \tANN training loss 0.041074\n",
      ">> Epoch 550 finished \tANN training loss 0.041214\n",
      ">> Epoch 551 finished \tANN training loss 0.041395\n",
      ">> Epoch 552 finished \tANN training loss 0.041317\n",
      ">> Epoch 553 finished \tANN training loss 0.041248\n",
      ">> Epoch 554 finished \tANN training loss 0.041178\n",
      ">> Epoch 555 finished \tANN training loss 0.041215\n",
      ">> Epoch 556 finished \tANN training loss 0.041663\n",
      ">> Epoch 557 finished \tANN training loss 0.041191\n",
      ">> Epoch 558 finished \tANN training loss 0.041086\n",
      ">> Epoch 559 finished \tANN training loss 0.041006\n",
      ">> Epoch 560 finished \tANN training loss 0.042754\n",
      ">> Epoch 561 finished \tANN training loss 0.040861\n",
      ">> Epoch 562 finished \tANN training loss 0.041013\n",
      ">> Epoch 563 finished \tANN training loss 0.041846\n",
      ">> Epoch 564 finished \tANN training loss 0.041487\n",
      ">> Epoch 565 finished \tANN training loss 0.041191\n",
      ">> Epoch 566 finished \tANN training loss 0.041092\n",
      ">> Epoch 567 finished \tANN training loss 0.043500\n",
      ">> Epoch 568 finished \tANN training loss 0.041151\n",
      ">> Epoch 569 finished \tANN training loss 0.040823\n",
      ">> Epoch 570 finished \tANN training loss 0.040713\n",
      ">> Epoch 571 finished \tANN training loss 0.040748\n",
      ">> Epoch 572 finished \tANN training loss 0.040857\n",
      ">> Epoch 573 finished \tANN training loss 0.041062\n",
      ">> Epoch 574 finished \tANN training loss 0.040835\n",
      ">> Epoch 575 finished \tANN training loss 0.040539\n",
      ">> Epoch 576 finished \tANN training loss 0.040644\n",
      ">> Epoch 577 finished \tANN training loss 0.040874\n",
      ">> Epoch 578 finished \tANN training loss 0.040731\n",
      ">> Epoch 579 finished \tANN training loss 0.040348\n",
      ">> Epoch 580 finished \tANN training loss 0.040313\n",
      ">> Epoch 581 finished \tANN training loss 0.040428\n",
      ">> Epoch 582 finished \tANN training loss 0.040637\n",
      ">> Epoch 583 finished \tANN training loss 0.040546\n",
      ">> Epoch 584 finished \tANN training loss 0.040777\n",
      ">> Epoch 585 finished \tANN training loss 0.041120\n",
      ">> Epoch 586 finished \tANN training loss 0.040626\n",
      ">> Epoch 587 finished \tANN training loss 0.040546\n",
      ">> Epoch 588 finished \tANN training loss 0.040382\n",
      ">> Epoch 589 finished \tANN training loss 0.040453\n",
      ">> Epoch 590 finished \tANN training loss 0.040450\n",
      ">> Epoch 591 finished \tANN training loss 0.040434\n",
      ">> Epoch 592 finished \tANN training loss 0.040405\n",
      ">> Epoch 593 finished \tANN training loss 0.041523\n",
      ">> Epoch 594 finished \tANN training loss 0.040900\n",
      ">> Epoch 595 finished \tANN training loss 0.040809\n",
      ">> Epoch 596 finished \tANN training loss 0.040543\n",
      ">> Epoch 597 finished \tANN training loss 0.041577\n",
      ">> Epoch 598 finished \tANN training loss 0.041478\n",
      ">> Epoch 599 finished \tANN training loss 0.040736\n",
      ">> Epoch 600 finished \tANN training loss 0.040525\n",
      ">> Epoch 601 finished \tANN training loss 0.040386\n",
      ">> Epoch 602 finished \tANN training loss 0.041154\n",
      ">> Epoch 603 finished \tANN training loss 0.040444\n",
      ">> Epoch 604 finished \tANN training loss 0.040361\n",
      ">> Epoch 605 finished \tANN training loss 0.040429\n",
      ">> Epoch 606 finished \tANN training loss 0.040418\n",
      ">> Epoch 607 finished \tANN training loss 0.040372\n",
      ">> Epoch 608 finished \tANN training loss 0.040303\n",
      ">> Epoch 609 finished \tANN training loss 0.040416\n",
      ">> Epoch 610 finished \tANN training loss 0.041117\n",
      ">> Epoch 611 finished \tANN training loss 0.040843\n",
      ">> Epoch 612 finished \tANN training loss 0.040767\n",
      ">> Epoch 613 finished \tANN training loss 0.040455\n",
      ">> Epoch 614 finished \tANN training loss 0.040814\n",
      ">> Epoch 615 finished \tANN training loss 0.040480\n",
      ">> Epoch 616 finished \tANN training loss 0.040237\n",
      ">> Epoch 617 finished \tANN training loss 0.040480\n",
      ">> Epoch 618 finished \tANN training loss 0.040377\n",
      ">> Epoch 619 finished \tANN training loss 0.040218\n",
      ">> Epoch 620 finished \tANN training loss 0.039854\n",
      ">> Epoch 621 finished \tANN training loss 0.040013\n",
      ">> Epoch 622 finished \tANN training loss 0.040089\n",
      ">> Epoch 623 finished \tANN training loss 0.040016\n",
      ">> Epoch 624 finished \tANN training loss 0.039938\n",
      ">> Epoch 625 finished \tANN training loss 0.039812\n",
      ">> Epoch 626 finished \tANN training loss 0.039875\n",
      ">> Epoch 627 finished \tANN training loss 0.039890\n",
      ">> Epoch 628 finished \tANN training loss 0.039690\n",
      ">> Epoch 629 finished \tANN training loss 0.039659\n",
      ">> Epoch 630 finished \tANN training loss 0.039547\n",
      ">> Epoch 631 finished \tANN training loss 0.039820\n",
      ">> Epoch 632 finished \tANN training loss 0.039832\n",
      ">> Epoch 633 finished \tANN training loss 0.040202\n",
      ">> Epoch 634 finished \tANN training loss 0.039956\n",
      ">> Epoch 635 finished \tANN training loss 0.040355\n",
      ">> Epoch 636 finished \tANN training loss 0.041825\n",
      ">> Epoch 637 finished \tANN training loss 0.041632\n",
      ">> Epoch 638 finished \tANN training loss 0.039854\n",
      ">> Epoch 639 finished \tANN training loss 0.039795\n",
      ">> Epoch 640 finished \tANN training loss 0.039877\n",
      ">> Epoch 641 finished \tANN training loss 0.039840\n",
      ">> Epoch 642 finished \tANN training loss 0.039862\n",
      ">> Epoch 643 finished \tANN training loss 0.039949\n",
      ">> Epoch 644 finished \tANN training loss 0.039864\n",
      ">> Epoch 645 finished \tANN training loss 0.039970\n",
      ">> Epoch 646 finished \tANN training loss 0.040977\n",
      ">> Epoch 647 finished \tANN training loss 0.040117\n",
      ">> Epoch 648 finished \tANN training loss 0.040200\n",
      ">> Epoch 649 finished \tANN training loss 0.039862\n",
      ">> Epoch 650 finished \tANN training loss 0.039940\n",
      ">> Epoch 651 finished \tANN training loss 0.039891\n",
      ">> Epoch 652 finished \tANN training loss 0.040016\n",
      ">> Epoch 653 finished \tANN training loss 0.040127\n",
      ">> Epoch 654 finished \tANN training loss 0.040136\n",
      ">> Epoch 655 finished \tANN training loss 0.040651\n",
      ">> Epoch 656 finished \tANN training loss 0.040289\n",
      ">> Epoch 657 finished \tANN training loss 0.040394\n",
      ">> Epoch 658 finished \tANN training loss 0.040419\n",
      ">> Epoch 659 finished \tANN training loss 0.040371\n",
      ">> Epoch 660 finished \tANN training loss 0.040465\n",
      ">> Epoch 661 finished \tANN training loss 0.040317\n",
      ">> Epoch 662 finished \tANN training loss 0.040187\n",
      ">> Epoch 663 finished \tANN training loss 0.041278\n",
      ">> Epoch 664 finished \tANN training loss 0.040209\n",
      ">> Epoch 665 finished \tANN training loss 0.040192\n",
      ">> Epoch 666 finished \tANN training loss 0.040332\n",
      ">> Epoch 667 finished \tANN training loss 0.040033\n",
      ">> Epoch 668 finished \tANN training loss 0.040112\n",
      ">> Epoch 669 finished \tANN training loss 0.040146\n",
      ">> Epoch 670 finished \tANN training loss 0.040268\n",
      ">> Epoch 671 finished \tANN training loss 0.040204\n",
      ">> Epoch 672 finished \tANN training loss 0.040593\n",
      ">> Epoch 673 finished \tANN training loss 0.040310\n",
      ">> Epoch 674 finished \tANN training loss 0.040106\n",
      ">> Epoch 675 finished \tANN training loss 0.040162\n",
      ">> Epoch 676 finished \tANN training loss 0.040235\n",
      ">> Epoch 677 finished \tANN training loss 0.040321\n",
      ">> Epoch 678 finished \tANN training loss 0.040285\n",
      ">> Epoch 679 finished \tANN training loss 0.040231\n",
      ">> Epoch 680 finished \tANN training loss 0.040923\n",
      ">> Epoch 681 finished \tANN training loss 0.039920\n",
      ">> Epoch 682 finished \tANN training loss 0.040191\n",
      ">> Epoch 683 finished \tANN training loss 0.040401\n",
      ">> Epoch 684 finished \tANN training loss 0.040335\n",
      ">> Epoch 685 finished \tANN training loss 0.040529\n",
      ">> Epoch 686 finished \tANN training loss 0.040291\n",
      ">> Epoch 687 finished \tANN training loss 0.040233\n",
      ">> Epoch 688 finished \tANN training loss 0.039987\n",
      ">> Epoch 689 finished \tANN training loss 0.039938\n",
      ">> Epoch 690 finished \tANN training loss 0.040108\n",
      ">> Epoch 691 finished \tANN training loss 0.039834\n",
      ">> Epoch 692 finished \tANN training loss 0.040054\n",
      ">> Epoch 693 finished \tANN training loss 0.039842\n",
      ">> Epoch 694 finished \tANN training loss 0.039812\n",
      ">> Epoch 695 finished \tANN training loss 0.040400\n",
      ">> Epoch 696 finished \tANN training loss 0.039967\n",
      ">> Epoch 697 finished \tANN training loss 0.039748\n",
      ">> Epoch 698 finished \tANN training loss 0.039986\n",
      ">> Epoch 699 finished \tANN training loss 0.040250\n",
      ">> Epoch 700 finished \tANN training loss 0.040368\n",
      ">> Epoch 701 finished \tANN training loss 0.040511\n",
      ">> Epoch 702 finished \tANN training loss 0.040242\n",
      ">> Epoch 703 finished \tANN training loss 0.039879\n",
      ">> Epoch 704 finished \tANN training loss 0.039829\n",
      ">> Epoch 705 finished \tANN training loss 0.039987\n",
      ">> Epoch 706 finished \tANN training loss 0.040019\n",
      ">> Epoch 707 finished \tANN training loss 0.039829\n",
      ">> Epoch 708 finished \tANN training loss 0.039821\n",
      ">> Epoch 709 finished \tANN training loss 0.040119\n",
      ">> Epoch 710 finished \tANN training loss 0.039978\n",
      ">> Epoch 711 finished \tANN training loss 0.039939\n",
      ">> Epoch 712 finished \tANN training loss 0.039786\n",
      ">> Epoch 713 finished \tANN training loss 0.039822\n",
      ">> Epoch 714 finished \tANN training loss 0.039916\n",
      ">> Epoch 715 finished \tANN training loss 0.039758\n",
      ">> Epoch 716 finished \tANN training loss 0.039657\n",
      ">> Epoch 717 finished \tANN training loss 0.039850\n",
      ">> Epoch 718 finished \tANN training loss 0.040192\n",
      ">> Epoch 719 finished \tANN training loss 0.039730\n",
      ">> Epoch 720 finished \tANN training loss 0.040076\n",
      ">> Epoch 721 finished \tANN training loss 0.040016\n",
      ">> Epoch 722 finished \tANN training loss 0.039631\n",
      ">> Epoch 723 finished \tANN training loss 0.039651\n",
      ">> Epoch 724 finished \tANN training loss 0.039565\n",
      ">> Epoch 725 finished \tANN training loss 0.040470\n",
      ">> Epoch 726 finished \tANN training loss 0.039514\n",
      ">> Epoch 727 finished \tANN training loss 0.039285\n",
      ">> Epoch 728 finished \tANN training loss 0.039390\n",
      ">> Epoch 729 finished \tANN training loss 0.039499\n",
      ">> Epoch 730 finished \tANN training loss 0.039385\n",
      ">> Epoch 731 finished \tANN training loss 0.039246\n",
      ">> Epoch 732 finished \tANN training loss 0.039182\n",
      ">> Epoch 733 finished \tANN training loss 0.039268\n",
      ">> Epoch 734 finished \tANN training loss 0.039187\n",
      ">> Epoch 735 finished \tANN training loss 0.039303\n",
      ">> Epoch 736 finished \tANN training loss 0.039316\n",
      ">> Epoch 737 finished \tANN training loss 0.039426\n",
      ">> Epoch 738 finished \tANN training loss 0.039175\n",
      ">> Epoch 739 finished \tANN training loss 0.039548\n",
      ">> Epoch 740 finished \tANN training loss 0.039152\n",
      ">> Epoch 741 finished \tANN training loss 0.039216\n",
      ">> Epoch 742 finished \tANN training loss 0.039230\n",
      ">> Epoch 743 finished \tANN training loss 0.039397\n",
      ">> Epoch 744 finished \tANN training loss 0.040231\n",
      ">> Epoch 745 finished \tANN training loss 0.039432\n",
      ">> Epoch 746 finished \tANN training loss 0.039351\n",
      ">> Epoch 747 finished \tANN training loss 0.039592\n",
      ">> Epoch 748 finished \tANN training loss 0.039556\n",
      ">> Epoch 749 finished \tANN training loss 0.039606\n",
      ">> Epoch 750 finished \tANN training loss 0.039562\n",
      ">> Epoch 751 finished \tANN training loss 0.039702\n",
      ">> Epoch 752 finished \tANN training loss 0.039739\n",
      ">> Epoch 753 finished \tANN training loss 0.039486\n",
      ">> Epoch 754 finished \tANN training loss 0.039576\n",
      ">> Epoch 755 finished \tANN training loss 0.039361\n",
      ">> Epoch 756 finished \tANN training loss 0.039066\n",
      ">> Epoch 757 finished \tANN training loss 0.039129\n",
      ">> Epoch 758 finished \tANN training loss 0.039547\n",
      ">> Epoch 759 finished \tANN training loss 0.039583\n",
      ">> Epoch 760 finished \tANN training loss 0.039571\n",
      ">> Epoch 761 finished \tANN training loss 0.039251\n",
      ">> Epoch 762 finished \tANN training loss 0.039768\n",
      ">> Epoch 763 finished \tANN training loss 0.039674\n",
      ">> Epoch 764 finished \tANN training loss 0.039297\n",
      ">> Epoch 765 finished \tANN training loss 0.039447\n",
      ">> Epoch 766 finished \tANN training loss 0.039276\n",
      ">> Epoch 767 finished \tANN training loss 0.039250\n",
      ">> Epoch 768 finished \tANN training loss 0.039134\n",
      ">> Epoch 769 finished \tANN training loss 0.039043\n",
      ">> Epoch 770 finished \tANN training loss 0.038991\n",
      ">> Epoch 771 finished \tANN training loss 0.039063\n",
      ">> Epoch 772 finished \tANN training loss 0.039206\n",
      ">> Epoch 773 finished \tANN training loss 0.039331\n",
      ">> Epoch 774 finished \tANN training loss 0.039135\n",
      ">> Epoch 775 finished \tANN training loss 0.039015\n",
      ">> Epoch 776 finished \tANN training loss 0.038755\n",
      ">> Epoch 777 finished \tANN training loss 0.039142\n",
      ">> Epoch 778 finished \tANN training loss 0.039015\n",
      ">> Epoch 779 finished \tANN training loss 0.038952\n",
      ">> Epoch 780 finished \tANN training loss 0.038899\n",
      ">> Epoch 781 finished \tANN training loss 0.038914\n",
      ">> Epoch 782 finished \tANN training loss 0.038871\n",
      ">> Epoch 783 finished \tANN training loss 0.039159\n",
      ">> Epoch 784 finished \tANN training loss 0.038802\n",
      ">> Epoch 785 finished \tANN training loss 0.038953\n",
      ">> Epoch 786 finished \tANN training loss 0.038904\n",
      ">> Epoch 787 finished \tANN training loss 0.039336\n",
      ">> Epoch 788 finished \tANN training loss 0.040993\n",
      ">> Epoch 789 finished \tANN training loss 0.039563\n",
      ">> Epoch 790 finished \tANN training loss 0.039106\n",
      ">> Epoch 791 finished \tANN training loss 0.038846\n",
      ">> Epoch 792 finished \tANN training loss 0.038883\n",
      ">> Epoch 793 finished \tANN training loss 0.038819\n",
      ">> Epoch 794 finished \tANN training loss 0.038797\n",
      ">> Epoch 795 finished \tANN training loss 0.038622\n",
      ">> Epoch 796 finished \tANN training loss 0.038749\n",
      ">> Epoch 797 finished \tANN training loss 0.040153\n",
      ">> Epoch 798 finished \tANN training loss 0.039642\n",
      ">> Epoch 799 finished \tANN training loss 0.039695\n",
      ">> Epoch 800 finished \tANN training loss 0.039210\n",
      ">> Epoch 801 finished \tANN training loss 0.039518\n",
      ">> Epoch 802 finished \tANN training loss 0.038997\n",
      ">> Epoch 803 finished \tANN training loss 0.040415\n",
      ">> Epoch 804 finished \tANN training loss 0.039200\n",
      ">> Epoch 805 finished \tANN training loss 0.039120\n",
      ">> Epoch 806 finished \tANN training loss 0.038847\n",
      ">> Epoch 807 finished \tANN training loss 0.038874\n",
      ">> Epoch 808 finished \tANN training loss 0.038801\n",
      ">> Epoch 809 finished \tANN training loss 0.038630\n",
      ">> Epoch 810 finished \tANN training loss 0.038538\n",
      ">> Epoch 811 finished \tANN training loss 0.039116\n",
      ">> Epoch 812 finished \tANN training loss 0.039035\n",
      ">> Epoch 813 finished \tANN training loss 0.039129\n",
      ">> Epoch 814 finished \tANN training loss 0.038777\n",
      ">> Epoch 815 finished \tANN training loss 0.038725\n",
      ">> Epoch 816 finished \tANN training loss 0.039635\n",
      ">> Epoch 817 finished \tANN training loss 0.038851\n",
      ">> Epoch 818 finished \tANN training loss 0.038708\n",
      ">> Epoch 819 finished \tANN training loss 0.038898\n",
      ">> Epoch 820 finished \tANN training loss 0.038550\n",
      ">> Epoch 821 finished \tANN training loss 0.038535\n",
      ">> Epoch 822 finished \tANN training loss 0.038550\n",
      ">> Epoch 823 finished \tANN training loss 0.038525\n",
      ">> Epoch 824 finished \tANN training loss 0.038706\n",
      ">> Epoch 825 finished \tANN training loss 0.038472\n",
      ">> Epoch 826 finished \tANN training loss 0.038473\n",
      ">> Epoch 827 finished \tANN training loss 0.038323\n",
      ">> Epoch 828 finished \tANN training loss 0.038522\n",
      ">> Epoch 829 finished \tANN training loss 0.038479\n",
      ">> Epoch 830 finished \tANN training loss 0.040001\n",
      ">> Epoch 831 finished \tANN training loss 0.040311\n",
      ">> Epoch 832 finished \tANN training loss 0.039848\n",
      ">> Epoch 833 finished \tANN training loss 0.039846\n",
      ">> Epoch 834 finished \tANN training loss 0.039137\n",
      ">> Epoch 835 finished \tANN training loss 0.038903\n",
      ">> Epoch 836 finished \tANN training loss 0.038714\n",
      ">> Epoch 837 finished \tANN training loss 0.038665\n",
      ">> Epoch 838 finished \tANN training loss 0.038704\n",
      ">> Epoch 839 finished \tANN training loss 0.038648\n",
      ">> Epoch 840 finished \tANN training loss 0.038651\n",
      ">> Epoch 841 finished \tANN training loss 0.038965\n",
      ">> Epoch 842 finished \tANN training loss 0.038971\n",
      ">> Epoch 843 finished \tANN training loss 0.038942\n",
      ">> Epoch 844 finished \tANN training loss 0.038524\n",
      ">> Epoch 845 finished \tANN training loss 0.038308\n",
      ">> Epoch 846 finished \tANN training loss 0.038413\n",
      ">> Epoch 847 finished \tANN training loss 0.038384\n",
      ">> Epoch 848 finished \tANN training loss 0.038229\n",
      ">> Epoch 849 finished \tANN training loss 0.038438\n",
      ">> Epoch 850 finished \tANN training loss 0.038472\n",
      ">> Epoch 851 finished \tANN training loss 0.038332\n",
      ">> Epoch 852 finished \tANN training loss 0.038333\n",
      ">> Epoch 853 finished \tANN training loss 0.038491\n",
      ">> Epoch 854 finished \tANN training loss 0.042236\n",
      ">> Epoch 855 finished \tANN training loss 0.039638\n",
      ">> Epoch 856 finished \tANN training loss 0.039321\n",
      ">> Epoch 857 finished \tANN training loss 0.038531\n",
      ">> Epoch 858 finished \tANN training loss 0.038706\n",
      ">> Epoch 859 finished \tANN training loss 0.039063\n",
      ">> Epoch 860 finished \tANN training loss 0.039130\n",
      ">> Epoch 861 finished \tANN training loss 0.038773\n",
      ">> Epoch 862 finished \tANN training loss 0.038338\n",
      ">> Epoch 863 finished \tANN training loss 0.038198\n",
      ">> Epoch 864 finished \tANN training loss 0.038495\n",
      ">> Epoch 865 finished \tANN training loss 0.038386\n",
      ">> Epoch 866 finished \tANN training loss 0.038076\n",
      ">> Epoch 867 finished \tANN training loss 0.038256\n",
      ">> Epoch 868 finished \tANN training loss 0.038156\n",
      ">> Epoch 869 finished \tANN training loss 0.038570\n",
      ">> Epoch 870 finished \tANN training loss 0.038346\n",
      ">> Epoch 871 finished \tANN training loss 0.038340\n",
      ">> Epoch 872 finished \tANN training loss 0.038258\n",
      ">> Epoch 873 finished \tANN training loss 0.038109\n",
      ">> Epoch 874 finished \tANN training loss 0.038227\n",
      ">> Epoch 875 finished \tANN training loss 0.038606\n",
      ">> Epoch 876 finished \tANN training loss 0.038469\n",
      ">> Epoch 877 finished \tANN training loss 0.039130\n",
      ">> Epoch 878 finished \tANN training loss 0.038386\n",
      ">> Epoch 879 finished \tANN training loss 0.038322\n",
      ">> Epoch 880 finished \tANN training loss 0.038226\n",
      ">> Epoch 881 finished \tANN training loss 0.038140\n",
      ">> Epoch 882 finished \tANN training loss 0.037987\n",
      ">> Epoch 883 finished \tANN training loss 0.037933\n",
      ">> Epoch 884 finished \tANN training loss 0.038232\n",
      ">> Epoch 885 finished \tANN training loss 0.037938\n",
      ">> Epoch 886 finished \tANN training loss 0.038069\n",
      ">> Epoch 887 finished \tANN training loss 0.038338\n",
      ">> Epoch 888 finished \tANN training loss 0.038026\n",
      ">> Epoch 889 finished \tANN training loss 0.037908\n",
      ">> Epoch 890 finished \tANN training loss 0.040849\n",
      ">> Epoch 891 finished \tANN training loss 0.038942\n",
      ">> Epoch 892 finished \tANN training loss 0.038403\n",
      ">> Epoch 893 finished \tANN training loss 0.038121\n",
      ">> Epoch 894 finished \tANN training loss 0.037973\n",
      ">> Epoch 895 finished \tANN training loss 0.038035\n",
      ">> Epoch 896 finished \tANN training loss 0.037981\n",
      ">> Epoch 897 finished \tANN training loss 0.037948\n",
      ">> Epoch 898 finished \tANN training loss 0.038161\n",
      ">> Epoch 899 finished \tANN training loss 0.037907\n",
      ">> Epoch 900 finished \tANN training loss 0.037691\n",
      ">> Epoch 901 finished \tANN training loss 0.039052\n",
      ">> Epoch 902 finished \tANN training loss 0.038065\n",
      ">> Epoch 903 finished \tANN training loss 0.038186\n",
      ">> Epoch 904 finished \tANN training loss 0.038418\n",
      ">> Epoch 905 finished \tANN training loss 0.037998\n",
      ">> Epoch 906 finished \tANN training loss 0.037767\n",
      ">> Epoch 907 finished \tANN training loss 0.037764\n",
      ">> Epoch 908 finished \tANN training loss 0.037694\n",
      ">> Epoch 909 finished \tANN training loss 0.037855\n",
      ">> Epoch 910 finished \tANN training loss 0.037675\n",
      ">> Epoch 911 finished \tANN training loss 0.037774\n",
      ">> Epoch 912 finished \tANN training loss 0.038830\n",
      ">> Epoch 913 finished \tANN training loss 0.038099\n",
      ">> Epoch 914 finished \tANN training loss 0.037962\n",
      ">> Epoch 915 finished \tANN training loss 0.037856\n",
      ">> Epoch 916 finished \tANN training loss 0.038005\n",
      ">> Epoch 917 finished \tANN training loss 0.037816\n",
      ">> Epoch 918 finished \tANN training loss 0.037626\n",
      ">> Epoch 919 finished \tANN training loss 0.037564\n",
      ">> Epoch 920 finished \tANN training loss 0.037511\n",
      ">> Epoch 921 finished \tANN training loss 0.037470\n",
      ">> Epoch 922 finished \tANN training loss 0.037774\n",
      ">> Epoch 923 finished \tANN training loss 0.037878\n",
      ">> Epoch 924 finished \tANN training loss 0.037626\n",
      ">> Epoch 925 finished \tANN training loss 0.038157\n",
      ">> Epoch 926 finished \tANN training loss 0.037848\n",
      ">> Epoch 927 finished \tANN training loss 0.038225\n",
      ">> Epoch 928 finished \tANN training loss 0.037437\n",
      ">> Epoch 929 finished \tANN training loss 0.038272\n",
      ">> Epoch 930 finished \tANN training loss 0.037454\n",
      ">> Epoch 931 finished \tANN training loss 0.038249\n",
      ">> Epoch 932 finished \tANN training loss 0.037398\n",
      ">> Epoch 933 finished \tANN training loss 0.037604\n",
      ">> Epoch 934 finished \tANN training loss 0.037504\n",
      ">> Epoch 935 finished \tANN training loss 0.037654\n",
      ">> Epoch 936 finished \tANN training loss 0.037659\n",
      ">> Epoch 937 finished \tANN training loss 0.037728\n",
      ">> Epoch 938 finished \tANN training loss 0.038221\n",
      ">> Epoch 939 finished \tANN training loss 0.038226\n",
      ">> Epoch 940 finished \tANN training loss 0.037928\n",
      ">> Epoch 941 finished \tANN training loss 0.038059\n",
      ">> Epoch 942 finished \tANN training loss 0.038160\n",
      ">> Epoch 943 finished \tANN training loss 0.038041\n",
      ">> Epoch 944 finished \tANN training loss 0.037945\n",
      ">> Epoch 945 finished \tANN training loss 0.037644\n",
      ">> Epoch 946 finished \tANN training loss 0.038109\n",
      ">> Epoch 947 finished \tANN training loss 0.037944\n",
      ">> Epoch 948 finished \tANN training loss 0.037702\n",
      ">> Epoch 949 finished \tANN training loss 0.037672\n",
      ">> Epoch 950 finished \tANN training loss 0.038062\n",
      ">> Epoch 951 finished \tANN training loss 0.037957\n",
      ">> Epoch 952 finished \tANN training loss 0.037929\n",
      ">> Epoch 953 finished \tANN training loss 0.038297\n",
      ">> Epoch 954 finished \tANN training loss 0.037739\n",
      ">> Epoch 955 finished \tANN training loss 0.037703\n",
      ">> Epoch 956 finished \tANN training loss 0.038056\n",
      ">> Epoch 957 finished \tANN training loss 0.043318\n",
      ">> Epoch 958 finished \tANN training loss 0.038715\n",
      ">> Epoch 959 finished \tANN training loss 0.037719\n",
      ">> Epoch 960 finished \tANN training loss 0.038265\n",
      ">> Epoch 961 finished \tANN training loss 0.040856\n",
      ">> Epoch 962 finished \tANN training loss 0.039291\n",
      ">> Epoch 963 finished \tANN training loss 0.037927\n",
      ">> Epoch 964 finished \tANN training loss 0.037778\n",
      ">> Epoch 965 finished \tANN training loss 0.038027\n",
      ">> Epoch 966 finished \tANN training loss 0.037686\n",
      ">> Epoch 967 finished \tANN training loss 0.037441\n",
      ">> Epoch 968 finished \tANN training loss 0.037722\n",
      ">> Epoch 969 finished \tANN training loss 0.038493\n",
      ">> Epoch 970 finished \tANN training loss 0.037725\n",
      ">> Epoch 971 finished \tANN training loss 0.037720\n",
      ">> Epoch 972 finished \tANN training loss 0.039239\n",
      ">> Epoch 973 finished \tANN training loss 0.037952\n",
      ">> Epoch 974 finished \tANN training loss 0.037640\n",
      ">> Epoch 975 finished \tANN training loss 0.037624\n",
      ">> Epoch 976 finished \tANN training loss 0.037708\n",
      ">> Epoch 977 finished \tANN training loss 0.037731\n",
      ">> Epoch 978 finished \tANN training loss 0.037595\n",
      ">> Epoch 979 finished \tANN training loss 0.037532\n",
      ">> Epoch 980 finished \tANN training loss 0.037559\n",
      ">> Epoch 981 finished \tANN training loss 0.037486\n",
      ">> Epoch 982 finished \tANN training loss 0.037601\n",
      ">> Epoch 983 finished \tANN training loss 0.037845\n",
      ">> Epoch 984 finished \tANN training loss 0.037448\n",
      ">> Epoch 985 finished \tANN training loss 0.037586\n",
      ">> Epoch 986 finished \tANN training loss 0.037613\n",
      ">> Epoch 987 finished \tANN training loss 0.038226\n",
      ">> Epoch 988 finished \tANN training loss 0.037963\n",
      ">> Epoch 989 finished \tANN training loss 0.038120\n",
      ">> Epoch 990 finished \tANN training loss 0.037908\n",
      ">> Epoch 991 finished \tANN training loss 0.037607\n",
      ">> Epoch 992 finished \tANN training loss 0.037631\n",
      ">> Epoch 993 finished \tANN training loss 0.037425\n",
      ">> Epoch 994 finished \tANN training loss 0.037190\n",
      ">> Epoch 995 finished \tANN training loss 0.037123\n",
      ">> Epoch 996 finished \tANN training loss 0.037241\n",
      ">> Epoch 997 finished \tANN training loss 0.037411\n",
      ">> Epoch 998 finished \tANN training loss 0.037381\n",
      ">> Epoch 999 finished \tANN training loss 0.037917\n",
      "[END] Fine tuning step\n"
     ]
    }
   ],
   "source": [
    "oof_train2_2, oof_test2_2 = get_oof(dbn, new_train, y_train, new_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, 1, 9)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_lstm = new_test1.reshape(new_test1.shape[0],1,new_test1.shape[1])\n",
    "new_test_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1, 9)]            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               44000     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,403\n",
      "Trainable params: 54,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "lstm_1 = buildLSTM(timeStep=1,inputColNum=9,outStep=3,learnRate=1e-4)\n",
    "oof_train3_3 = np.zeros((new_train.shape[0], _N_CLASS))  \n",
    "oof_test3_3 = np.empty((new_test1.shape[0], _N_CLASS))  \n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(new_train, y_train)):\n",
    "    kf_X_train = new_train[train_index] \n",
    "    kf_X_train = kf_X_train.reshape(kf_X_train.shape[0],1,kf_X_train.shape[1])\n",
    "    kf_y_train = y_train[train_index] \n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = new_train[test_index] \n",
    "    kf_X_test = kf_X_test.reshape(kf_X_test.shape[0],1,kf_X_test.shape[1])\n",
    "\n",
    "    lstm_1.fit(kf_X_train, kf_y_train,epochs=1000,verbose=0,batch_size=20)  \n",
    "\n",
    "    oof_train3_3[test_index] = lstm_1.predict(kf_X_test) \n",
    "    oof_test3_3 += lstm_1.predict(new_test_lstm) \n",
    "\n",
    "oof_test3_3 /= _N_FOLDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_1 = []\n",
    "new_train_1.append(oof_train1_1)\n",
    "new_train_1.append(oof_train2_2)\n",
    "new_train_1.append(oof_train3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_1 = []\n",
    "new_test_1.append(oof_test1_1)\n",
    "new_test_1.append(oof_test2_2)\n",
    "new_test_1.append(oof_test3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_1 = np.concatenate(new_train_1, axis=1)  \n",
    "new_test_1 = np.concatenate(new_test_1, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=33, random_state=4)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = RandomForestRegressor()\n",
    "clf1.fit(new_train_1, y_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFR_pred_train_1 = clf1.predict(new_train_1)\n",
    "RFR_pred_train_label_1 = Predict(RFR_pred_train_1)\n",
    "accuracy_score(y_train, RFR_pred_train_label_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6276595744680851"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_11 = np.nan_to_num(new_test_1.astype(np.float32))\n",
    "RFR_pred_1 = clf1.predict(new_test_11)\n",
    "RFR_pred_label_1 = Predict(RFR_pred_1)\n",
    "accuracy_score(y_test, RFR_pred_label_1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
